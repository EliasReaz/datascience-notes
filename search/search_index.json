{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Science Essential Notes","text":"<p>Driven by a passion for making complex ideas simple, I\u2019ve formatted useful data science notes for quick reference, revision, and real-world use. I hope you find them concise, practical, and helpful.</p> <p>Start exploring by clicking on the tabs above!</p>"},{"location":"#whats-inside","title":"What's Inside","text":"<ul> <li> <p>LLM &amp; RAG   Notes on LLM models, RAG workflow, FAISS, ChromaDB</p> </li> <li> <p>ROC and PR curve   Difference between ROC and PR curve - what, why, when to use</p> </li> <li> <p>Pandas dataframe wrangling   map(), apply(), transform(), pivot_table(), melt()</p> </li> <li> <p>Python   Some useful algorithms.</p> </li> <li> <p>SQL   Real-world query examples covering data retrieval, join, groupby, aggregation, &amp; window functions.</p> </li> <li> <p>Applied Statistics   Key concepts like p-values, statistical power, A/B testing, and bootstrapping\u2014explained simply.</p> </li> <li> <p>Basic Business Metrics   Definitions and use cases of essential KPIs such as conversion rate, churn, and customer lifetime value (LTV).</p> </li> <li> <p>Git Commands   Frequently used Git commands for efficient version control.</p> </li> <li> <p>MLOps Basics   Beginner-friendly notes on Docker, FastAPI, and Flask for building deployable ML solutions.</p> </li> <li>ML Pipeline   Reference code snippets for an end-to-end ML cycle - EDA, Feature Engineering, Scaling, Encoding, Training, Cross-validation, and Testing a ML Classificaton Model  </li> </ul>"},{"location":"AB_testing/AB_testing/","title":"A/B Testing","text":"<p>A/B testing is a statistical method used to determine whether a new variant (e.g., a website design, pricing model, or treatment) results in a measurable improvement over an existing version (the control).</p> <p>It helps answer questions such as:</p> <ul> <li>Does the new website design increase conversion rates for an e-commerce store?</li> <li>Does a new drug improve recovery rates in a clinical trial?</li> <li>Does offering free shipping increase average order value in a retail business?</li> <li>Does a personalized email campaign increase click-through rates in a marketing platform?</li> <li>Does a redesigned onboarding flow reduce user churn for a SaaS product?</li> <li>Does a new pricing model improve subscription upgrades in a fintech app?</li> </ul>"},{"location":"AB_testing/AB_testing/#steps-to-conduct-an-ab-test","title":"Steps to Conduct an A/B Test","text":""},{"location":"AB_testing/AB_testing/#1-formulate-hypotheses-and-set-significance-level","title":"1. Formulate Hypotheses and Set Significance Level","text":"<ul> <li>Null Hypothesis (H\u2080): There is no difference between the control and variant.</li> <li>Alternative Hypothesis (H\u2081): There is a statistically significant difference.</li> <li>Set the significance level (\u03b1), typically 0.05.</li> </ul>"},{"location":"AB_testing/AB_testing/#2-random-assignment-and-isolation","title":"2. Random Assignment and Isolation","text":"<ul> <li>Randomly assign users into:</li> <li>Group A (Control): Existing version</li> <li>Group B (Treatment/Variant): New version</li> <li> <p>Ensure both groups are balanced in key features (e.g., device type, geography, traffic source).</p> </li> <li> <p>Isolation ensures that users in one group (A or B) do not influence the behavior or outcomes of users in the other group.</p> </li> <li>Why isolation matters: Without isolation, results may be biased due to spillover effects, where the treatment indirectly impacts the control group (or vice versa).</li> <li>Example: Suppose you're testing a new referral program (Treatment B). If a user in Group B refers a user in Group A, the latter might behave like a Treatment user, contaminating the control group and invalidating the results.</li> <li>Best practice: Assign at the user level (not session level), and avoid shared environments (e.g., shared devices or accounts) when possible.</li> <li>Ensure users in both groups are using the same operating system (e.g., only iOS or only Android) to eliminate platform-related variability.</li> </ul>"},{"location":"AB_testing/AB_testing/#3-ensure-adequate-sample-size-and-power","title":"3. Ensure Adequate Sample Size and Power","text":"<ul> <li>Before running an A/B test, make sure you have enough users (sample size) to detect a meaningful difference.</li> <li>Effect size (lift): This is the minimum improvement you want to be able to detect \u2014 for example, a 5% increase in conversion rate.</li> <li>Baseline conversion rate: The current conversion rate without any changes (Control group).</li> <li>Power: The chance of correctly detecting a real effect (usually set at 80% or higher). If power is too low, you might miss real improvements.</li> <li>Why it matters: If your sample size is too small, the test may not detect true differences \u2014 leading to false negatives.</li> </ul>"},{"location":"AB_testing/AB_testing/#how-to-calculate-sample-size-simply-in-python-with-statsmodels","title":"How to calculate sample size simply in Python with <code>statsmodels</code>:","text":"<pre><code>from statsmodels.stats.power import NormalIndPower\nfrom statsmodels.stats.proportion import proportion_effectsize\n\n# Baseline conversion rate (control)\np1 = 0.10  # e.g., 10%\n# Expected conversion rate after change (treatment)\np2 = 0.12  # e.g., 12%, which is a 2% absolute lift\n\n# Calculate effect size (Cohen's h)\neffect_size = proportion_effectsize(p2, p1)\n\n# Set power and significance level (alpha)\npower = 0.8        # 80% chance to detect effect\nalpha = 0.05       # 5% chance of false positive (Type I error)\n\n# Initialize power analysis object\nanalysis = NormalIndPower()\n\n# Calculate required sample size per group\nsample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=1)\n\nprint(f\"Sample size needed per group: {int(sample_size)}\")\n</code></pre>"},{"location":"AB_testing/AB_testing/#4-run-the-test-for-sufficient-duration","title":"4. Run the Test for Sufficient Duration","text":"<ul> <li>The test should run long enough to:</li> <li>Cover natural behavior cycles (weekends, holidays)</li> <li>Avoid early stopping or \"peeking\"</li> </ul>"},{"location":"AB_testing/AB_testing/#5-analyze-results-using-p-value","title":"5. Analyze Results Using p-value","text":"<ul> <li>Calculate the p-value.</li> <li>If p &lt; \u03b1, reject the null hypothesis and conclude the effect is statistically significant.</li> </ul>"},{"location":"AB_testing/AB_testing/#6-check-guardrail-metrics","title":"6. Check Guardrail Metrics","text":"<ul> <li>Monitor secondary metrics such as:</li> <li>Bounce rate</li> <li>Customer complaints</li> <li>Load times</li> <li>Ensure there are no unintended negative impacts.</li> </ul>"},{"location":"AB_testing/AB_testing/#7-decide-whether-to-roll-out","title":"7. Decide Whether to Roll Out","text":"<ul> <li>If the result is:</li> <li>Statistically significant</li> <li>Practically meaningful</li> <li>No harm to guardrails</li> </ul> <p>Then, proceed with deploying the new version.</p>"},{"location":"AB_testing/AB_testing/#additional-notes","title":"Additional Notes","text":"<ul> <li>Multiple Testing: Adjust for multiple comparisons (e.g., Bonferroni correction).</li> <li>Effect Size vs. Statistical Significance: Small p-values do not always mean large or meaningful effects.</li> <li>Practical Significance: Consider the business impact beyond statistical metrics.</li> </ul>"},{"location":"AB_testing/AB_testing/#summary-table","title":"Summary Table","text":"Concept Description Example Null Hypothesis (H\u2080) No difference between control and variant \"Conversion rate in group A = group B\" Alternative Hypothesis (H\u2081) Statistically significant difference exists \"Conversion rate in group B &gt; group A\" \u03b1 (Significance Level) Probability of false positive (Type I error), typically 0.05 If p-value &lt; 0.05, reject H\u2080 Power Probability of detecting a true effect, typically 80% 80% chance of detecting a 5% lift if it exists Effect Size (Lift) The measurable difference between variant and control Group A: 10% CR, Group B: 12% CR \u2192 Effect size = 2% Guardrail Metrics Secondary metrics to monitor for unintended consequences Bounce rate, time on site, refund requests, site load time"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/","title":"\ud83d\udcd8 Notes on SDK, API, REST API, HTTP Calls, and Endpoints","text":""},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#1-environment-variables","title":"1. Environment Variables","text":"<ul> <li>Definition: Key-value pairs stored in the OS (Operating System) environment.  </li> <li>Purpose: Programs and SDKs read them at runtime to decide behavior.  </li> <li>Example: <code>python   os.environ['ANONYMIZED_TELEMETRY'] = 'False'</code></li> <li>Tells IBM watsonx SDK whether telemetry data should be anonymized.  </li> <li>General use cases: API keys, config paths, tracking URIs, device settings.</li> </ul> <p>ANOTHER EXAMPLE:</p> <pre><code>import os\n\n# Set environment variable\nos.environ['DB_CONNECTION'] = 'postgresql://user:password@localhost:5432/mydatabase'\n\n# Later in your code, retrieve it\ndb_url = os.environ['DB_CONNECTION']\nprint(\"Connecting to:\", db_url)\n</code></pre> <ul> <li> <p>Key: DB_CONNECTION</p> </li> <li> <p>Value: the database connection string.</p> </li> <li> <p>Usage: Your script or SDK can read this variable to know how to connect to the database, without hard\u2011coding sensitive info in the code.</p> </li> </ul>"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#2-api-application-programming-interface","title":"2. API (Application Programming Interface)","text":"<ul> <li>Definition: A set of rules/contracts for communication between software components.  </li> <li>Form: Functions, methods, or endpoints.  </li> <li>Example: IBM Cloud API endpoint <code>POST /generate_text</code>.  </li> <li>Key idea: API = the interface (the \u201cdoorway\u201d to a system).</li> </ul>"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#3-sdk-software-development-kit","title":"3. SDK (Software Development Kit)","text":"<ul> <li>Definition: A toolbox/package that wraps APIs and provides developer\u2011friendly tools.  </li> <li>Contents: Libraries, classes, utilities, documentation.  </li> <li>Example: IBM watsonx SDK (<code>ibm_watsonx_ai</code>) provides <code>ModelInference</code>.  </li> <li>Role: SDK uses APIs internally, but exposes clean Python methods.  </li> <li>Analogy: SDK = dashboard + controls to drive the API easily.</li> </ul>"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#4-rest-api","title":"4. REST API","text":"<ul> <li>Definition: A design style for building APIs using HTTP.  </li> <li>Principles:</li> <li>Resources (users, orders, models) exposed via URLs.  </li> <li>Standard HTTP methods (<code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>).  </li> <li>Stateless (each request is independent, can not recall/remember earlier request).  </li> <li>JSON/XML for resource representation.  </li> <li>Example:  </li> <li><code>GET /users/123</code> \u2192 fetch user info  </li> <li><code>POST /orders</code> \u2192 make/genrate order  </li> </ul>"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#5-http-call","title":"5. HTTP Call","text":"<ul> <li>Definition: The raw request sent using the HTTP protocol.  </li> <li>Components: Method (GET/POST), URL, headers, body.  </li> <li>Example (Python <code>requests</code>):   <code>python   response = requests.post(\"https://api.example.com/generate\",                            headers={\"Authorization\": \"Bearer token\"},                            json={\"input\": \"Hello\"})</code></li> <li>Key idea: REST APIs are built on top of HTTP calls.</li> </ul>"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#6-endpoint","title":"6. Endpoint","text":"<ul> <li>Definition: A specific URL where an API action happens.  </li> <li>Base URL vs Endpoint:</li> <li>Base: <code>https://api.example.com/</code> </li> <li>Endpoint: <code>https://api.example.com/users/123</code> </li> <li>Role: Endpoint = the \u201caddress\u201d of the resource/action.  </li> <li>In SDKs: You don\u2019t see endpoints directly; the SDK calls them internally.</li> </ul>"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#7-putting-it-all-together-flow","title":"7. Putting It All Together (Flow)","text":"<pre><code>Your Python Code\n   |\n   v\nSDK (ibm_watsonx_ai)\n   - Classes like ModelInference\n   - Wraps API calls\n   |\n   v\nREST API Endpoint (URL)\n   - e.g. https://api.watsonx.ibm.com/v1/generate\n   |\n   v\nHTTP Call\n   - POST request with headers + JSON\n   |\n   v\nIBM Cloud Service\n   - Processes request, returns response\n   |\n   v\nYour Python Code (receives result)\n</code></pre>"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#8-sidebyside-example","title":"8. Side\u2011by\u2011Side Example","text":"<p>Raw HTTP (manual):</p> <pre><code>import requests\nurl = \"https://api.watsonx.ibm.com/v1/generate\"\nheaders = {\"Authorization\": \"Bearer &lt;api_key&gt;\", \"Content-Type\": \"application/json\"}\npayload = {\"model_id\": \"some-model-id\", \"input\": \"Hello world\"}\nresponse = requests.post(url, headers=headers, json=payload)\nprint(response.json())\n</code></pre> <p>SDK (simplified):</p> <pre><code>from ibm_watsonx_ai.foundation_models import ModelInference\nmodel = ModelInference(model_id=\"some-model-id\")\nresult = model.generate_text(prompt=\"Hello world\")\nprint(result)\n</code></pre>"},{"location":"API_SDK_QuickRef/api_sdk_rest_http_endpoints/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Environment variable = global config knob.  </li> <li>API = communication rules/contracts.  </li> <li>SDK = toolbox wrapping APIs for easier use.  </li> <li>REST API = structured style of using HTTP for resources.  </li> <li>HTTP call = the raw request/response transport.  </li> <li>Endpoint = the specific URL where the API action happens.  </li> <li>SDK vs API: SDK simplifies API usage; API is the underlying contract.  </li> <li>REST vs HTTP: REST is a design style; HTTP is the protocol it uses.</li> </ul>"},{"location":"Applied_statistics/AB_testing/","title":"A/B Testing","text":"<p>A/B testing is a statistical method used to determine whether a new variant (e.g., a website design, pricing model, or treatment) results in a measurable improvement over an existing version (the control).</p> <p>It helps answer questions such as:</p> <ul> <li>Does the new website design increase conversion rates for an e-commerce store?</li> <li>Does a new drug improve recovery rates in a clinical trial?</li> <li>Does offering free shipping increase average order value in a retail business?</li> <li>Does a personalized email campaign increase click-through rates in a marketing platform?</li> <li>Does a redesigned onboarding flow reduce user churn for a SaaS product?</li> <li>Does a new pricing model improve subscription upgrades in a fintech app?</li> </ul>"},{"location":"Applied_statistics/AB_testing/#steps-to-conduct-an-ab-test","title":"Steps to Conduct an A/B Test","text":""},{"location":"Applied_statistics/AB_testing/#1-formulate-hypotheses-and-set-significance-level","title":"1. Formulate Hypotheses and Set Significance Level","text":"<ul> <li>Null Hypothesis (H\u2080): There is no difference between the control and variant.</li> <li>Alternative Hypothesis (H\u2081): There is a statistically significant difference.</li> <li>Set the significance level (\u03b1), typically 0.05.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#2-random-assignment-and-isolation","title":"2. Random Assignment and Isolation","text":"<ul> <li>Randomly assign users into:</li> <li>Group A (Control): Existing version</li> <li>Group B (Treatment/Variant): New version</li> <li> <p>Ensure both groups are balanced in key features (e.g., device type, geography, traffic source).</p> </li> <li> <p>Isolation ensures that users in one group (A or B) do not influence the behavior or outcomes of users in the other group.</p> </li> <li>Why isolation matters: Without isolation, results may be biased due to spillover effects, where the treatment indirectly impacts the control group (or vice versa).</li> <li>Example: Suppose you're testing a new referral program (Treatment B). If a user in Group B refers a user in Group A, the latter might behave like a Treatment user, contaminating the control group and invalidating the results.</li> <li>Best practice: Assign at the user level (not session level), and avoid shared environments (e.g., shared devices or accounts) when possible.</li> <li>Ensure users in both groups are using the same operating system (e.g., only iOS or only Android) to eliminate platform-related variability.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#3-ensure-adequate-sample-size-and-power","title":"3. Ensure Adequate Sample Size and Power","text":"<ul> <li>Before running an A/B test, make sure you have enough users (sample size) to detect a meaningful difference.</li> <li>Effect size (lift): This is the minimum improvement you want to be able to detect \u2014 for example, a 5% increase in conversion rate.</li> <li>Baseline conversion rate: The current conversion rate without any changes (Control group).</li> <li>Power: The chance of correctly detecting a real effect (usually set at 80% or higher). If power is too low, you might miss real improvements.</li> <li>Why it matters: If your sample size is too small, the test may not detect true differences \u2014 leading to false negatives.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#how-to-calculate-sample-size-simply-in-python-with-statsmodels","title":"How to calculate sample size simply in Python with <code>statsmodels</code>:","text":"<pre><code>from statsmodels.stats.power import NormalIndPower\nfrom statsmodels.stats.proportion import proportion_effectsize\n\n# Baseline conversion rate (control)\np1 = 0.10  # e.g., 10%\n# Expected conversion rate after change (treatment)\np2 = 0.12  # e.g., 12%, which is a 2% absolute lift\n\n# Calculate effect size (Cohen's h)\neffect_size = proportion_effectsize(p2, p1)\n\n# Set power and significance level (alpha)\npower = 0.8        # 80% chance to detect effect\nalpha = 0.05       # 5% chance of false positive (Type I error)\n\n# Initialize power analysis object\nanalysis = NormalIndPower()\n\n# Calculate required sample size per group\nsample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=1)\n\nprint(f\"Sample size needed per group: {int(sample_size)}\")\n</code></pre>"},{"location":"Applied_statistics/AB_testing/#4-run-the-test-for-sufficient-duration","title":"4. Run the Test for Sufficient Duration","text":"<ul> <li>The test should run long enough to:</li> <li>Cover natural behavior cycles (weekends, holidays)</li> <li>Avoid early stopping or \"peeking\"</li> </ul>"},{"location":"Applied_statistics/AB_testing/#5-analyze-results-using-p-value","title":"5. Analyze Results Using p-value","text":"<ul> <li>Calculate the p-value.</li> <li>If p &lt; \u03b1, reject the null hypothesis and conclude the effect is statistically significant.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#6-check-guardrail-metrics","title":"6. Check Guardrail Metrics","text":"<ul> <li>Monitor secondary metrics such as:</li> <li>Bounce rate</li> <li>Customer complaints</li> <li>Load times</li> <li>Ensure there are no unintended negative impacts.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#7-decide-whether-to-roll-out","title":"7. Decide Whether to Roll Out","text":"<ul> <li>If the result is:</li> <li>Statistically significant</li> <li>Practically meaningful</li> <li>No harm to guardrails</li> </ul> <p>Then, proceed with deploying the new version.</p>"},{"location":"Applied_statistics/AB_testing/#additional-notes","title":"Additional Notes","text":"<ul> <li>Multiple Testing: Adjust for multiple comparisons (e.g., Bonferroni correction).</li> <li>Effect Size vs. Statistical Significance: Small p-values do not always mean large or meaningful effects.</li> <li>Practical Significance: Consider the business impact beyond statistical metrics.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#summary-table","title":"Summary Table","text":"Concept Description Example Null Hypothesis (H\u2080) No difference between control and variant \"Conversion rate in group A = group B\" Alternative Hypothesis (H\u2081) Statistically significant difference exists \"Conversion rate in group B &gt; group A\" \u03b1 (Significance Level) Probability of false positive (Type I error), typically 0.05 If p-value &lt; 0.05, reject H\u2080 Power Probability of detecting a true effect, typically 80% 80% chance of detecting a 5% lift if it exists Effect Size (Lift) The measurable difference between variant and control Group A: 10% CR, Group B: 12% CR \u2192 Effect size = 2% Guardrail Metrics Secondary metrics to monitor for unintended consequences Bounce rate, time on site, refund requests, site load time"},{"location":"Applied_statistics/Explanation_pvalue/","title":"Explanation of p-value in statistical inferences","text":""},{"location":"Applied_statistics/Explanation_pvalue/#what-does-a-p-value-mean","title":"What does a p-value mean?","text":"<ul> <li>A p-value is the probability of obtaining test results at least as extreme as the observed results, under the assumption that the null hypothesis (H\u2080) is true.</li> <li>A p-value helps us figure out how surprising our results are if we assume the null hypothesis (H\u2080) is true.</li> <li>When we say \"at least as extreme\" we're asking: if the null hypothesis were true, what's the probability of getting results that are this unusual or even more unusual than what we actually observed?</li> <li>Think of it this way: your observed data gives you a certain amount of \"evidence against\" the null hypothesis. The p-value asks, What's the chance of getting this much evidence against the null hypothesis, or even stronger evidence, if the null hypothesis is actually true?</li> <li>Let's say we think a coin is fair (H\u2080: The coin is fair - the chance of heads is 50%). We toss it 100 times and get only 5 heads. That seems strange, right?</li> <li>The p-value tells us: \"If the coin really is fair, what\u2019s the chance we\u2019d see something as extreme as 5 heads (or fewer)?\"</li> <li>That\u2019s a one-tailed test - looking just at one end (very few heads).</li> <li>A two-tailed test would ask: \"What\u2019s the chance of getting something really extreme on either end - like 5 or fewer heads or 95 or more heads?\"</li> <li>If the p-value is really small (say, less than 0.05), that means our result is pretty unusual under the assumption of fairness. So we might start to doubt the coin is actually fair.</li> </ul> <p>Note 1: A small p-value doesn\u2019t prove the coin is unfair - it just means what we saw would be unlikely if it were fair.</p> <p>Note 2: A small p-value does not mean H\u2080 is false. It means the observed data would be unlikely under H\u2080. It means we fail to reject H$_o$.</p> <pre><code>import scipy.stats as stats\n\n# Set the parameters\nn = 100          # Number of coin tosses\np = 0.5          # Probability of heads under the null hypothesis (fair coin)\nk = 5            # Observed number of heads (very low!)\n\n# ================================================\n# One-tailed p-value:\n# We want the probability of getting k or fewer heads if the coin is fair.\n# That's why we use CDF (Cumulative Distribution Function) \u2014 it adds up\n# the probability of 0, 1, 2, ..., up to k heads.\n# If we used PDF, it would only give the probability of *exactly* 5 heads.\n# ================================================\np_value_one_tail = stats.binom.cdf(k, n, p)\nprint(\"One-tailed p-value (5 or fewer heads):\", p_value_one_tail)\n\n# ================================================\n# Two-tailed p-value:\n# We want to find how extreme the result is on *both* sides of the expected value (which is 50 heads).\n# Getting only 5 heads is 45 less than expected \u2192 delta = 45\n# So, we also look at the *other* extreme: 50 + 45 = 95 heads or more\n# CDF gives cumulative up to a value, so for 95 or more, we subtract CDF(94) from 1.\n# ================================================\n\nexpected = n * p                   # Expected number of heads = 50\ndelta = abs(k - expected)         # How far away our result (5) is from expected (50)\n\n# Left tail: probability of getting &lt;= 5 heads (already calculated, but let's redo for clarity)\np_low = stats.binom.cdf(expected - delta, n, p)  # same as stats.binom.cdf(5, 100, 0.5)\n\n# Right tail: probability of getting &gt;= 95 heads\n# Since CDF gives probability of &lt;= x, we subtract from 1 to get &gt;= 95\np_high = 1 - stats.binom.cdf(expected + delta - 1, n, p)  # 1 - CDF(94)\n\n# Add both tails to get the total two-tailed p-value\np_value_two_tail = p_low + p_high\nprint(\"Two-tailed p-value (5 or fewer OR 95 or more heads):\", p_value_two_tail)\n</code></pre>"},{"location":"Applied_statistics/ci_median/","title":"Confidence Interval (CI) for the Median using Bootstrap","text":""},{"location":"Applied_statistics/ci_median/#why-we-need-it","title":"\ud83d\udccc Why We Need It","text":"<p>The median is a robust measure of central tendency, especially in skewed distributions. However, it does not have a straightforward formula for calculating its confidence interval (unlike the mean). So, we use bootstrap resampling to estimate the confidence interval for the median.</p>"},{"location":"Applied_statistics/ci_median/#step-by-step-process","title":"\ud83d\udd22 Step-by-Step Process","text":"<ol> <li>Start with your dataset.</li> <li> <p>Example: <code>[3, 5, 7, 2, 4, 20, 7]</code></p> </li> <li> <p>Use bootstrap resampling:</p> </li> <li>Randomly sample with replacement from the dataset.</li> <li>Each bootstrap sample should be the same size as the original dataset.</li> <li> <p>Repeat this process many times (e.g., 10,000 iterations).</p> </li> <li> <p>Calculate the median of each resampled dataset and store the values.</p> </li> <li> <p>Sort the list of all bootstrap medians (optional for plotting).</p> </li> <li> <p>Compute percentiles:</p> </li> <li>For a 95% confidence interval, take the 2.5th percentile and 97.5th percentile of the bootstrap medians.</li> </ol>"},{"location":"Applied_statistics/ci_median/#correct-python-code","title":"\u2705 Correct Python Code","text":"<pre><code>import numpy as np\n\n# Original dataset\noriginal_dataset = [3, 5, 7, 2, 4, 20, 7]\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Number of bootstrap samples\nN = 10000\n\n# Store medians\nmedians = []\n\nfor _ in range(N):\n    sample = np.random.choice(original_dataset, size=len(original_dataset), replace=True)\n    medians.append(np.median(sample))\n\n# Compute 95% confidence interval\nlower, upper = np.percentile(medians, [2.5, 97.5])\n\nprint(\"95% Confidence Interval for the median:\", lower, \"to\", upper)\n</code></pre>"},{"location":"Applied_statistics/ci_median/#notes","title":"\ud83d\udd0d Notes","text":"<ul> <li>The method used here is called the percentile bootstrap method.</li> <li>You can adjust the confidence level (e.g., use [5, 95] for a 90% CI).</li> <li>Bootstrap is useful when theoretical distributions are unknown or complicated.</li> </ul>"},{"location":"Applied_statistics/ci_median/#optional-plotting-the-bootstrap-distribution","title":"\ud83d\udcca Optional: Plotting the Bootstrap Distribution","text":"<pre><code>import matplotlib.pyplot as plt\n\nplt.hist(medians, bins=50, edgecolor='k')\nplt.axvline(lower, color='red', linestyle='--', label=f'2.5% = {lower:.2f}')\nplt.axvline(upper, color='red', linestyle='--', label=f'97.5% = {upper:.2f}')\nplt.title('Bootstrap Distribution of Median')\nplt.xlabel('Median')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Applied_statistics/ci_median/#summary","title":"\ud83e\udde0 Summary","text":"<p>Bootstrap is a powerful, non-parametric tool to compute confidence intervals for statistics that don't have easy closed-form solutions \u2014 like the median. It's simple to implement and widely used in real-world data science.</p>"},{"location":"Applied_statistics/ci_median/#optional-clarification","title":"Optional Clarification","text":"<ul> <li> <p>This is called the percentile bootstrap method for estimating confidence intervals.</p> </li> <li> <p>You can also use bias-corrected and accelerated (BCa) bootstrap for improved accuracy, especially when the sampling distribution is skewed.</p> </li> </ul>"},{"location":"Common_business_metrics/common_business_metrics/","title":"Common Business metrics","text":"Term Plain Definition (with Time-Frame) Example (with Time-Frame) Conversion Rate The % of users who take a desired action (like signing up or buying) within a specific period. If 100 people visit a site in one day and 5 make a purchase, conversion rate = 5%. Click-Through Rate (CTR) The % of people who clicked on a link out of those who saw it during a campaign or time window. If 1,000 people see an ad over a week and 50 click on it, CTR = 5%. Bounce Rate The % of visitors who leave a website after viewing only one page during a session or time frame. High bounce rate last week may mean users didn\u2019t find what they wanted. Churn Rate The % of users or customers who stop using your product or cancel a subscription within a given time frame (e.g., monthly). If you have 100 customers and 5 leave in January, churn rate = 5%. Retention Rate The % of users who keep using your product after a certain time period (e.g., after 1 month). If 100 people sign up and 60 are still active after 1 month, retention = 60%. Average Order Value (AOV) The average amount spent per purchase, usually calculated over a time frame like a day or month. If 10 orders total \\$500 in a month, then AOV = \\$50. Lifetime Value (LTV) The total money a customer is expected to spend over their relationship typically measured in months or years. If a typical customer pays \\$20/month for 2 years, LTV = \\$480. Impressions The number of times your ad or content is shown to users during a campaign or time window. 1,000 people saw your ad in March = 1,000 impressions. Engagement Rate A measure of how much users interact with your content (likes, comments, clicks) within a given period. 200 likes from 1,000 views last week = 20% post engagement rate. Upsell When a customer buys a more expensive version or adds extras usually within a specific time after initial purchase. Buying a laptop and adding an extended warranty within 30 days. A/B Test Lift The difference (increase or decrease) in a metric between control and variant measured during the test period. If control conversion = 10% and variant = 12% after 2 weeks test, lift = +2%."},{"location":"Confusion_matrix/ROC_PR_curve/","title":"ROC vs PR curve","text":""},{"location":"Confusion_matrix/ROC_PR_curve/#annotated-confusion-matrix-roc-plot","title":"\ud83d\udcca Annotated Confusion Matrix (ROC plot)","text":"<ul> <li>The x-axis is the False Positive Rate (FPR), also known as 1 - Specificity.</li> <li>The y-axis is the True Positive Rate (TPR), also known as Sensitivity or Recall</li> </ul> <pre><code>                Predicted Positive   Predicted Negative\nActual Positive    TP                  FN\n                  \u2191                   \u2191\n                  |                   |\n                  |                   \u2514\u2500\u2500 Missed positives \u2192 lowers Recall (TPR)\n                  \u2514\u2500\u2500 True Positives \u2192 boosts Recall (TPR)\n\nActual Negative    FP                  TN\n                  \u2191                   \u2191\n                  |                   |\n                  |                   \u2514\u2500\u2500 True Negatives \u2192 boosts Specificity\n                  \u2514\u2500\u2500 False Positives \u2192 raises False Positive Rate (FPR)\n</code></pre>"},{"location":"Confusion_matrix/ROC_PR_curve/#key-metrics","title":"Key Metrics","text":"<ul> <li> <p>TPR (Recall, Sensitivity) = $$\\frac{TP}{TP + FN}$$   \u2192 Fraction of actual positives correctly identified. </p> </li> <li> <p>FPR (False Positive Rate) = $$\\frac{FP}{FP + TN}$$   \u2192 Fraction of actual negatives wrongly flagged as positive.  </p> </li> <li>ROC Curve = Plot of TPR vs FPR across thresholds.  </li> <li>AUC (Area Under Curve) = Single number summarizing ROC; closer to 1 = better classifier.  </li> </ul>"},{"location":"Confusion_matrix/ROC_PR_curve/#why-tpr-vs-fpr-matters","title":"\ud83d\udd0d Why TPR vs FPR Matters","text":"<ul> <li> <p>Trade\u2011off visualization:   Every threshold you set for a classifier changes both TPR (sensitivity) and FPR (false alarm rate). Plotting them against each other shows the trade\u2011off \u2014 you can\u2019t usually maximize both at the same time.</p> </li> <li> <p>Threshold independence:   Accuracy at a single threshold can be misleading. By plotting TPR vs FPR across all thresholds, you see the classifier\u2019s overall behavior, not just one arbitrary cutoff.</p> </li> <li> <p>Comparing models:   ROC curves let you compare different classifiers. A curve closer to the top\u2011left corner (high TPR, low FPR) indicates a better model. The Area Under the Curve (AUC) is a single summary metric of this performance.</p> </li> <li> <p>Application sensitivity: </p> </li> <li>In medical diagnosis, you want high TPR (catch all true cases), even if FPR rises.  </li> <li>In fraud detection, you want low FPR (avoid false alarms), even if TPR drops.   The ROC curve helps you choose the right threshold depending on the domain\u2019s tolerance for false positives vs false negatives.</li> </ul>"},{"location":"Confusion_matrix/ROC_PR_curve/#analogy","title":"\u2696\ufe0f Analogy","text":"<p>Think of it like a security checkpoint: - TPR = how many actual threats you catch. - FPR = how many innocent travelers you wrongly flag. Plotting TPR vs FPR shows whether your scanner is too strict (flags everyone) or too lenient (misses threats). The ROC curve helps balance these extremes.</p>"},{"location":"Confusion_matrix/ROC_PR_curve/#key-insight","title":"\ud83d\udcca Key Insight","text":"<p>The importance lies in understanding the balance between sensitivity and specificity. Without this curve, you\u2019d only see performance at one threshold, missing the bigger picture of how the model behaves across all possible decision boundaries.</p>"},{"location":"Confusion_matrix/ROC_PR_curve/#worked-examples","title":"\ud83e\uddea Worked Examples","text":""},{"location":"Confusion_matrix/ROC_PR_curve/#1-medical-diagnosis-disease-test","title":"1. Medical Diagnosis (Disease Test)","text":"<ul> <li>Suppose 100 patients: 40 sick, 60 healthy.  </li> <li>Test results: TP=35, FN=5, FP=10, TN=50.  </li> </ul> <p>Metrics: - TPR (Recall) = 35 / (35+5) = 87.5% - FPR = 10 / (10+50) = 16.7% - ROC Curve: If threshold is lowered, TPR \u2191 but FPR \u2191 too. - AUC: If classifier is strong, curve hugs top-left; here maybe ~0.9.  </p> <p>Interpretation: Good recall (few sick missed), but some false alarms.</p>"},{"location":"Confusion_matrix/ROC_PR_curve/#2-customer-churn-prediction","title":"2. Customer Churn Prediction","text":"<ul> <li>Dataset: 200 customers, 50 churned, 150 stayed.  </li> <li>Model predicts: TP=40 churners correctly, FN=10 missed churners, FP=30 wrongly flagged, TN=120 correctly retained.  </li> </ul> <p>Metrics: - TPR = 40 / (40+10) = 80% - FPR = 30 / (30+120) = 20% - ROC: Adjusting threshold changes balance between catching churners vs wrongly alarming loyal customers. - AUC: ~0.85 indicates decent discrimination.  </p> <p>Interpretation: Model is good at catching churn, but 20% of loyal customers wrongly flagged could waste retention budget.</p>"},{"location":"Confusion_matrix/ROC_PR_curve/#3-loan-default-prediction","title":"3. Loan Default Prediction","text":"<ul> <li>Dataset: 500 applicants, 100 defaulters, 400 non-defaulters.</li> <li>Model predicts: TP=70 defaulters correctly, FN=30 missed, FP=40 wrongly flagged, TN=360 correctly cleared.  </li> </ul> <p>Metrics: - TPR = 70 / (70+30) = 70% - FPR = 40 / (40+360) = 10% - ROC: Lower threshold \u2192 more defaults caught but more false alarms. - AUC: ~0.88 shows strong separation.  </p> <p>Interpretation: Bank balances risk: catching most defaulters while keeping false alarms low.</p>"},{"location":"Confusion_matrix/ROC_PR_curve/#cheat-sheet-summary","title":"\ud83d\udcdd Cheat Sheet Summary","text":"<ul> <li>TPR (Recall) \u2192 \u201cHow many actual positives did we catch?\u201d  </li> <li>FPR \u2192 \u201cHow many actual negatives did we wrongly flag?\u201d  </li> <li>ROC Curve \u2192 Trade-off between TPR and FPR at different thresholds.  </li> <li>AUC \u2192 Overall classifier quality (closer to 1 = better).  </li> </ul> <p>QUESTION: Difference between ROC and Precision-Rcall curves</p> <p>Quick Answer: The ROC curve plots True Positive Rate (Recall) vs False Positive Rate, while the Precision-Recall (PR) curve plots Precision vs Recall. ROC is more informative when classes are balanced, whereas PR curves are more useful for imbalanced datasets, since they highlight how well a model identifies the minority (positive) class.</p>"},{"location":"Confusion_matrix/ROC_PR_curve/#roc-curve-receiver-operating-characteristic","title":"\ud83d\udcca ROC Curve (Receiver Operating Characteristic)","text":"<ul> <li>Axes: </li> <li>X-axis \u2192 False Positive Rate (FPR = FP / (FP + TN))  </li> <li>Y-axis \u2192 True Positive Rate (TPR = Recall = TP / (TP + FN))  </li> <li>Interpretation: </li> <li>Shows trade-off between sensitivity (recall) and specificity (1 \u2212 FPR).  </li> <li>A diagonal line represents random guessing; curves above it indicate better performance.  </li> <li>Best Use Case: </li> <li>Balanced datasets where both positive and negative classes are equally important.  </li> <li>Good for evaluating overall discrimination ability across thresholds.</li> </ul>"},{"location":"Confusion_matrix/ROC_PR_curve/#precision-recall-curve","title":"\ud83c\udfaf Precision-Recall Curve","text":"<ul> <li>Axes: </li> <li>X-axis \u2192 Recall (TP / (TP + FN))  </li> <li>Y-axis \u2192 Precision (TP / (TP + FP))  </li> <li>Interpretation: </li> <li>Focuses on the trade-off between precision (how many predicted positives are correct) and recall (how many actual positives are captured).  </li> <li>High precision with high recall indicates strong performance.  </li> <li>Best Use Case: </li> <li>Imbalanced datasets (e.g., fraud detection, rare disease diagnosis).  </li> <li>More sensitive to performance on the minority class, unlike ROC which can be overly optimistic when negatives dominate.</li> </ul>"},{"location":"Confusion_matrix/ROC_PR_curve/#key-differences","title":"\ud83d\udd11 Key Differences","text":"Aspect ROC Curve Precision-Recall Curve X-axis False Positive Rate Recall Y-axis True Positive Rate (Recall) Precision Focus Overall discrimination ability Positive class performance Best for Balanced datasets Imbalanced datasets Risk Can look good even if positives are poorly predicted Highlights poor precision/recall clearly <p>Sources: </p> <p>\ud83d\udc49 Think of it this way: ROC tells you how well your model separates classes overall, while PR tells you how well your model finds the rare positives without drowning in false alarms.  </p>"},{"location":"LLM_RAG_notes/FAISS_ChromaDB/","title":"\ud83d\udccc Vector Search Reference Notes","text":""},{"location":"LLM_RAG_notes/FAISS_ChromaDB/#faiss-facebook-ai-similarity-search","title":"\ud83d\udd39 FAISS (Facebook AI Similarity Search)","text":"<ul> <li>Type: Library (not a database)</li> <li>Strengths:</li> <li>High-performance similarity search on single machine (CPU/GPU)</li> <li>Multiple indexing options (flexibility + control)</li> <li>Great for millions of vectors</li> <li>Limitations:</li> <li>No metadata storage</li> <li>No built-in distributed scaling</li> <li>Requires coding to integrate</li> </ul>"},{"location":"LLM_RAG_notes/FAISS_ChromaDB/#index-options-in-faiss","title":"\ud83d\uddc2 Index Options in FAISS","text":"Index Type How It Works Pros Cons / Trade-offs Flat Index Brute force: compares query to every vector Exact, highest accuracy Very slow for large datasets IVF (Inverted File Index) Clusters vectors, searches only nearest clusters Much faster, scalable Approximate, may miss close neighbors LSH (Locality-Sensitive Hashing) Hash functions bucket similar vectors Memory-efficient, good for high-dimensional sparse data (e.g., text embeddings) Lower accuracy, not always fastest HNSW (Hierarchical Navigable Small World) Multi-layer graph search Fast, high recall (~90\u201399%), tunable Approximate, memory-heavy, less ideal for dynamic datasets"},{"location":"LLM_RAG_notes/FAISS_ChromaDB/#chromadb","title":"\ud83d\udd39 ChromaDB","text":"<ul> <li>Type: Full vector database</li> <li>Strengths:</li> <li>Stores vectors + metadata (tags, descriptions)</li> <li>Easy integration with LangChain and AI prototyping</li> <li>Service-based, convenient setup</li> <li>Limitations:</li> <li>Only supports HNSW indexing</li> <li>Less control compared to FAISS</li> <li>Best for: Rapid prototyping, metadata filtering, ease of use</li> </ul>"},{"location":"LLM_RAG_notes/FAISS_ChromaDB/#hnsw-hierarchical-navigable-small-world-graph","title":"\ud83d\udd39 HNSW (Hierarchical Navigable Small World Graph)","text":"<ul> <li>Concept: Multi-layer graph search (highways \u2192 main roads \u2192 local streets analogy)</li> <li>Search process:</li> <li>Start at sparse top layer \u2192 greedy search</li> <li>Move down layers until reaching dense bottom layer</li> <li>Index building:</li> <li>Each point gets random height</li> <li>Connected to closest neighbors at each level</li> <li>Performance:</li> <li>Recall: ~90\u201399% (approximate, not exact)</li> <li>Tunable parameters: connections, breadth of search</li> <li>Trade-offs:</li> <li>Best for mostly-static datasets</li> <li>Memory-heavy if tuned for high accuracy</li> <li>Not ideal for frequent insertions/deletions</li> <li>Approximate results (not guaranteed exact)</li> </ul>"},{"location":"LLM_RAG_notes/FAISS_ChromaDB/#milvus","title":"\ud83d\udd39 Milvus","text":"<ul> <li>Type: Distributed vector database</li> <li>Strengths:</li> <li>Production-ready scaling</li> <li>Hybrid search (structured + vector)</li> <li>Uses FAISS and HNSW under the hood</li> <li>Best for: Large-scale, distributed, production deployments</li> </ul>"},{"location":"LLM_RAG_notes/FAISS_ChromaDB/#quick-comparison-table","title":"\u2696\ufe0f Quick Comparison Table","text":"Feature FAISS (Library) ChromaDB (DB) HNSW (Index) Milvus (DB) Nature Toolkit/library Full database Index algo Distributed DB Metadata support \u274c \u2705 \u274c \u2705 Index options Flat, IVF, LSH, HNSW Only HNSW N/A Multiple (via FAISS/HNSW) Ease of use Low (code req.) High N/A Medium (setup needed) Performance Very high local Good, convenient Fast approx Scalable, production Scaling Single machine Limited N/A Distributed Best for Control + speed Prototyping Approx search Enterprise scale <p>\u2705 Key Takeaway: - FAISS \u2192 control + speed, multiple index choices (Flat, IVF, LSH, HNSW) - ChromaDB \u2192 convenience + metadata, prototyping - HNSW \u2192 fast approximate search, static datasets - Milvus \u2192 distributed, production-ready  </p>"},{"location":"LLM_RAG_notes/RAG_Retrievers_LlamaIndex_Notes/","title":"Retriever in RAG","text":"<p>Perfect \u2014 now that we have Part 1 and Part 2, let\u2019s stitch them into a layered set of notes. I\u2019ll structure this so that both a novice learner and an advanced practitioner can benefit.  </p>"},{"location":"LLM_RAG_notes/RAG_Retrievers_LlamaIndex_Notes/#rag-llamaindex-retrievers-notes-part-1-part-2","title":"\ud83e\uddfe RAG &amp; LlamaIndex Retrievers \u2014 Notes (Part 1 + Part 2)","text":""},{"location":"LLM_RAG_notes/RAG_Retrievers_LlamaIndex_Notes/#beginner-friendly-notes-novice-level","title":"\ud83c\udf31 Beginner-Friendly Notes (Novice Level)","text":"<p>Think of retrievers as search helpers inside a Retrieval-Augmented Generation (RAG) pipeline. Each retriever has a different way of finding the right information:  </p> <ul> <li>VectorStoreIndex \u2192 Stores \u201cmeaning\u201d of text chunks as numbers (embeddings). Best for finding semantically similar content (like Google\u2019s \u201cDid you mean\u2026\u201d).  </li> <li>DocumentSummaryIndex \u2192 Creates short summaries of documents. Helps filter large sets quickly before diving into details.  </li> <li>KeywordTableIndex \u2192 Extracts keywords and maps them to chunks. Good for exact keyword searches or rule-based lookups.  </li> <li>VectorIndexRetriever \u2192 Uses embeddings to find related content. General-purpose retriever for RAG.  </li> <li>BM25 Retriever \u2192 Classic keyword-based search. Matches exact words, not meanings.  </li> <li>Document Summary Index Retriever \u2192 Uses summaries instead of full docs. Two versions:  </li> <li>LLM-based (uses language model to interpret summaries).  </li> <li>Semantic similarity-based (uses embeddings).  </li> <li>Auto Merging Retriever \u2192 Handles long documents by breaking them into parent-child chunks, keeping context intact.  </li> <li>Recursive Retriever \u2192 Follows links between nodes (like citations or metadata). Useful for connected documents.  </li> <li>Query Fusion Retriever \u2192 Combines results from multiple retrievers. Uses fusion strategies:  </li> <li>Reciprocal Rank Fusion (balances rankings).  </li> <li>Relative Score Fusion (weights scores).  </li> <li>Distribution-Based Fusion (probability-based merging).  </li> </ul> <p>\ud83d\udc49 Analogy: Imagine retrievers as different librarians. Some look for meaning (VectorStore), some skim summaries (DocumentSummary), some check exact words (BM25), and some combine multiple librarians\u2019 answers (Query Fusion).  </p>"},{"location":"LLM_RAG_notes/RAG_Retrievers_LlamaIndex_Notes/#advanced-practitioner-notes-expert-level","title":"\u2699\ufe0f Advanced Practitioner Notes (Expert Level)","text":"<p>For those building modular RAG pipelines, here\u2019s how these retrievers fit strategically:  </p> <ul> <li>VectorStoreIndex </li> <li>Strength: Semantic retrieval at scale.  </li> <li>Weakness: Embedding drift if documents are updated often.  </li> <li> <p>Best use: General-purpose RAG pipelines with LLMs.  </p> </li> <li> <p>DocumentSummaryIndex </p> </li> <li>Strength: Efficient filtering for large heterogeneous corpora.  </li> <li>Weakness: Summaries may lose nuance.  </li> <li> <p>Best use: Pre-filtering before deep semantic search.  </p> </li> <li> <p>KeywordTableIndex / BM25 Retriever </p> </li> <li>Strength: Deterministic, rule-based retrieval.  </li> <li>Weakness: No semantic flexibility.  </li> <li> <p>Best use: Compliance-heavy or keyword-sensitive domains.  </p> </li> <li> <p>Document Summary Index Retriever (LLM vs Semantic) </p> </li> <li>LLM-based: Better contextual reasoning, but higher cost/latency.  </li> <li>Semantic similarity-based: Faster, cheaper, but less nuanced.  </li> <li> <p>Best use: Choose based on budget vs accuracy trade-off.  </p> </li> <li> <p>Auto Merging Retriever </p> </li> <li>Strength: Preserves hierarchical context in long docs.  </li> <li>Weakness: Complexity in chunking strategy.  </li> <li> <p>Best use: Legal, technical, or hierarchical documents (contracts, manuals).  </p> </li> <li> <p>Recursive Retriever </p> </li> <li>Strength: Exploits graph-like relationships (citations, metadata).  </li> <li>Weakness: Requires well-structured references.  </li> <li> <p>Best use: Academic papers, linked datasets.  </p> </li> <li> <p>Query Fusion Retriever </p> </li> <li>Strength: Hybrid retrieval combining multiple signals.  </li> <li>Weakness: Fusion tuning required.  </li> <li>Best use: Multi-retriever pipelines where semantic + keyword + summary retrieval all matter.  </li> </ul>"},{"location":"LLM_RAG_notes/RAG_Retrievers_LlamaIndex_Notes/#quick-reference-cheat-sheet","title":"\ud83d\udccc Quick Reference Cheat Sheet","text":"Retriever Type Core Mechanism Best Use Case VectorStoreIndex Embeddings (semantic similarity) General RAG pipelines DocumentSummaryIndex Summaries for filtering Large diverse corpora KeywordTableIndex Keyword mapping Rule-based search VectorIndexRetriever Embedding-based retrieval Semantic search BM25 Retriever Keyword ranking Exact keyword match Doc Summary Retriever Summaries (LLM or semantic) Fast filtering Auto Merging Retriever Hierarchical chunking Long structured docs Recursive Retriever Node relationships Academic/linked data Query Fusion Retriever Fusion strategies Hybrid retrieval <p>\u2728 With this layered view: - A newbie gets the librarian analogy and simple definitions. - An advanced practitioner gets trade-offs, workflow hygiene, and integration strategies.  </p> <pre><code>RAG &amp; LlamaIndex Retrievers\n\u2502\n\u251c\u2500\u2500 Vector-based\n\u2502   \u251c\u2500\u2500 VectorStoreIndex              # Stores embeddings of documents for similarity search\n\u2502   \u2514\u2500\u2500 VectorIndexRetriever          # Retrieves docs by nearest-neighbor search in embedding space\n\u2502\n\u251c\u2500\u2500 Keyword-based\n\u2502   \u251c\u2500\u2500 KeywordTableIndex             # Indexes docs by keyword occurrences (like inverted index)\n\u2502   \u2514\u2500\u2500 BM25 Retriever                # Classic IR algorithm scoring keyword relevance with term frequency &amp; doc length\n\u2502\n\u251c\u2500\u2500 Summary-based\n\u2502   \u251c\u2500\u2500 DocumentSummaryIndex          # Stores LLM-generated summaries of docs for lightweight retrieval\n\u2502   \u2514\u2500\u2500 Document Summary Index Retriever\n\u2502       \u251c\u2500\u2500 LLM-based                 # Uses LLM reasoning over summaries to pick relevant docs\n\u2502       \u2514\u2500\u2500 Semantic similarity-based # Compares query vs summaries using embeddings for match\n\u2502\n\u251c\u2500\u2500 Hierarchical / Graph-based\n\u2502   \u251c\u2500\u2500 Auto Merging Retriever        # Dynamically merges smaller chunks into larger ones for context-aware retrieval\n\u2502   \u2514\u2500\u2500 Recursive Retriever           # Traverses hierarchical graph (e.g., section \u2192 chapter \u2192 doc) to refine results\n\u2502\n\u2514\u2500\u2500 Fusion-based\n    \u2514\u2500\u2500 Query Fusion Retriever\n        \u251c\u2500\u2500 Reciprocal Rank Fusion    # Combines ranked lists from multiple retrievers by reciprocal rank weighting\n        \u251c\u2500\u2500 Relative Score Fusion     # Normalizes scores across retrievers and merges based on relative strength\n        \u2514\u2500\u2500 Distribution-Based Fusion # Uses statistical distribution of scores to balance multiple retrievers\n\n</code></pre>"},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/","title":"\ud83e\udde9 RAG Workflow Cheat Sheet","text":""},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/#1-loading-documents","title":"1. \ud83d\udcc4 Loading Documents","text":"<ul> <li> <p>What it is: Ingesting raw data (PDFs, text files, web pages, etc.) into your pipeline.</p> </li> <li> <p>Tools: PyPDFLoader, TextLoader, UnstructuredLoader, etc.</p> </li> <li> <p>Use-case: Load a company\u2019s policy PDF so the LLM can answer HR-related queries.</p> </li> </ul> <p>Code Example:</p> <pre><code>from langchain.document_loaders import PyPDFLoader\n\nloader = PyPDFLoader(\"company_handbook.pdf\")\ndocuments = loader.load()\n</code></pre> <p>Parameters to tune: - File type loader (<code>PyPDFLoader</code>, <code>TextLoader</code>, <code>UnstructuredLoader</code>) \u2192 depends on source format. - Pre-cleaning options (remove headers, normalize whitespace) \u2192 improves chunk quality.</p> <p>Notes: Choose loaders that preserve structure. For PDFs, some loaders extract text line-by-line, others preserve layout.</p>"},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/#2-chunking","title":"2. \u2702\ufe0f Chunking","text":"<ul> <li> <p>What it is: Splitting documents into smaller pieces for embedding.</p> </li> <li> <p>Parameters:</p> <ul> <li>chunk_size: Number of characters/tokens per chunk (e.g., 500\u20131000).</li> <li>chunk_overlap: Overlap between chunks (e.g., 50\u2013100) to preserve context.</li> </ul> </li> <li> <p>Why: Embedding models have input size limits; smaller chunks improve retrieval accuracy.</p> </li> <li> <p>Use-case: Split a 50-page research paper into 500-character chunks with 50 overlap so queries don\u2019t miss context across page breaks. Code Example:</p> </li> </ul> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=800,      # size of each chunk\n    chunk_overlap=100,   # overlap between chunks\n    length_function=len  # how length is measured\n)\nchunks = text_splitter.split_documents(documents)\n</code></pre> <p>Parameters to tune: - <code>chunk_size</code> \u2192 Larger chunks = more context, but risk exceeding embedding limits. - <code>chunk_overlap</code> \u2192 Prevents context loss across boundaries. - <code>length_function</code> \u2192 Can be <code>len</code> (characters) or token-based (using tokenizer). - Granularity strategy \u2192 Sentence-based vs character-based splitting.</p> <p>Notes: Balance between retrieval precision and context completeness. For legal docs, use smaller chunks with overlap; for FAQs, larger chunks may suffice.</p>"},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/#3-embeddings","title":"3. \ud83d\udd22 Embeddings","text":"<ul> <li> <p>What it is: Converting text chunks into numerical vectors that capture semantic meaning.</p> </li> <li> <p>Models:</p> </li> <li> <p>OpenAI (text-embedding-ada-002)</p> </li> <li> <p>HuggingFace (sentence-transformers)</p> </li> <li> <p>IBM watsonx embeddings</p> </li> <li> <p>Parameters:</p> </li> <li> <p>dimension (size of vector, e.g., 768 or 1536)</p> </li> <li> <p>model choice (trade-off: speed vs accuracy)</p> </li> <li> <p>Use-case: Represent customer support FAQs as embeddings so queries like \u201cHow do I reset my password?\u201d match semantically similar answers. Code Example:</p> </li> </ul> <pre><code>from langchain.embeddings import OpenAIEmbeddings\n\nembedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\nvector = embedding_model.embed_query(\"reset password instructions\")\n</code></pre> <p>Parameters to tune: - Model choice \u2192 <code>ada-002</code> (fast, cheap), <code>text-embedding-3-large</code> (higher accuracy). - Embedding dimension \u2192 512, 768, 1536 depending on model. - Batch size \u2192 Controls throughput when embedding many chunks. - Normalization \u2192 Some DBs require normalized vectors (unit length).</p> <p>Notes: Higher dimension = richer representation but heavier storage. Always align DB dimension with embedding model output.</p>"},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/#4-vector-databases","title":"4. \ud83d\uddc4\ufe0f Vector Databases","text":"<ul> <li> <p>What it is: Specialized DBs for storing and searching embeddings.</p> </li> <li> <p>How different from traditional DBs:</p> </li> <li> <p>Traditional DB \u2192 exact match (SQL queries).</p> </li> <li> <p>Vector DB \u2192 similarity search (nearest neighbors in vector space).</p> </li> <li> <p>Types / Examples:</p> </li> <li> <p>Local: FAISS, Annoy, Milvus</p> </li> <li> <p>Cloud: Pinecone, Weaviate, ChromaDB</p> </li> <li> <p>Use-case: Store embeddings of product manuals in Pinecone so engineers can query \u201cerror code 404\u201d and retrieve relevant troubleshooting steps. Code Example (ChromaDB):</p> </li> </ul> <pre><code>from langchain.vectorstores import Chroma\n\nvectordb = Chroma.from_documents(documents=chunks, embedding=embedding_model)\n</code></pre> <p>Parameters to tune: - Distance metric \u2192 cosine, dot product, Euclidean. - Persistence \u2192 in-memory vs disk-backed. - Index type (FAISS: Flat, IVF, HNSW) \u2192 trade-off between speed and accuracy. - Replication / sharding (cloud DBs like Pinecone) \u2192 scale for large datasets.</p> <p>Notes: Cosine similarity is common for text embeddings. IVF/HNSW indexes accelerate search but may approximate results.</p>"},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/#5-retrievers","title":"5. \ud83d\udd0d Retrievers","text":"<ul> <li> <p>What it is: A wrapper around the vector DB that defines how to fetch relevant chunks.</p> </li> <li> <p>Types:</p> </li> <li> <p>Single-query retriever: Basic similarity search.</p> <ul> <li>Use-case: Simple Q&amp;A over one document.</li> </ul> </li> <li> <p>Multi-query retriever: Expands query into multiple variations.</p> <ul> <li>Use-case: User asks \u201cAI courses\u201d \u2192 expands to \u201cmachine learning courses,\u201d \u201cdeep learning classes.\u201d</li> </ul> </li> <li> <p>MMR (Maximal Marginal Relevance): Balances relevance + diversity.</p> <ul> <li>Use-case: Avoids retrieving 5 nearly identical chunks, ensures broader coverage.</li> </ul> </li> <li> <p>Parent-document retriever: Retrieves the parent doc when a chunk is matched.</p> <ul> <li>Use-case: Legal contracts or Research papers where context across clauses matters.</li> </ul> </li> </ul> <p>Code Example:</p> <pre><code>retriever = vectordb.as_retriever(\n    search_type=\"mmr\",   # retrieval strategy\n    search_kwargs={\"k\": 5, \"lambda_mult\": 0.5}\n)\n</code></pre> <p>Parameters to tune: - <code>k</code> \u2192 Number of chunks retrieved. Larger <code>k</code> = more coverage, but risk of noise. - MMR diversity (<code>lambda_mult</code>) \u2192 Controls balance between relevance and diversity. - Multi-query expansion size \u2192 Number of query variations generated. - Parent-document retriever window size \u2192 How much of parent doc is returned.</p> <p>Notes: - Use MMR when you want diverse evidence (research, multi-topic queries). - Use multi-query retriever when queries are ambiguous. - Use parent-doc retriever for structured docs (contracts, manuals).</p>"},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/#6-retrievalqa","title":"6. \ud83e\udd16 RetrievalQA","text":"<ul> <li> <p>What it is: Combines retriever + LLM to answer queries with external knowledge.</p> </li> <li> <p>Components:</p> </li> <li> <p>Retriever: Supplies relevant chunks.</p> </li> <li> <p>LLM: Generates final answer.</p> </li> <li> <p>Memory (optional): Keeps track of conversation history.</p> </li> <li> <p>Use-case:</p> </li> <li> <p>Customer support chatbot \u2192 retrieves product manual chunks, LLM answers naturally.</p> </li> <li> <p>Research assistant \u2192 retrieves academic papers, LLM summarizes findings.</p> </li> </ul> <p>Code Example:</p> <pre><code>from langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=OpenAI(model=\"gpt-4\"),\n    retriever=retriever,\n    return_source_documents=True\n)\n\nresponse = qa_chain.run(\"What is the parental leave policy?\")\n</code></pre> <p>Parameters to tune: - LLM choice \u2192 GPT-4 (accuracy), GPT-3.5 (speed/cost). - Return source docs \u2192 Useful for transparency. - Memory type \u2192 ConversationBufferMemory, SummaryMemory. - Temperature \u2192 Controls creativity vs factuality. - Max tokens \u2192 Controls length of generated answer.</p> <p>Notes: For factual Q&amp;A, keep temperature low (0\u20130.3). For brainstorming, increase it. Always return sources for trustworthiness.</p>"},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/#practical-parameters-cheat-sheet","title":"\u2699\ufe0f Practical Parameters Cheat Sheet","text":"Step Key Parameters Why They Matter Chunking <code>chunk_size</code>, <code>chunk_overlap</code>, <code>length_function</code> Balance context vs precision Embeddings <code>model</code>, <code>dimension</code>, <code>batch_size</code>, <code>normalize</code> Accuracy vs cost/storage Vector DB <code>distance_metric</code>, <code>index_type</code>, <code>persistence</code> Search speed vs accuracy Retriever <code>k</code>, <code>lambda_mult</code>, <code>multi-query size</code>, <code>parent window</code> Relevance vs diversity RetrievalQA <code>llm</code>, <code>temperature</code>, <code>max_tokens</code>, <code>memory</code> Answer quality, transparency"},{"location":"LLM_RAG_notes/RAG_workflow_cheatsheet/#end-to-end-example-with-tuned-parameters","title":"\ud83d\udee0\ufe0f End-to-End Example (with tuned parameters)","text":"<pre><code># Load\nloader = PyPDFLoader(\"handbook.pdf\")\ndocs = loader.load()\n\n# Chunk\nsplitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\nchunks = splitter.split_documents(docs)\n\n# Embed\nembedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n\n# Vector DB\nvectordb = Chroma.from_documents(chunks, embedding_model)\n\n# Retriever (MMR with diversity)\nretriever = vectordb.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\"k\": 8, \"lambda_mult\": 0.7}\n)\n\n# RetrievalQA\nqa_chain = RetrievalQA.from_chain_type(\n    llm=OpenAI(model=\"gpt-4\", temperature=0.2),\n    retriever=retriever,\n    return_source_documents=True\n)\n\nanswer = qa_chain.run(\"Explain parental leave policy\")\n</code></pre>"},{"location":"LLM_RAG_notes/basicQA_llm_transformer/","title":"Basics of LLM","text":""},{"location":"LLM_RAG_notes/basicQA_llm_transformer/#machine-learning-llm-fundamentals-quiz","title":"\ud83e\udd16 Machine Learning + LLM Fundamentals Quiz","text":""},{"location":"LLM_RAG_notes/basicQA_llm_transformer/#i-machine-learning-fundamentals","title":"I. Machine Learning Fundamentals","text":"<ol> <li> <p>What is the primary difference between supervised learning and unsupervised learning?</p> <ul> <li>Answer: Supervised learning uses labeled data (input-output pairs) to train a model to predict the output for new inputs. Unsupervised learning uses unlabeled data to find patterns, structures, or relationships within the data, such as clustering or dimensionality reduction.</li> </ul> </li> <li> <p>Define the term \"overfitting\" and state one common technique used to mitigate it.</p> <ul> <li>Answer: Overfitting occurs when a model learns the training data and noise too well, resulting in high accuracy on the training set but poor generalization (low accuracy) on unseen data (test set).</li> <li>Mitigation Technique: Regularization (L1 or L2), Dropout, Early Stopping, or Cross-Validation. </li> </ul> </li> </ol> <p>[Image of overfitting and underfitting in machine learning]</p> <ol> <li>In the context of classification, what do the terms Precision and Recall measure?<ul> <li>Answer:<ul> <li>Precision measures the proportion of positive predictions that were actually correct (out of all positive predictions made by the model).</li> <li>Recall measures the proportion of actual positive cases that were correctly identified (out of all actual positive cases).</li> </ul> </li> </ul> </li> </ol>"},{"location":"LLM_RAG_notes/basicQA_llm_transformer/#ii-transformer-architecture","title":"II. Transformer Architecture","text":"<ol> <li> <p>What is the core, groundbreaking mechanism that allows the Transformer model to process sequences without relying on recurrence (like RNNs or LSTMs)?</p> <ul> <li>Answer: The Self-Attention mechanism (specifically, Scaled Dot-Product Attention). </li> </ul> </li> <li> <p>Explain the purpose of Positional Encoding in the Transformer architecture.</p> <ul> <li>Answer: Since the self-attention mechanism processes all tokens simultaneously without an inherent order, Positional Encoding is added to the input embeddings to inject information about the position (or order) of each token in the sequence.</li> </ul> </li> <li> <p>A single Transformer block consists of two main sub-layers. What are they?</p> <ul> <li>Answer:<ol> <li>A Multi-Head Self-Attention mechanism.</li> <li>A simple, position-wise Feed-Forward Network (FFN). (Note: Each sub-layer is typically followed by a Residual Connection and Layer Normalization).</li> </ol> </li> </ul> </li> </ol>"},{"location":"LLM_RAG_notes/basicQA_llm_transformer/#iii-llm-fundamentals","title":"III. LLM Fundamentals","text":"<ol> <li> <p>What does the acronym LLM stand for, and what is the primary learning paradigm used to train these models?</p> <ul> <li>Answer: LLM stands for Large Language Model. The primary learning paradigm is Self-Supervised Learning on massive amounts of text data, usually through a next-word prediction (causal language modeling) or masked language modeling objective.</li> </ul> </li> <li> <p>In an LLM, what is \"In-Context Learning\" (ICL), and how is it achieved?</p> <ul> <li>Answer: In-Context Learning (ICL) is the ability of a pre-trained LLM (like GPT-3 or Gemini) to learn a new task and generate desired output simply by being provided with a few examples or instructions within the prompt itself, without requiring a formal weight update (fine-tuning). It is a property that emerges from the scale of the model and data.</li> </ul> </li> <li> <p>What are the three common modes or forms of attention masks used in different LLM training or use cases (e.g., in a decoder, encoder, or encoder-decoder setup)?</p> <ul> <li>Answer:<ol> <li>Full/Bidirectional Mask (Encoder): Allows a token to attend to all other tokens (before and after it) in the sequence.</li> <li>Causal/Look-ahead Mask (Decoder/Generative LLMs): Prevents a token from attending to any subsequent tokens, ensuring that the prediction of a word is only based on the words that came before it.</li> <li>Cross-Attention (Encoder-Decoder): Allows the tokens in the decoder to attend to all tokens in the encoder's output.</li> </ol> </li> </ul> </li> </ol>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/","title":"LLM decoding strategy","text":""},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#workflow","title":"\ud83c\udfd7\ufe0f Workflow","text":""},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#select-a-foundation-model","title":"Select a Foundation Model","text":"<ul> <li>Examples: <code>mistralai/mistral-small</code>, <code>meta-llama/Llama-3</code>, <code>gpt-j</code>, etc.</li> <li>Consider:</li> <li>Size vs. latency (small models for speed, large for accuracy).</li> <li>Domain fit (general-purpose vs. fine-tuned).</li> <li>Hosting (IBM Watsonx, Hugging Face, Azure, local inference).</li> </ul>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#configure-model-parameters","title":"Configure Model Parameters","text":"<ul> <li>Common parameters:</li> <li><code>MAX_NEW_TOKENS</code>: controls output length.</li> <li><code>TEMPERATURE</code>: randomness/creativity.</li> <li><code>TOP_K</code> / <code>TOP_P</code>: sampling constraints.</li> <li><code>DECODING_METHOD</code>: greedy, sample, beam search.</li> <li>Credentials &amp; project setup (e.g., Watsonx API keys, project IDs).</li> </ul>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#wrap-with-langchain-or-orchestration-layer","title":"Wrap with LangChain (or Orchestration Layer)","text":"<ul> <li>Purpose: abstract the raw model into a LangChain LLM interface.</li> <li>Benefits:</li> <li>Unified API across different providers.</li> <li>Easy chaining with tools (retrievers, agents, memory).</li> <li>Plug-and-play with pipelines (QA bots, RAG, structured workflows).</li> <li>Example:</li> </ul> <pre><code>from langchain_ibm import WatsonxLLM\nfrom ibm_watsonx_ai.foundation_models import ModelInference, GenParams\n\nmodel = ModelInference(\n    model_id=\"mistralai/mistral-small\",\n    params={\n        GenParams.MAX_NEW_TOKENS: 256,\n        GenParams.TEMPERATURE: 0.5,\n    },\n    credentials={\"url\": \"https://us-south.ml.cloud.ibm.com\"},\n    project_id=\"skills-network\"\n)\n\nllm = WatsonxLLM(model=model)\n</code></pre>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#integrate-into-workflow","title":"Integrate into Workflow","text":"<ul> <li>Prompt engineering: design input prompts.</li> <li>Chains/Agents: combine LLM with retrievers, tools, memory.</li> <li>Evaluation: test determinism vs. creativity depending on decoding.</li> </ul>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#decoding-strategies-after-wrapping","title":"\ud83d\udd11 Decoding Strategies (After Wrapping)","text":"<p>(same section as before, now positioned after the setup)</p> Method How it Works Pros Cons Best Use Cases Greedy Always picks highest-probability token Deterministic, reproducible Bland, repetitive Structured extraction, factual QA Sampling Randomly samples with temperature Creative, diverse Non-deterministic Brainstorming, storytelling Top-k Samples from top k tokens Balance diversity/coherence Needs tuning Creative writing Top-p Samples from cumulative probability mass Adaptive diversity Needs tuning Dialogue, open-ended tasks Beam Search Explores multiple sequences Higher-quality Expensive, less diverse Translation, summarization"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#rule-of-thumb","title":"\ud83d\udccc Rule of Thumb","text":"<ul> <li>Foundation model choice \u2192 determines baseline capability.</li> <li>LangChain wrapping \u2192 enables orchestration and workflow hygiene.</li> <li>Decoding strategy \u2192 tailors output style (deterministic vs. creative).</li> </ul>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#quick-takeaway","title":"\ud83c\udfaf Quick Takeaway","text":"<p>Think of it as a three-layer stack: 1. Foundation model (raw capability). 2. Wrapper (LangChain LLM) (workflow orchestration). 3. Decoding strategy (output style).  </p>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#parameter-interactions","title":"\u2699\ufe0f Parameter Interactions","text":"<ul> <li>Temperature</li> <li><code>0</code> \u2192 collapses sampling into greedy decoding.</li> <li><code>&gt;0</code> \u2192 introduces randomness.</li> <li>DECODING_METHOD</li> <li><code>\"greedy\"</code> \u2192 forces deterministic decoding.</li> <li><code>\"sample\"</code> \u2192 uses temperature/top-k/top-p.</li> <li>Max Tokens</li> <li>Controls length, independent of decoding method.</li> </ul>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#rule-of-thumb_1","title":"\ud83d\udccc Rule of Thumb","text":"<ul> <li>Use Greedy when:</li> <li>You need deterministic, reproducible outputs.</li> <li>Tasks are factual, structured, or extractive.</li> <li>Use Sampling when:</li> <li>You want creative, varied outputs.</li> <li>Tasks are open-ended, conversational, or generative.</li> </ul>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#example-configurations","title":"\ud83d\udee0\ufe0f Example Configurations","text":""},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#greedy-decoding","title":"Greedy Decoding","text":"<pre><code>parameters = {\n    GenParams.MAX_NEW_TOKENS: 256,\n    GenParams.TEMPERATURE: 0,\n    GenParams.DECODING_METHOD: \"greedy\"\n}\n</code></pre>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#sampling-with-temperature","title":"Sampling with Temperature","text":"<pre><code>parameters = {\n    GenParams.MAX_NEW_TOKENS: 256,\n    GenParams.TEMPERATURE: 0.7,\n    GenParams.DECODING_METHOD: \"sample\"\n}\n</code></pre>"},{"location":"LLM_RAG_notes/llm_decoding_strategy_notes/#quick-takeaway_1","title":"\ud83c\udfaf Quick Takeaway","text":"<ul> <li>Greedy = reproducibility </li> <li>Sampling = creativity </li> <li>Explicitly set <code>DECODING_METHOD</code> when you want control.  </li> <li>If you don\u2019t set it, the system defaults to sampling (since you provided <code>temperature &gt; 0</code>).  </li> </ul>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/","title":"\ud83d\udcdd TF\u2011IDF Reference Note","text":"<p>TF\u2011IDF emphasizes word relevancy. In document search, it helps identify which documents are most relevant to a user\u2019s query by highlighting words that are distinctive to specific documents rather than common across all.</p>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#why-tfidf-is-useful-in-search","title":"\ud83d\udd0e Why TF\u2011IDF is Useful in Search","text":"<ul> <li> <p>Balances frequency and distinctiveness: Common words like \u201cthe\u201d or \u201cand\u201d are down\u2011weighted, while rare but meaningful words like \u201cclimate\u201d or \u201cinflation\u201d are up\u2011weighted.</p> </li> <li> <p>Improves ranking: Documents where query terms have high TF\u2011IDF scores are ranked higher because those terms are both frequent and distinctive in those documents.</p> </li> <li> <p>Contextual relevance: A word\u2019s importance is judged relative to the entire corpus, so search results reflect not just raw frequency but discriminative power.</p> </li> </ul>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#1-core-definitions","title":"1. Core Definitions","text":"<ul> <li>Term Frequency (TF):   Frequency of a word within a single document. High if the word appears often in that document.</li> <li>Document Frequency (DF):   Number of documents in the corpus that contain the word at least once.  </li> <li>High DF \u2192 word appears in many documents.  </li> <li>Low DF \u2192 word appears in few documents.</li> <li>Inverse Document Frequency (IDF):   Measures rarity across documents:   [   IDF = \\log\\left(\\frac{N}{DF}\\right)   ]   where (N) = total number of documents.  </li> <li>Low DF \u2192 High IDF.  </li> <li>High DF \u2192 Low IDF.</li> <li>TF\u2011IDF Score:   [   TF\\text{-}IDF = TF \\times IDF   ]   Highlights words that are frequent in one document but rare across the corpus.</li> </ul>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#2-interpretation","title":"2. Interpretation","text":"<ul> <li>High TF\u2011IDF Score: </li> <li>Word is frequent in a specific document.  </li> <li>Word is rare across other documents.  </li> <li>Meaning \u2192 the word is distinctive or signature for that document.</li> <li>Low TF\u2011IDF Score: </li> <li>Word appears across many documents.  </li> <li>Meaning \u2192 the word is common background noise (e.g., \u201cthe\u201d, \u201cand\u201d).</li> </ul>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#3-examples","title":"3. Examples","text":""},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#case-a-5-books","title":"\ud83d\udcda Case A: 5 Books","text":"<ul> <li>Word \u201cclimate\u201d appears 50 times in Book A, absent in others.  </li> <li>TF (Book A) = high.  </li> <li>DF = 1 \u2192 IDF = high.  </li> <li>TF\u2011IDF (Book A) = high.  </li> <li>TF\u2011IDF (Books B\u2013E) = 0.</li> </ul>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#case-b-1-book-with-5-pages","title":"\ud83d\udcc4 Case B: 1 Book with 5 Pages","text":"<ul> <li>Treat each page as a document.  </li> <li>If \u201cclimate\u201d appears only on Page 1:  </li> <li>DF = 1 (out of 5).  </li> <li>IDF = high.  </li> <li>TF\u2011IDF (Page 1) = high.  </li> <li>TF\u2011IDF (Pages 2\u20135) = 0.</li> </ul>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#case-c-news-articles","title":"\ud83d\udcf0 Case C: News Articles","text":"<ul> <li>Corpus = 100 articles.  </li> <li>Word \u201cinflation\u201d appears in 80 articles.  </li> <li>DF = 80 \u2192 IDF = low.  </li> <li>Even if TF is high in one article, TF\u2011IDF score is lower because the word is common.</li> </ul>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#4-rule-of-thumb","title":"4. Rule of Thumb","text":"<ul> <li>High score \u2192 specific to one document. </li> <li> <p>Low score \u2192 common across all documents.</p> </li> <li> <p>High TF\u2011IDF score \u2192 query word is highly relevant to that document.</p> </li> <li> <p>Low TF\u2011IDF score \u2192 query word is common across many documents (e.g. the, and), less useful for distinguishing relevance.</p> </li> <li> <p>Granularity matters: </p> </li> <li>If you treat each book as a document \u2192 TF\u2011IDF highlights words distinctive to each book.  </li> <li>If you treat each page as a document \u2192 TF\u2011IDF highlights words distinctive to each page.</li> </ul>"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#tfidf-example-table","title":"\ud83d\udcca TF\u2011IDF Example Table","text":"Document Term Frequency (TF) of \u201cclimate\u201d Document Frequency (DF) across corpus Inverse Document Frequency (IDF) TF\u2011IDF Score Interpretation Book A 30 DF = 2 (appears in Book A &amp; Book B) log(5/2) \u2248 0.92 27.6 High \u2192 \u201cclimate\u201d is distinctive for Book A Book B 2 DF = 2 log(5/2) \u2248 0.92 1.84 Low \u2192 word appears but not strongly distinctive Book C 0 DF = 2 log(5/2) \u2248 0.92 0 Absent \u2192 no relevance Book D 0 DF = 2 log(5/2) \u2248 0.92 0 Absent \u2192 no relevance Book E 0 DF = 2 log(5/2) \u2248 0.92 0 Absent \u2192 no relevance"},{"location":"LLM_RAG_notes/tf-idf_score_why_how/#key-takeaways","title":"\ud83d\udd0e Key Takeaways","text":"<ul> <li>Book A: High TF \u00d7 High IDF \u2192 high TF\u2011IDF score \u2192 \u201cclimate\u201d is a signature word for Book A.  </li> <li>Book B: Low TF \u00d7 High IDF \u2192 low TF\u2011IDF score \u2192 \u201cclimate\u201d is present but not distinctive.  </li> <li>Books C\u2013E: TF = 0 \u2192 TF\u2011IDF = 0 \u2192 \u201cclimate\u201d is irrelevant.  </li> </ul> <p>\u2705 This table shows how the same word gets different TF\u2011IDF scores depending on the document. It\u2019s high where the word is frequent and rare elsewhere, and low where the word is common or absent.</p>"},{"location":"MLOps/devops_notes/","title":"\ud83d\udcc1 MLOps Basics: Docker, FastAPI, and Flask","text":"<p>Welcome! This guide explains essential backend tools:</p> <ul> <li>\ud83d\udea2 Docker: Package and run your apps anywhere</li> <li>\u26a1 FastAPI: Fast, async-ready web APIs</li> <li>\ud83d\udd25 Flask: Lightweight web apps and APIs</li> </ul>"},{"location":"MLOps/devops_notes/#docker","title":"\ud83d\udea2 Docker","text":""},{"location":"MLOps/devops_notes/#what-is-docker","title":"What is Docker?","text":"<p>Docker is like a magic container box that carries your app and everything it needs (code, Python, libraries) so it runs anywhere - your laptop, server, or the cloud.</p>"},{"location":"MLOps/devops_notes/#when-to-use-docker","title":"When to Use Docker","text":"<ul> <li>You want to share your project easily with someone else.</li> <li>Your app works on your machine, but not on someone else's.</li> <li>You're deploying your app to a cloud service.</li> </ul>"},{"location":"MLOps/devops_notes/#key-concepts","title":"\ud83d\udd0d Key Concepts","text":"Term What It Means Image A snapshot/blueprint of your app and everything it needs Container A running instance of an image Dockerfile A script with steps to create an image"},{"location":"MLOps/devops_notes/#sample-dockerfile","title":"\ud83d\udcc2 Sample Dockerfile","text":"<pre><code># Use the official Python image from Docker Hub\nFROM python:3.10\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the requirements file into the container\nCOPY requirements.txt .\n\n# Install the required Python packages\nRUN pip install -r requirements.txt\n\n# Copy the rest of the application code\nCOPY . .\n\n# Set the default command to run the app\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"MLOps/devops_notes/#docker-commands","title":"\u2696\ufe0f Docker Commands","text":""},{"location":"MLOps/devops_notes/#build-docker-image","title":"Build Docker Image:","text":"<pre><code>docker build -t myapp .\n</code></pre> <ul> <li><code>-t myapp</code>: names the image <code>myapp</code></li> <li><code>.</code>: current directory contains the Dockerfile</li> </ul>"},{"location":"MLOps/devops_notes/#run-docker-container","title":"Run Docker Container:","text":"<pre><code>docker run -p 8000:8000 myapp\n</code></pre> <ul> <li><code>-p 8000:8000</code>: maps your computer's port 8000 to Docker's port 8000</li> </ul>"},{"location":"MLOps/devops_notes/#run-python-shell-inside-docker","title":"Run Python Shell Inside Docker","text":"<pre><code>docker run -it python:3.10\n</code></pre> <ul> <li><code>-it</code>: interactive terminal</li> <li>Opens Python shell inside a container</li> </ul>"},{"location":"MLOps/devops_notes/#mount-local-folder-into-container","title":"Mount Local Folder into Container","text":"<pre><code>docker run -v $(pwd):/app -w /app python:3.10 python app.py\n</code></pre> <ul> <li><code>-v $(pwd):/app</code>: mounts your current folder to <code>/app</code> inside container</li> <li><code>-w /app</code>: sets the working directory</li> </ul>"},{"location":"MLOps/devops_notes/#fastapi","title":"\u26a1 FastAPI","text":""},{"location":"MLOps/devops_notes/#what-is-fastapi","title":"\ud83e\udd14 What is FastAPI?","text":"<p>FastAPI is a Python web framework used to build APIs that are fast, async, and come with automatic documentation.</p> <p>FastAPI is a fast, modern way to build APIs with Python. It's like building an express highway \ud83d\ude97 for your data.</p>"},{"location":"MLOps/devops_notes/#when-to-use-fastapi","title":"When to Use FastAPI","text":"<ul> <li>You need a REST API for your app or machine learning model.</li> <li>You want built-in docs (OpenAPI, Swagger).</li> <li>You care about speed and async support.</li> </ul>"},{"location":"MLOps/devops_notes/#how-to-use","title":"How to Use","text":""},{"location":"MLOps/devops_notes/#step-1-install-fastapi-and-uvicorn","title":"Step 1: Install FastAPI and Uvicorn","text":"<pre><code>pip install fastapi uvicorn\n</code></pre>"},{"location":"MLOps/devops_notes/#step-2-create-mainpy","title":"Step 2: Create main.py","text":"<pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Hello FastAPI\"}\n</code></pre>"},{"location":"MLOps/devops_notes/#step-3-run-the-app","title":"Step 3: Run the App","text":"<pre><code>uvicorn main:app --reload\n</code></pre>"},{"location":"MLOps/devops_notes/#example-use-case","title":"Example Use Case","text":"<ul> <li>You built a machine learning model and want to expose a <code>/predict</code> endpoint for it. FastAPI makes this super easy.</li> </ul> Term What It Means API A way to connect programs (like a menu at a restaurant) Endpoint A specific URL that your app listens to (e.g. <code>/predict</code>) GET/POST Types of HTTP requests (read vs. send data) Pydantic Helps validate data like forms with rules Uvicorn The web server that runs FastAPI"},{"location":"MLOps/devops_notes/#a-sample-mainpy-with-comments","title":"\ud83d\udd39 A Sample <code>main.py</code> with Comments","text":"<pre><code># Import FastAPI library\nfrom fastapi import FastAPI\n\n# Create the FastAPI app instance\napp = FastAPI()\n\n# Define a GET endpoint for the homepage\n@app.get(\"/\")\ndef home():\n    # Return a JSON response\n    return {\"message\": \"Hello FastAPI\"}\n\n# Define a GET endpoint with a path parameter\n@app.get(\"/greet/{name}\")\ndef greet(name: str):\n    return {\"message\": f\"Hello, {name}!\"}\n\n# Use a query parameter with a default value\n@app.get(\"/search\")\ndef search(q: str = \"default\"):\n    return {\"result\": q}\n\n# Use a POST endpoint with data validation\nfrom pydantic import BaseModel\n\nclass Item(BaseModel):\n    name: str\n    price: float\n\n@app.post(\"/items\")\ndef create_item(item: Item):\n    # Pydantic auto-validates the item structure\n    return {\"item\": item}\n</code></pre>"},{"location":"MLOps/devops_notes/#run-fastapi-app","title":"\ud83d\udd27 Run FastAPI App","text":"<pre><code>uvicorn main:app --reload\n</code></pre> <ul> <li><code>main:app</code>: file name = <code>main.py</code>, object = <code>app</code></li> <li><code>--reload</code>: restarts server on code changes</li> </ul>"},{"location":"MLOps/devops_notes/#dockerfile-for-fastapi","title":"\ud83d\udcc2 Dockerfile for FastAPI","text":"<pre><code>FROM python:3.10\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"MLOps/devops_notes/#docker-fastapi-example","title":"\ud83e\uddea Docker + FastAPI Example","text":""},{"location":"MLOps/devops_notes/#requirementstxt","title":"requirements.txt","text":"<pre><code>fastapi\nuvicorn\n</code></pre>"},{"location":"MLOps/devops_notes/#build-and-run","title":"Build and Run","text":"<pre><code>docker build -t fastapi-app .        # Swagger UI\ndocker run -p 8000:8000 fastapi-app  # FastAPI homepage\n</code></pre>"},{"location":"MLOps/devops_notes/#access-fastapi","title":"Access (FastAPI)","text":"<ul> <li> <p>http://localhost:8000/docs \u2192 Swagger UI</p> </li> <li> <p>http://localhost:8000 \u2192 FastAPI homepage</p> </li> </ul>"},{"location":"MLOps/devops_notes/#flask","title":"\ud83d\udd25 Flask","text":""},{"location":"MLOps/devops_notes/#what-is-flask","title":"\ud83e\udd14 What is Flask?","text":"<p>Flask is a minimal Python web framework. You can build anything: a website, API, or dashboard.</p> <p>It\u2019s like a barebones food truck \ud83c\udf54 where you add features as you need.</p> Term Meaning Route A URL that triggers a function Template HTML file rendered dynamically Flask App Main object that connects routes to logic Flask CLI Command line tool to run apps (<code>flask run</code>)"},{"location":"MLOps/devops_notes/#sample-apppy-with-comments","title":"\ud83d\udd39 Sample <code>app.py</code> with Comments","text":"<pre><code># Import Flask class\nfrom flask import Flask, jsonify, render_template\n\n# Create the Flask app instance\napp = Flask(__name__)\n\n# Define a basic homepage route\n@app.route(\"/\")\ndef hello():\n    return \"Hello from Flask!\"\n\n# Define a route with a variable path\n@app.route(\"/hello/&lt;name&gt;\")\ndef greet(name):\n    return f\"Hello, {name}!\"\n\n# Define a JSON API endpoint\n@app.route(\"/api\")\ndef api_data():\n    return jsonify({\"data\": 123})\n\n# Render HTML from a template\n@app.route(\"/home\")\ndef home():\n    return render_template(\"home.html\")\n</code></pre>"},{"location":"MLOps/devops_notes/#run-flask-app","title":"\ud83d\udd27 Run Flask App","text":"<pre><code>export FLASK_APP=app.py\nflask run\n</code></pre>"},{"location":"MLOps/devops_notes/#dockerfile-for-flask","title":"\ud83d\udcc2 Dockerfile for Flask","text":"<pre><code>FROM python:3.10\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nENV FLASK_APP=app.py\nCMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n</code></pre>"},{"location":"MLOps/devops_notes/#docker-flask-example","title":"\ud83e\uddea Docker + Flask Example","text":""},{"location":"MLOps/devops_notes/#requirementstx","title":"requirements.tx","text":"<pre><code>flask\n</code></pre>"},{"location":"MLOps/devops_notes/#build-and-run-flask","title":"Build and Run Flask","text":"<pre><code>docker build -t flask-app .\ndocker run -p 5000:5000 flask-app\n</code></pre> <p>```</p>"},{"location":"MLOps/devops_notes/#access-flask","title":"Access (Flask)","text":""},{"location":"MLOps/devops_notes/#httplocalhost5000-flask-homepage","title":"* http://localhost:5000 \u2192 Flask homepage","text":""},{"location":"MLOps/devops_notes/#summary-table","title":"\ud83d\udcda Summary Table","text":"Tool Purpose Common Use Docker Package apps + environments Make sure apps run the same everywhere FastAPI High-speed web APIs ML models, web APIs with docs Flask Lightweight web framework Quick websites, APIs, dashboards"},{"location":"ML_Pipeline/ml_pipeline/","title":"Production-ready end-to-end layout for a tabular classification ML pipeline","text":"<p>\u2705 Feature Engineering \u2705 Preprocessing (Imputation + Scaling + Encoding) \u2705 Handling Imbalanced Data \u2705 Pipeline \u2705 Cross-Validation \u2705 Evaluation Metrics</p>"},{"location":"ML_Pipeline/ml_pipeline/#1-import-libraries","title":"1. Import libraries","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom imblearn.pipeline import Pipeline as ImbPipeline  # For SMOTE\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#2-load-data","title":"2. Load data","text":"<pre><code>df = pd.read_csv(\"your_dataset.csv\")  # Replace with your real dataset path\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#3-exploratory-data-aanlysis-eda","title":"3. Exploratory Data Aanlysis (EDA)","text":"<pre><code>print(df.shape)\nprint(df.dtypes)\nprint(df.isnull().sum())\nprint(df['target'].value_counts())  # replace column name 'target' as needed\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#3-feature-engineering-custom-transformation","title":"3. Feature Engineering (custom transformation)","text":"<pre><code>def add_custom_features(X):\n    X = X.copy()\n    if \"applicant_income\" in X and \"loan_amount\" in X:\n        X[\"income_to_loan_ratio\"] = X[\"applicant_income\"] / (X[\"loan_amount\"] + 1)\n    return X\n\nfeature_engineering = FunctionTransformer(add_custom_features, validate=False)\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#4-define-features-and-target-x-y","title":"4. Define Features and target: X, y","text":"<pre><code>X = df.drop(\"target\", axis=1)\ny = df[\"target\"]\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#5-train-test-split-with-stratify","title":"5. Train-Test Split With Stratify","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#6-separate-numeric-and-categorical-features","title":"6. Separate Numeric and Categorical Features","text":"<pre><code>num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\n# Add engineered column name to num_cols if added\n\nif \"income_to_loan_ratio\" not in num_cols and \"income_to_loan_ratio\" in X_train.columns:\n    num_cols.append(\"income_to_loan_ratio\")\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#7-define-transformer","title":"7. Define Transformer","text":""},{"location":"ML_Pipeline/ml_pipeline/#numerical-transformer","title":"Numerical Transformer","text":"<pre><code>num_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", StandardScaler())\n])\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#categorical-transformer","title":"Categorical Transformer","text":"<pre><code>cat_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#8-combine-with-columntransformer","title":"8. Combine with ColumnTransformer","text":"<pre><code>preprocessor = ColumnTransformer(transformers=[\n    (\"num\", num_pipeline, num_cols),\n    (\"cat\", cat_pipeline, cat_cols)\n])\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#9-model-hyperparameter-setup","title":"9. Model &amp; Hyperparameter Setup","text":"<pre><code>param_grids = {\n    \"LogisticRegression\": {\n        \"classifier\": [LogisticRegression(max_iter=1000, class_weight=\"balanced\")],\n        \"classifier__C\": [0.1, 1, 10]\n    },\n\n    \"RandomForest\": {\n        \"classifier\": [RandomForestClassifier(class_weight=\"balanced\")],\n        \"classifier__n_estimators\": [100, 200],\n        \"classifier__max_depth\": [None, 10, 20]\n    },\n\n    \"XGBoost\": {\n        \"classifier\": [XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")],\n        \"classifier__n_estimators\": [100, 200],\n        \"classifier__max_depth\": [3, 5],\n        \"classifier__learning_rate\": [0.01, 0.1]\n    }\n}\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#9-make-a-full-pipeline","title":"9. Make a full Pipeline","text":"<ul> <li>Define pipeline with a generic \"classifier\" step (can be any model)</li> </ul> <pre><code>pipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"classifier\", LogisticRegression()) # a generic placeholder\n])\n</code></pre> <ul> <li>Note Pipeline with SMOTE for imbalance handling</li> </ul> <pre><code># pipeline = ImbPipeline(steps=[\n#     (\"feature_engineering\", feature_engineering),\n#     (\"preprocessor\", preprocessor),\n#     (\"sampler\", SMOTE(random_state=42)),\n#     (\"classifier\", RandomForestClassifier(class_weight='balanced', random_state=42))\n# ])\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#10-train-with-gridsearchcv-and-select-best-model","title":"10. Train with GridSearchCV and Select Best Model","text":"<pre><code>results = {}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor name, param_grid in param_grids.items():\n\n    grid = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, scoring=\"f1_macro\", n_jobs=-1)\n\n    # fit each model\n    grid.fit(X_train, y_train)\n\n    results[name] = {\n        \"best_score\": grid.best_score_,\n        \"best_estimator\": grid.best_estimator_, # Mean cross-validated score\n        \"best_params\": grid.best_params_ # Best hyperparameters\n    }\n\n# Print comparison\nfor model_name, res in results.items():\n    print(f\"{model_name}: F1 Score = {res['best_score']:.4f}\")\n\n# Select best model\nbest_model_name = max(results, key=lambda k: results[k]['best_score'])\nbest_model = results[best_model_name][\"best_estimator_\"]\nprint(f\"Best model: {best_model_name}\")\n\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#11-evaluate-on-test-data-predict-and-evaluate-on-test-data","title":"11. Evaluate on test data - predict and evaluate on test data","text":"<pre><code>y_pred = best_model.predict(X_test)\ny_prob = best_model.predict_proba(X_test)[:, 1]\n\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"ROC AUC Score:\", roc_auc_score(y_test, y_prob))\n</code></pre>"},{"location":"SQL/best_paid_title/","title":"Best Paid Job Titles","text":"<p>SELECT the job titles of the highest-paid employees.Your output should list the job title or titles with the highest salary, considering the possibility of multiple titles sharing the same salary.</p> <p>worker table:</p> Field Description worker_id Identifier of worker first_name First name of worker last_name Last name of worker salary Salary of worker joining_date Date of joining department Department name <p>title table:</p> Field Description title_id ID of the title worker_ref_id Identifier of worker worker_title Name of the position affected_from Date of modification <pre><code>with tbl_join_worker_title AS\n(SELECT W.WORKER_ID, W.SALARY, T.WORKER_TITLE,\nDENSE_RANK() OVER(ORDER BY W.SALARY desc) AS RNK\nFROM WORKER AS W \nINNER JOIN TITLE T ON W.WORKER_ID = T.WORKER_REF_ID\n)\nSELECT WORKER_TITLE AS BEST_PAID_TITLE\nFROM tbl_join_worker_title \nWHERE RNK=1 \n</code></pre>"},{"location":"SQL/consecutive_login_days/","title":"Consecutive log-in streak","text":"<p>Absolutely! Let\u2019s expand the markdown with a detailed example involving 3 users: <code>1032</code>, <code>1050</code>, and <code>1002</code>. This will help you clearly see how the <code>streak_marker</code>, <code>streak_id</code>, and final output work.</p> <p>Here\u2019s the updated and elaborated markdown example:</p> <pre><code># SQL Walkthrough: Finding Users with 10+ Consecutive Login Days\n\nThis walkthrough explains how to identify users who have logged in for **at least 10 consecutive days**, using SQL Common Table Expressions (CTEs). We show the process **step-by-step**, using a sample dataset for 3 users: `1032`, `1050`, and `1002`.\n\n---\n\n## \ud83e\uddfe Table Structure\n\nWe start with the following table:\n\n```sql\nuser_login(\n  user_id INT,\n  login_date DATE\n)\n</code></pre>"},{"location":"SQL/consecutive_login_days/#sample-data-for-3-users","title":"\ud83d\udc65 Sample Data for 3 Users","text":"user_id login_date 1032 2019-01-01 1032 2019-01-02 1032 2019-01-03 1032 2019-01-04 1032 2019-01-05 1032 2019-01-06 1032 2019-01-07 1032 2019-01-08 1032 2019-01-09 1032 2019-01-10 1032 2019-01-11 1032 2019-01-13 1050 2020-02-10 1050 2020-02-11 1050 2020-02-12 1050 2020-02-13 1050 2020-02-14 1050 2020-02-15 1050 2020-02-16 1050 2020-02-17 1050 2020-02-18 1050 2020-02-19 1002 2021-05-01 1002 2021-05-03 1002 2021-05-04 1002 2021-05-06"},{"location":"SQL/consecutive_login_days/#step-1-add-previous-login-date","title":"Step 1\ufe0f\u20e3: Add Previous Login Date","text":"<pre><code>WITH my_prior_login_dates AS (\n  SELECT\n    user_id,\n    login_date,\n    LAG(login_date) OVER(PARTITION BY user_id ORDER BY login_date) AS prior_login_date\n  FROM user_login\n)\n</code></pre>"},{"location":"SQL/consecutive_login_days/#output-simplified","title":"\ud83d\udd0d Output (simplified)","text":"user_id login_date prior_login_date 1032 2019-01-01 (null) 1032 2019-01-02 2019-01-01 ... ... ... 1050 2020-02-10 (null) 1050 2020-02-11 2020-02-10 ... ... ... 1002 2021-05-01 (null) 1002 2021-05-03 2021-05-01 ... ... ..."},{"location":"SQL/consecutive_login_days/#step-2-mark-streak-breaks","title":"Step 2\ufe0f\u20e3: Mark Streak Breaks","text":"<pre><code>, my_marker AS (\n  SELECT\n    *,\n    CASE \n      WHEN DATE_PART('day', login_date - prior_login_date) = 1 \n           OR prior_login_date IS NULL THEN 0\n      ELSE 1\n    END AS streak_marker\n  FROM my_prior_login_dates\n)\n</code></pre>"},{"location":"SQL/consecutive_login_days/#output","title":"\ud83d\udd0d Output","text":"user_id login_date prior_login_date streak_marker 1032 2019-01-01 (null) 0 1032 2019-01-02 2019-01-01 0 1032 2019-01-03 2019-01-02 0 ... ... ... ... 1032 2019-01-13 2019-01-11 1 1050 2020-02-10 (null) 0 1050 2020-02-11 2020-02-10 0 ... ... ... ... 1002 2021-05-01 (null) 0 1002 2021-05-03 2021-05-01 1 1002 2021-05-04 2021-05-03 0 1002 2021-05-06 2021-05-04 1"},{"location":"SQL/consecutive_login_days/#step-3-assign-streak-id","title":"Step 3\ufe0f\u20e3: Assign Streak ID","text":"<pre><code>, my_streaks AS (\n  SELECT\n    user_id,\n    login_date,\n    SUM(streak_marker) OVER (PARTITION BY user_id ORDER BY login_date) AS streak_id\n  FROM my_marker\n)\n</code></pre>"},{"location":"SQL/consecutive_login_days/#output_1","title":"\ud83d\udd0d Output","text":"user_id login_date streak_id 1032 2019-01-01 0 1032 2019-01-02 0 1032 2019-01-03 0 ... ... ... 1032 2019-01-13 1 1050 2020-02-10 0 1050 2020-02-11 0 ... ... ... 1002 2021-05-01 0 1002 2021-05-03 1 1002 2021-05-04 1 1002 2021-05-06 2 <ul> <li>Each <code>streak_id</code> represents a separate run of consecutive login days.</li> </ul>"},{"location":"SQL/consecutive_login_days/#step-4-filter-10-day-streaks","title":"Step 4\ufe0f\u20e3: Filter 10+ Day Streaks","text":"<pre><code>SELECT \n  user_id, \n  MIN(login_date) AS first_login_date, \n  COUNT(*) AS consecutive_login_days\nFROM my_streaks\nGROUP BY user_id, streak_id\nHAVING COUNT(*) &gt;= 10;\n</code></pre>"},{"location":"SQL/consecutive_login_days/#final-output","title":"\u2705 Final Output","text":"user_id first_login_date consecutive_login_days 1032 2019-01-01 11 1050 2020-02-10 10"},{"location":"SQL/consecutive_login_days/#why-1002-is-not-in-result","title":"\u26a0\ufe0f Why 1002 is not in result:","text":"<p>User <code>1002</code> never had 10 consecutive login days. Their logins were non-consecutive or had short streaks:</p> <ul> <li>2021-05-01 \u2192 gap \u2192 2021-05-03 \u2192 2021-05-04 \u2192 gap \u2192 2021-05-06</li> </ul>"},{"location":"SQL/consecutive_login_days/#summary","title":"\u2705 Summary","text":"Concept Description <code>LAG()</code> Finds prior login for each user <code>streak_marker</code> 1 if gap occurred, 0 if login is consecutive <code>streak_id</code> Increases when a new streak starts <code>GROUP BY</code> Each <code>(user_id, streak_id)</code> group is one streak <code>HAVING COUNT(*) &gt;= 10</code> Filters only those streaks with 10 or more days <p>This method allows you to track multiple streaks per user, and analyze login behavior in a clean, scalable way.</p>"},{"location":"SQL/first_name/","title":"Concat name counts and names","text":""},{"location":"SQL/first_name/#sql-task","title":"SQL Task","text":"<p>Goal: Return a single string like <code>2, Alan, Adam</code> for all student names that start with 'A' or 'a', using the <code>students</code> table.</p> <p>Table: <code>students</code></p> Column Name Description first_name The first name of student"},{"location":"SQL/first_name/#postgresql-query","title":"\u2705 PostgreSQL Query","text":"<pre><code>SELECT \n    COUNT(*) || ', ' || STRING_AGG(first_name, ', ') AS result\nFROM students\nWHERE first_name ILIKE 'a%';\n</code></pre>"},{"location":"SQL/first_name/#explanation-for-mysql","title":"Explanation for MySQL","text":"<ul> <li><code>ILIKE 'a%'</code>: case-insensitive match for names starting with 'a' or 'A'</li> <li><code>STRING_AGG(...)</code>: concatenates the names with a comma and a space</li> <li><code>COUNT(*) || ', ' || ...</code>: formats the output as a single string</li> </ul>"},{"location":"SQL/first_name/#mysql-query-80","title":"\u2705 MySQL Query (8.0+)","text":"<pre><code>SELECT \n    CONCAT(COUNT(*), ', ', GROUP_CONCAT(first_name SEPARATOR ', ')) AS result\nFROM students\nWHERE LOWER(first_name) LIKE 'a%';\n</code></pre>"},{"location":"SQL/first_name/#explanation","title":"Explanation:","text":"<ul> <li><code>LOWER(first_name) LIKE 'a%'</code>: ensures case-insensitive matching</li> <li><code>GROUP_CONCAT(...)</code>: joins names with a comma and a space</li> <li><code>CONCAT(...)</code>: formats the final output</li> </ul>"},{"location":"SQL/first_name/#output-in-a-tablular-form","title":"Output in a tablular form","text":"<pre><code>SELECT \n    COUNT(*) AS name_count,\n    STRING_AGG(first_name, ', ') AS names\nFROM students\nWHERE first_name ILIKE 'a%';\n</code></pre> <p>If the table has different first name like:</p> first_name Alan adam Bob Annie <p>The output will be</p> name_count names 3 Alan, adam, Annie"},{"location":"SQL/groupby_monthly_customer/","title":"\ud83d\udcca SQL: GroupBy Monthly Customer Stats","text":""},{"location":"SQL/groupby_monthly_customer/#objective","title":"\ud83d\udd0d Objective","text":"<p>Get the total number of customers per month and average sales per customer per month.</p>"},{"location":"SQL/groupby_monthly_customer/#sample-table-structure","title":"\ud83e\uddf1 Sample Table Structure","text":"<p>Assume we have a table called <code>sales_data</code>:</p> Column Data Type Description sale_id INT Unique identifier for the sale customer_id INT Customer making the purchase sale_amount DECIMAL Total amount of the sale sale_date DATE Date when the sale occurred"},{"location":"SQL/groupby_monthly_customer/#sql-query","title":"\ud83d\udccc SQL Query","text":"<pre><code>SELECT\n  DATE_FORMAT(sale_date, '%Y-%m') AS sale_month,\n  COUNT(DISTINCT customer_id) AS total_customers,\n  SUM(sale_amount) / COUNT(DISTINCT customer_id) AS avg_sales_per_customer\nFROM\n  sales_data\nGROUP BY\n  DATE_FORMAT(sale_date, '%Y-%m')\nORDER BY\n  sale_month;\n</code></pre>"},{"location":"SQL/imp_notes/","title":"\ud83d\udcbe Advanced SQL Patterns: Comprehensive Reference","text":"<p>This guide summarizes four advanced SQL techniques: Self-Joins for pairing, Window Functions for streak analysis (<code>ROW_NUMBER()</code>), time-series comparison (<code>LAG()</code>), and the difference between ranking functions.</p>"},{"location":"SQL/imp_notes/#1-pattern-self-join-for-unique-pair-counting","title":"1. \ud83e\udd1d Pattern: Self-Join for Unique Pair Counting","text":"<p>\ud83c\udfaf Goal: Find and count unique relationships between entities in the same table (e.g., lawyers sharing a trial).</p> <p>\ud83c\udff7\ufe0f Table Schema: | Table | Column | Type | | :---: | :---: | :---: | | <code>TrialParticipation</code> | <code>trial_id</code> | INT | | | <code>lawyer_id</code> | INT |</p> <p>\ud83d\udd11 Key Logic: Eliminating Duplicate Pairs Joining a table to itself creates redundant pairs (A, B and B, A). The condition below ensures only one is counted: - <code>T1.lawyer_id &lt; T2.lawyer_id</code>: Forces the lawyer with the lower ID to always be listed first.</p> <p>\ud83d\udcbb SQL Code Template:</p> <pre><code>SELECT\n    T1.lawyer_id AS Lawyer_A,\n    T2.lawyer_id AS Lawyer_B,\n    COUNT(T1.trial_id) AS Shared_Count\nFROM\n    TrialParticipation AS T1\nJOIN\n    TrialParticipation AS T2\n    ON T1.trial_id = T2.trial_id\n    AND T1.lawyer_id &lt; T2.lawyer_id -- CRITICAL: Eliminates duplicates\nGROUP BY\n    Lawyer_A, Lawyer_B\nORDER BY\n    Shared_Count DESC;\n````\n\n-----\n\n## 2\\. \ud83d\uddd3\ufe0f Pattern: Streak Finding (`ROW_NUMBER()`)\n\n**\ud83c\udfaf Goal:** Find the longest continuous sequence of successful days (sales \\&gt; $1000).\n\n**\ud83c\udff7\ufe0f Table Schema:**\n| Table | Column | Type |\n| :---: | :---: | :---: |\n| `DailySales` | `sale_date` | DATE |\n| | `sales_amount` | INT |\n\n**\ud83d\udd11 Key Logic: The Group ID Trick**\nThe difference between the date and the row number is **constant** during a streak but **changes** when a date gap occurs.\n$$\\text{Group ID} = \\text{Sale Date} - \\text{ROW\\_NUMBER}()$$\n\n**\ud83d\udcbb SQL Code Template (PostgreSQL):**\n\n```sql\nWITH tbl_high_sales AS (\n    SELECT sale_date FROM DailySales WHERE sales_amount &gt; 1000\n),\ntbl_streaks_grouped AS (\n    SELECT \n        sale_date, \n        -- Creates a unique ID for each continuous streak\n        sale_date - (ROW_NUMBER() OVER (ORDER BY sale_date) * INTERVAL '1 day') AS streak_group_id\n    FROM tbl_high_sales\n)\nSELECT \n    MIN(sale_date) AS Start_Date,\n    MAX(sale_date) AS End_Date,\n    COUNT(sale_date) AS Streak_Length \nFROM \n    tbl_streaks_grouped\nGROUP BY \n    streak_group_id\nORDER BY \n    Streak_Length DESC\nLIMIT 1;\n</code></pre>"},{"location":"SQL/imp_notes/#3-pattern-time-series-comparison-lag","title":"3. \ud83d\udcc8 Pattern: Time Series Comparison (<code>LAG()</code>)","text":"<p>\ud83c\udfaf Goal: Calculate the Month-Over-Month (MOM) percentage change in sales.</p> <p>\ud83d\udd11 Key Logic: The <code>LAG()</code> Function &amp; Type Casting</p> <ul> <li><code>LAG(value, 1) OVER (ORDER BY time)</code>: Efficiently brings the previous row's value into the current row.</li> <li><code>CAST</code> to NUMERIC: Required to prevent integer division (which truncates decimals) when calculating the percentage change.</li> </ul> <p>\ud83d\udcbb SQL Code Template (PostgreSQL):</p> <pre><code>WITH tbl_monthly_sales AS (\n    SELECT  \n        DATE_TRUNC('month', sale_date) AS sale_month,\n        SUM(sales_amount) AS total_sales\n    FROM DailySales\n    GROUP BY sale_month\n),\ntbl_lagged_sales AS (\n    SELECT \n        sale_month, \n        total_sales,\n        LAG(total_sales, 1) OVER(ORDER BY sale_month ASC) AS prev_month_sales \n    FROM tbl_monthly_sales\n)\nSELECT \n    sale_month, \n    total_sales, \n    prev_month_sales,\n    -- Forces floating-point division for accurate percentage\n    CAST((total_sales - prev_month_sales) AS NUMERIC) / prev_month_sales AS mom_percent_change\nFROM tbl_lagged_sales;\n</code></pre>"},{"location":"SQL/imp_notes/#4-window-function-ranking-rank-vs-dense_rank","title":"4. \ud83d\udd22 Window Function: Ranking (<code>RANK</code> vs. <code>DENSE_RANK</code>)","text":"<p>\ud83c\udfaf Goal: Assign ranks based on value, differentiating how ties are handled.</p> Function Tie Handling Rank Jumps? Example (100, 100, 90, 80) <code>RANK()</code> Same rank for ties. Yes. 1, 1, 3, 4 <code>DENSE_RANK()</code> Same rank for ties. No. 1, 1, 2, 3 <p>\ud83d\udcbb SQL Code Template (<code>DENSE_RANK</code> Example):</p> <pre><code>SELECT \n    student_name,\n    exam_score,\n    -- Used to find the 'top N distinct score tiers'\n    DENSE_RANK() OVER (ORDER BY exam_score DESC) AS score_rank\nFROM \n    StudentScores\nWHERE\n    DENSE_RANK() OVER (ORDER BY exam_score DESC) &lt;= 2;\n</code></pre>"},{"location":"SQL/imp_notes_v2/","title":"\ud83d\udcda Advanced SQL Reference Guide: Core Concepts","text":""},{"location":"SQL/imp_notes_v2/#1-pattern-self-join-for-unique-pair-counting","title":"1. \ud83e\udd1d Pattern: Self-Join for Unique Pair Counting","text":"<p>\ud83c\udfaf Goal: Find and count unique relationships between entities in the same table (e.g., lawyers sharing a trial).</p> <p>\ud83d\udd11 Key Logic: Eliminating Duplicate Pairs Joining a table to itself (<code>T1</code> to <code>T2</code>) requires a condition to prevent double-counting of pairs (e.g., counting A/B and B/A).</p> <ul> <li><code>T1.id &lt; T2.id</code>: The most common and effective way to ensure only one order is counted.</li> </ul> <p>\ud83d\udcbb SQL Code Template:</p> <pre><code>SELECT\n    T1.lawyer_id AS Lawyer_A,\n    T2.lawyer_id AS Lawyer_B,\n    COUNT(T1.trial_id) AS Shared_Count\nFROM\n    TrialParticipation AS T1\nJOIN\n    TrialParticipation AS T2\n    ON T1.trial_id = T2.trial_id   \n    AND T1.lawyer_id &lt; T2.lawyer_id -- CRITICAL: Eliminate duplicates\nGROUP BY\n    Lawyer_A, Lawyer_B;\n</code></pre>"},{"location":"SQL/imp_notes_v2/#2-pattern-streak-finding-row_number","title":"2. \ud83d\uddd3\ufe0f Pattern: Streak Finding (<code>ROW_NUMBER()</code>)","text":"<p>\ud83c\udfaf Goal: Find the longest continuous sequence (streak) of dates or events.</p> <p>\ud83d\udd11 Key Logic: The Group ID Trick Use <code>ROW_NUMBER()</code> to assign a sequential integer to every successful row. Subtracting this integer from the date creates a <code>Group ID</code> that stays constant only during a continuous streak.</p> <p>\ud83d\udcbb SQL Code Template (PostgreSQL):</p> <pre><code>WITH tbl_high_sales AS (\n    SELECT sale_date FROM DailySales WHERE sales_amount &gt; 1000\n),\ntbl_streaks_grouped AS (\n    SELECT \n        sale_date, \n        -- Creates a unique ID for each continuous streak\n        sale_date - (ROW_NUMBER() OVER (ORDER BY sale_date) * INTERVAL '1 day') AS streak_group_id\n    FROM tbl_high_sales\n)\nSELECT \n    MIN(sale_date) AS Start_Date,\n    MAX(sale_date) AS End_Date,\n    COUNT(sale_date) AS Streak_Length \nFROM tbl_streaks_grouped\nGROUP BY streak_group_id;\n</code></pre>"},{"location":"SQL/imp_notes_v2/#3-pattern-time-series-comparison-lag","title":"3. \ud83d\udcc8 Pattern: Time Series Comparison (<code>LAG()</code>)","text":"<p>\ud83c\udfaf Goal: Calculate Month-Over-Month (MOM) percentage change.</p> <p>\ud83d\udd11 Key Logic: <code>LAG()</code> and Safe Casting</p> <ul> <li><code>LAG()</code> brings a value from a previous row into the current row for easy comparison.</li> <li><code>CAST(... AS NUMERIC)</code>: Essential to prevent integer division (which truncates the result to 0) when calculating percentages.</li> </ul> <p>\ud83d\udcbb SQL Code Template:</p> <pre><code>WITH tbl_monthly_sales AS (\n    SELECT DATE_TRUNC('month', sale_date) AS sale_month, SUM(sales_amount) AS total_sales\n    FROM DailySales GROUP BY sale_month\n),\ntbl_lagged_sales AS (\n    SELECT \n        total_sales,\n        LAG(total_sales, 1) OVER(ORDER BY sale_month ASC) AS prev_month_sales \n    FROM tbl_monthly_sales\n)\nSELECT \n    CAST((total_sales - prev_month_sales) AS NUMERIC) / prev_month_sales AS mom_percent_change\nFROM tbl_lagged_sales;\n</code></pre>"},{"location":"SQL/imp_notes_v2/#4-sql-execution-order-and-filtering-clauses","title":"4. \u2699\ufe0f SQL Execution Order and Filtering Clauses","text":"<p>The order in which these clauses are written determines when the filtering happens.</p> Clause Purpose Execution Order Can use Aggregation? <code>WHERE</code> Filters Individual Rows. 2nd (Before Grouping) \ud83d\udeab No. Filters raw data. <code>GROUP BY</code> Aggregates remaining rows. 3rd <code>HAVING</code> Filters Groups/Totals. 4th (After Grouping) \u2705 Yes. Filters based on <code>SUM()</code>, <code>COUNT()</code>, etc. <code>ORDER BY</code> Sorts final results. 6th (Last) \u2705 Yes. <p>Practical Example:</p> <pre><code>SELECT product_category, SUM(amount) AS total_sales\nFROM SalesTransactions\nWHERE EXTRACT(YEAR FROM transaction_date) = 2025 -- 1. WHERE: Filter sales in 2025\n  AND amount &gt; 100                               -- 1. WHERE: Filter individual transactions &gt; 100\nGROUP BY product_category\nHAVING SUM(amount) &gt; 50000;                      -- 2. HAVING: Filter categories with total sales &gt; 50k\n</code></pre>"},{"location":"SQL/imp_notes_v2/#5-postgresql-date-function-comparison","title":"5. \ud83d\udcc5 PostgreSQL Date Function Comparison","text":"<p>Both functions are used to filter dates, but they return different data types, which affects how you write your comparisons.</p> Function Output Data Type Comparison Example Use Case <code>EXTRACT(YEAR FROM date)</code> Integer (e.g., 2025) <code>WHERE EXTRACT(YEAR FROM date) = 2025</code> Simple comparison against a specific numerical part of the date (year, month, day). <code>DATE_TRUNC('unit', date)</code> Timestamp (e.g., <code>2025-01-01 00:00:00</code>) <code>WHERE DATE_TRUNC('year', date) = '2025-01-01'</code> Used for grouping time periods (like <code>GROUP BY DATE_TRUNC('month', date)</code>) or filtering ranges. <p>You've built a strong knowledge base! Since you've mastered filtering, aggregation, and time series, what would you like to explore next?</p> <ol> <li>\ud83c\udf32 Recursive CTEs: Learning to query hierarchical data (like finding all managers in a management chain).</li> <li>\ud83d\udd0d SQL Performance Tuning: Discussion of indexing and optimization for queries.</li> <li>\ud83e\ude9f Other Window Functions: Exploring <code>LEAD()</code> and using partitions (<code>PARTITION BY</code>).</li> </ol>"},{"location":"SQL/listening_habit/","title":"\ud83c\udfa7 SQL Challenge: Analyze Spotify-like Listening Habits","text":""},{"location":"SQL/listening_habit/#question","title":"\ud83d\udcd8 Question","text":"<p>You're assigned to analyze a Spotify-like dataset that records user listening habits.</p>"},{"location":"SQL/listening_habit/#task","title":"\ud83d\udcdd Task","text":"<p>For each user, compute:</p> <ul> <li>\u2705 The total listening time (in minutes), rounded to the nearest whole number</li> <li>\u2705 The number of unique songs they've listened to</li> </ul>"},{"location":"SQL/listening_habit/#table-listening_habits","title":"\ud83d\uddc3\ufe0f Table: <code>listening_habits</code>","text":"Column Name Description <code>user_id</code> Identifier of user <code>song_id</code> Identifier of song <code>listen_duration</code> Listening time (in seconds)"},{"location":"SQL/listening_habit/#expected-output","title":"\ud83d\udccc Expected Output","text":"user_id total_listen_duration unique_song_count 101 8 2 102 5 2"},{"location":"SQL/listening_habit/#sql-answer","title":"\u2705 SQL Answer","text":"<pre><code>SELECT \n  user_id, \n  ROUND(SUM(listen_duration) / 60.0) AS total_listen_duration, \n  COUNT(DISTINCT song_id) AS unique_song_count\nFROM listening_habits\nGROUP BY user_id;\n</code></pre>"},{"location":"SQL/most_popular_product/","title":"\ud83d\udce6 SQL Question: Most Popular Product on Instagram Shop","text":""},{"location":"SQL/most_popular_product/#requirements","title":"\ud83d\udcdd Requirements:","text":"<p>You are working for Instagram Shop, and your team wants to know what is the most popular product. Write a SQL query to find 1 product with the highest number of orders. In case of a tie in order counts, select the product that comes first in alphabetical order.</p>"},{"location":"SQL/most_popular_product/#tables","title":"\ud83d\udcca Tables","text":""},{"location":"SQL/most_popular_product/#orders-table","title":"<code>orders</code> table","text":"Column Name Description id Unique identifier for each order product_id Identifier for the product customer_id Identifier for the customer order_date Date the order was placed"},{"location":"SQL/most_popular_product/#products-table","title":"<code>products</code> table:","text":"Column Name Description id Unique identifier for each product name Name of the product price Price of the product category_id Identifier for the category"},{"location":"SQL/most_popular_product/#sql-answer","title":"\u2705 SQL Answer:","text":"<pre><code>WITH tbl_PRODUCT_COUNT AS (\n  SELECT \n    P.name AS product_name, \n    COUNT(O.product_id) AS order_counts\n  FROM orders O \n  JOIN products P ON O.product_id = P.id\n  GROUP BY P.name\n)\nSELECT product_name\nFROM tbl_PRODUCT_COUNT\nORDER BY order_counts DESC, product_name ASC\nLIMIT 1;\n</code></pre>"},{"location":"SQL/most_popular_product/#explanation","title":"\ud83d\udca1 Explanation:","text":"<ul> <li><code>JOIN</code> combines order and product info.</li> <li><code>COUNT(O.product_id)</code> counts how many times each product was ordered.</li> <li><code>GROUP BY P.name</code> groups orders by product.</li> <li> <p><code>ORDER BY order_counts DESC, product_name ASC</code> ensures:</p> </li> <li> <p>Most popular product comes first.</p> </li> <li>Alphabetical order is used to break ties.</li> <li><code>LIMIT 1</code> returns just the top product.</li> </ul>"},{"location":"SQL/total_duration/","title":"Calculate Total Active Hours for Each User","text":"<p>Calculate the total active hours for each user. You should use the start and end times of user sessions, defined by the session state: '1' for session start and '0' for session end.</p> <p>user_sessions table:</p> Column Name Description customer_id Customer's unique identifier state Session's state (1 for start, 0 for end) timestamp Timestamp of the session state customer_id state timestamp c001 1 07:00:00 c001 0 09:30:00 c001 1 12:00:00 <p>Example Output:</p> customer_id total_hours c005 19 <pre><code>WITH tbl_start_state AS \n(SELECT customer_id, state AS start_state, timestamp AS start_time,\nROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY timestamp) AS rn\nfrom user_sessions\nwhere state = 1\n),\n\ntbl_end_state AS (Select customer_id, state AS end_state, \ntimestamp AS end_time,\nROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY timestamp) AS rn\nfrom user_sessions\nwhere state = 0\n),\n\ntbl_combine AS (SELECT s.customer_id, \ns.start_state, e.end_state, s.start_time, e.end_time\nFROM tbl_start_state s JOIN tbl_end_state e \nON s.customer_id = e.customer_id AND s.rn=e.rn), \n\ntbl_duration AS(\nselect customer_id, start_state, end_state, \nstart_time, end_time, \nEXTRACT(EPOCH FROM (end_time-start_time))/3600.0 AS hr_duration from tbl_combine)\n\nSELECT customer_id, SUM(hr_duration) AS total_hours\nFROM tbl_duration\nGROUP BY customer_id\n</code></pre>"},{"location":"git_commands/handy_git_commands/","title":"\ud83e\udde0 Handy Git Command Reference (with Examples)","text":"<p>A simple guide to common Git commands with clear explanations.</p>"},{"location":"git_commands/handy_git_commands/#1-setup","title":"\ud83d\udd27 1. Setup","text":"<p>Before using Git, set your name and email (only once per machine):</p> <pre><code>git config --global user.name \"John Doe\"\ngit config --global user.email \"john@example.com\"\n</code></pre>"},{"location":"git_commands/handy_git_commands/#2-start-a-repository","title":"\ud83d\udcc1 2. Start a Repository","text":""},{"location":"git_commands/handy_git_commands/#create-a-new-git-repo-in-your-project-folder","title":"\u27a4 Create a new Git repo in your project folder","text":"<pre><code>git init\n</code></pre> <p>Think of this as telling Git: \u201cHey, I want to start tracking changes here!\u201d</p>"},{"location":"git_commands/handy_git_commands/#clone-an-existing-repo-download-code-from-github","title":"\u27a4 Clone an existing repo (download code from GitHub)","text":"<pre><code>git clone https://github.com/user/project.git\n</code></pre>"},{"location":"git_commands/handy_git_commands/#3-save-your-work-commit-flow","title":"\ud83d\udcbe 3. Save Your Work (Commit Flow)","text":""},{"location":"git_commands/handy_git_commands/#check-whats-changed","title":"\u27a4 Check what\u2019s changed","text":"<pre><code>git status\n</code></pre>"},{"location":"git_commands/handy_git_commands/#stage-a-file-for-saving","title":"\u27a4 Stage a file for saving","text":"<pre><code>git add index.html\n</code></pre> <p>\u201cI want to include this file in the next save.\u201d</p>"},{"location":"git_commands/handy_git_commands/#stage-everything","title":"\u27a4 Stage everything","text":"<pre><code>git add .\n</code></pre>"},{"location":"git_commands/handy_git_commands/#save-your-changes-with-a-message","title":"\u27a4 Save your changes with a message:","text":"<pre><code>git commit -m \"Add homepage layout\"\n</code></pre> <p>Like hitting \u201cSave\u201d in your editor, but with a label explaining what you changed.</p>"},{"location":"git_commands/handy_git_commands/#4-work-on-a-new-feature-branching","title":"\ud83c\udf3f 4. Work on a New Feature (Branching)","text":""},{"location":"git_commands/handy_git_commands/#see-all-branches","title":"\u27a4 See all branches:","text":"<pre><code>git branch\n</code></pre>"},{"location":"git_commands/handy_git_commands/#create-a-new-branch","title":"\u27a4 Create a new branch:","text":"<pre><code>git branch new-feature\n</code></pre>"},{"location":"git_commands/handy_git_commands/#switch-to-the-new-branch","title":"\u27a4 Switch to the new branch","text":"<pre><code>git checkout new-feature\n</code></pre> <p>Or both in one step:</p> <pre><code>git checkout -b new-feature\n</code></pre> <p>\"Let me work on something new without touching the main version.\"</p>"},{"location":"git_commands/handy_git_commands/#5-combine-changes-merge","title":"\ud83d\udd01 5. Combine Changes (Merge)","text":""},{"location":"git_commands/handy_git_commands/#go-back-to-main-branch","title":"\u27a4 Go back to main branch:","text":"<pre><code>git checkout main\n</code></pre>"},{"location":"git_commands/handy_git_commands/#merge-changes-from-your-feature-branch","title":"\u27a4 Merge changes from your feature branch","text":"<pre><code>git merge new-feature\n</code></pre>"},{"location":"git_commands/handy_git_commands/#6-connect-to-remote-eg-github","title":"\ud83c\udf0d 6. Connect to Remote (e.g., GitHub)","text":""},{"location":"git_commands/handy_git_commands/#add-github-as-remote","title":"\u27a4 Add GitHub as remote:","text":"<pre><code>git remote add origin https://github.com/user/project.git\n</code></pre>"},{"location":"git_commands/handy_git_commands/#push-your-code-online","title":"\u27a4 Push your code online:","text":"<pre><code>git push -u origin main\n</code></pre>"},{"location":"git_commands/handy_git_commands/#git-remote-add-vs-git-push","title":"git remote add vs. git push","text":"Command What it does When you use it <code>git remote add</code> Sets up the connection to a remote (like GitHub) One time per project <code>git push</code> Uploads your code to GitHub Every time you want to update the repo ---- <p>Upload your local work to GitHub.</p>"},{"location":"git_commands/handy_git_commands/#get-latest-changes-from-remote","title":"\u27a4 Get latest changes from remote","text":"<pre><code>git pull\n</code></pre>"},{"location":"git_commands/handy_git_commands/#7-view-history","title":"\ud83d\udcdc 7. View History","text":""},{"location":"git_commands/handy_git_commands/#see-list-of-commits","title":"\u27a4 See list of commits","text":"<pre><code>git log\n</code></pre>"},{"location":"git_commands/handy_git_commands/#simple-view","title":"\u27a4 Simple view","text":"<pre><code>git log --oneline\n</code></pre>"},{"location":"git_commands/handy_git_commands/#compare-changes-in-files","title":"\u27a4 Compare changes in files","text":"<pre><code>git diff\n</code></pre>"},{"location":"git_commands/handy_git_commands/#8-undo-mistakes","title":"\ud83e\uddfd 8. Undo Mistakes","text":""},{"location":"git_commands/handy_git_commands/#undo-changes-in-a-file","title":"\u27a4 Undo changes in a file","text":"<pre><code>git restore index.html\n</code></pre>"},{"location":"git_commands/handy_git_commands/#unstage-a-file","title":"\u27a4 Unstage a file","text":"<pre><code>git reset HEAD index.html\n</code></pre>"},{"location":"git_commands/handy_git_commands/#revert-a-commit-safe-undo","title":"\u27a4 Revert a commit (safe undo)","text":"<pre><code>git revert &lt;commit-id&gt;\n</code></pre>"},{"location":"git_commands/handy_git_commands/#9-git-stash-temporarily-save-your-change","title":"\ud83d\udcbc 9. <code>git stash</code>: Temporarily Save Your Change","text":"<p><code>git stash</code> is used to temporarily save your uncommitted changes (both staged and unstaged) without committing them. It clears your working directory so you can safely switch branches or perform other tasks, and later bring back your changes.</p>"},{"location":"git_commands/handy_git_commands/#why-use-git-stash","title":"Why Use <code>git stash</code>?","text":"<p>Imagine you're in the middle of editing files, and suddenly need to:</p> <ul> <li>Switch branches</li> <li>Pull the latest code</li> <li>Fix a critical bug</li> </ul> <p>But Git won't let you proceed because of your uncommitted changes. Use <code>git stash</code> to set aside your work, do the other task, and come back later.</p>"},{"location":"git_commands/handy_git_commands/#example-stash-workflow","title":"\ud83e\uddea Example <code>stash</code> Workflow","text":"<pre><code># You're working on changes\ngit stash           # Save and clear working directory\n\ngit switch other-branch   # Now Git allows switching\n\n# Do some work in another branch...\n\ngit switch original-branch\ngit stash pop       # Bring back your saved changes\n\n</code></pre>"},{"location":"git_commands/handy_git_commands/#see-saved-stashes","title":"\u27a4 See saved stashes","text":"<pre><code>git stash list\n</code></pre>"},{"location":"git_commands/handy_git_commands/#reapply-stash","title":"\u27a4 Reapply stash","text":"<pre><code>git stash pop\n</code></pre>"},{"location":"git_commands/handy_git_commands/#10-tag-a-version-release","title":"\ud83d\udd16 10. Tag a Version (Release)","text":""},{"location":"git_commands/handy_git_commands/#add-a-tag","title":"\u27a4 Add a tag","text":"<pre><code>git tag v1.0\n</code></pre>"},{"location":"git_commands/handy_git_commands/#push-tag-to-github","title":"\u27a4 Push tag to GitHub","text":"<pre><code>git push origin v1.0\n</code></pre>"},{"location":"git_commands/handy_git_commands/#bonus-tips","title":"\u2728 Bonus Tips","text":"<pre><code>git shortlog -sn          # List contributors\ngit clean -fd             # Delete untracked files\ngit show &lt;commit&gt;         # Details about a commit\n</code></pre> <p>\u2705 Remember: Git is like a time machine for your code. Use it often, commit small changes, and write clear messages!</p>"},{"location":"pandas_manipulation/DS_major_tools/","title":"DS workflow","text":""},{"location":"pandas_manipulation/DS_major_tools/#data-science-test-preparation","title":"\ud83d\udcdd Data Science Test Preparation","text":""},{"location":"pandas_manipulation/DS_major_tools/#part-i-data-analysis-cleaning-exploratory-phase","title":"Part I: Data Analysis &amp; Cleaning (Exploratory Phase)","text":"<p>This part focuses on initial data quality assessment, cleaning sparse/inconsistent data, and early discovery of usage patterns.</p>"},{"location":"pandas_manipulation/DS_major_tools/#1-data-inspection-missing-data","title":"1. Data Inspection &amp; Missing Data","text":"<ul> <li>Function: <code>df.info()</code> to check data types and count non-null values.</li> <li>Function: <code>df.isnull().sum()</code> to quickly count missing (null) values per column.</li> <li>Action on Sparsity: Used <code>df.dropna(axis=1, thresh=N)</code> (where $N$ is the required minimum of non-null values) to drop columns that were too sparse (e.g., threshold set to 20 for 100 rows).</li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#pandas-dfdropna-actions","title":"\ud83d\uddd1\ufe0f Pandas <code>df.dropna()</code> Actions","text":"Drop Action Code Rule Drop Sparse Columns <code>df.dropna(axis=1, thresh=N)</code> Keep a column only if it has at least N non-null values (applied across the rows). Drop Rows (Default) <code>df.dropna(axis=0)</code> or <code>how='any'</code> Drop the entire row if any value in that row is null. Drop Empty Rows <code>df.dropna(axis=0, how='all')</code> Drop the entire row only if all values in that row are null. Drop Rows (Specific Thresh) <code>df.dropna(axis=0, thresh=N)</code> Keep a row only if it has at least N non-null values (applied across the columns)."},{"location":"pandas_manipulation/DS_major_tools/#2-handling-inconsistent-text-data","title":"2. Handling Inconsistent Text Data","text":"<ul> <li>Action: Standardizing text features (like <code>'country'</code>) to a uniform case.<ul> <li>Code: <code>df['country'] = df['country'].str.lower()</code></li> </ul> </li> <li>Action: Removing inconsistent characters (spaces, periods) using string replacement.<ul> <li>Function: <code>df.str.replace(pattern, replacement, regex=False)</code></li> <li>Key Guardrail: Always use <code>regex=False</code> when replacing literal characters (like <code>.</code> or <code></code>) to prevent accidental deletion of entire strings due to regex wildcard matching.</li> </ul> </li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#3-handling-inconsistent-datetime-data","title":"3. Handling Inconsistent Date/Time Data","text":"<ul> <li>Function: <code>pd.to_datetime()</code> is the essential function for converting object/string columns to <code>datetime64</code>.</li> <li>Action on Mixed Formats: For columns with multiple date string formats (e.g., <code>'15-Jan-2025'</code> and <code>'01/15/2025'</code>), use the <code>infer_datetime_format=True</code> argument to allow Pandas to guess the format for each entry, ensuring successful conversion.</li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#4-numerical-analysis-outlier-detection","title":"4. Numerical Analysis &amp; Outlier Detection","text":"<ul> <li>Function: <code>df.describe()</code> provides the essential summary statistics (mean, min, max, $Q_1$, $Q_2$, $Q_3$) for numerical data.</li> <li>Outlier Method: The IQR (Interquartile Range) method is the most robust technique for skewed usage data.<ul> <li>Key Concept: The Median ($Q_2$) is the best measure of a \"typical\" user session for right-skewed data because it is not inflated by extreme outliers (unlike the mean).</li> </ul> </li> <li>Visualization: Use <code>sns.boxplot(data=df['column'])</code> to visually confirm IQR outliers.</li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#5-categorical-analysis-visualization","title":"5. Categorical Analysis &amp; Visualization","text":"<ul> <li>Function: <code>df['column'].value_counts()</code> returns the count (frequency) of unique values.</li> <li>Function for Percentage: Use <code>df['column'].value_counts(normalize=True)</code> to get the relative frequency (proportion).</li> <li>Visualization (Frequency): Use <code>df['column'].value_counts().plot(kind='bar')</code> for a quick frequency bar chart.</li> <li>Visualization (Comparison): Use the <code>hue</code> argument in Seaborn (<code>sns.countplot</code> or <code>sns.boxplot</code>) to compare distributions or counts across a third variable (e.g., <code>sns.boxplot(x=..., y=..., hue='Region')</code>).</li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#6-relationship-analysis","title":"6. Relationship Analysis","text":"<ul> <li>Function: <code>df.corr()</code> generates the correlation matrix for all numerical pairs.</li> <li>Visualization: <code>sns.heatmap(df.corr(), annot=True)</code> is used to visually represent the correlation matrix, with <code>annot=True</code> displaying the numerical correlation coefficients.</li> <li>Key Concept: Strong correlation (multicollinearity) means features are redundant; the recommendation is to drop one or create a composite feature.</li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#part-ii-data-wrangling-reporting-transformation-phase","title":"Part II: Data Wrangling &amp; Reporting (Transformation Phase)","text":"<p>This part focuses on efficiently transforming data into custom, structured reports based on specific business criteria.</p>"},{"location":"pandas_manipulation/DS_major_tools/#1-filtering-subsetting","title":"1. Filtering &amp; Subsetting","text":"<ul> <li>Technique: Boolean Indexing (Masking).</li> <li>Code: <code>df[ (df['condition_A']) &amp; (df['condition_B']) ]</code> using parentheses for each condition and <code>&amp;</code> (AND) or <code>|</code> (OR) operators.</li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#2-advanced-aggregation-for-custom-reports","title":"2. Advanced Aggregation for Custom Reports","text":"<ul> <li>Function: <code>df.groupby().agg()</code> is used to apply multiple aggregation functions to multiple columns.</li> <li>Code Structure: Use a dictionary within <code>agg()</code> to specify functions per column:     <code>python     df.groupby('Device').agg({         'session_duration': ['mean', 'median'],  # Multiple functions for one column         'user_id': 'nunique'                    # Different function for a second column     })</code></li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#3-restructuring-for-wide-reports","title":"3. Restructuring for Wide Reports","text":"<ul> <li>Function: <code>pd.pivot_table()</code> is used to transform data from a long format to a wide summary table.</li> <li>Code Structure: Requires defining the three key components:     <code>python     pd.pivot_table(         data=df,         index='Region',      # New ROWS         columns='Device_Type', # New COLUMN HEADERS         values='Duration',   # Data in the cells         aggfunc='mean'       # Aggregation (default is mean)     )</code></li> </ul>"},{"location":"pandas_manipulation/DS_major_tools/#4-communication-strategy-the-email","title":"4. Communication Strategy (The Email)","text":"<ul> <li>Three Key Findings: Prioritize the three most critical findings for the email:<ol> <li>Missing Data (Sparsity): Which columns are $&gt;80\\%$ null? Caution: Statistics from these columns are not representative.</li> <li>Usage Profile (Skew/Outliers): Report the Median session duration as the reliable center, noting the mean is inflated by outliers.</li> <li>Correlation/Redundancy: Report any strong correlations (e.g., $r=0.85$) and recommend dropping one feature or creating a composite feature for future modeling.</li> </ol> </li> </ul> <p>This summary covers every major technical detail and communication point from our preparation.</p>"},{"location":"pandas_manipulation/map_apply_transform/","title":"map, apply, transfer","text":""},{"location":"pandas_manipulation/map_apply_transform/#pandas-reference-map-apply-transform-with-business-use-cases","title":"Pandas Reference: <code>map()</code>, <code>apply()</code>, <code>transform()</code> - with Business Use Cases","text":""},{"location":"pandas_manipulation/map_apply_transform/#sample-business-dataset","title":"\ud83d\uddc3 Sample Business Dataset","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'CustomerID': [101, 102, 103, 104, 105, 106, 107, 108],\n    'Region': ['East', 'West', 'East', 'South', 'West', 'East', 'South', 'West'],\n    'PurchaseAmount': [250, 300, 150, 400, 500, 100, 350, 450],\n    'LoyaltyLevel': ['Gold', 'Silver', 'Gold', 'Platinum', 'Silver', 'Bronze', 'Gold', 'Silver']\n})\n</code></pre> CustomerID Region PurchaseAmount LoyaltyLevel 101 East 250 Gold 102 West 300 Silver 103 East 150 Gold 104 South 400 Platinum 105 West 500 Silver 106 East 100 Bronze 107 South 350 Gold 108 West 450 Silver"},{"location":"pandas_manipulation/map_apply_transform/#1-map-use-for-element-wise-transformation-on-a-series","title":"1\ufe0f\u20e3 <code>map()</code> - Use for Element-wise Transformation on a Series","text":""},{"location":"pandas_manipulation/map_apply_transform/#goal-map-loyalty-level-to-discount-rate","title":"\ud83d\udccc Goal: Map loyalty level to discount rate","text":"<pre><code>discount_map = {'Gold': 0.10, 'Silver': 0.05, 'Platinum': 0.15, 'Bronze': 0.02}\ndf['DiscountRate'] = df['LoyaltyLevel'].map(discount_map)\n</code></pre>"},{"location":"pandas_manipulation/map_apply_transform/#another-example-format-customerid-with-prefix","title":"\ud83d\udccc Another Example: Format CustomerID with prefix","text":"<pre><code>df['CustomerTag'] = df['CustomerID'].map(lambda x: f'CUST-{x}')\n</code></pre> <p>\u2705 Use <code>map()</code> when:</p> <ul> <li>We are transforming values element-by-element</li> <li>We are working with a single column (Series)</li> <li>We can use a function or dictionary</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#2-apply-use-for-rowcolumn-wise-custom-logic","title":"2\ufe0f\u20e3 <code>apply()</code> - Use for Row/Column-wise Custom Logic","text":""},{"location":"pandas_manipulation/map_apply_transform/#goal-compute-final-price-after-discount-row-wise","title":"\ud83d\udccc Goal: Compute final price after discount (row-wise)","text":"<pre><code>df['FinalAmount'] = df.apply(\n    lambda row: row['PurchaseAmount'] * (1 - row['DiscountRate']),\n    axis=1\n)\n</code></pre>"},{"location":"pandas_manipulation/map_apply_transform/#another-example-tag-high-value-customers","title":"\ud83d\udccc Another Example: Tag high-value customers","text":"<pre><code>df['HighValueTag'] = df.apply(\n    lambda row: 'VIP' if row['FinalAmount'] &gt; 300 else 'Regular',\n    axis=1\n)\n</code></pre> <p>\u2705 Use <code>apply()</code> when:</p> <ul> <li>We want access to multiple columns at once (row-wise logic)</li> <li>We need to return more complex values</li> <li>Be aware: <code>apply()</code> can change the shape</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#3-transform-use-for-group-wise-computation-shape-preservation","title":"3\ufe0f\u20e3 <code>transform()</code> \u2014 Use for Group-wise Computation &amp; Shape Preservation","text":""},{"location":"pandas_manipulation/map_apply_transform/#goal-normalize-purchase-amount-within-each-region","title":"\ud83d\udccc Goal: Normalize purchase amount within each region","text":"<pre><code>df['RegionMean'] = df.groupby('Region')['PurchaseAmount'].transform('mean')\ndf['NormalizedPurchase'] = df['PurchaseAmount'] / df['RegionMean']\n</code></pre>"},{"location":"pandas_manipulation/map_apply_transform/#another-example-z-score-of-purchases-within-each-region","title":"\ud83d\udccc Another Example: Z-score of purchases within each region","text":"<pre><code>df['Zscore'] = df.groupby('Region')['PurchaseAmount'].transform(\n    lambda x: (x - x.mean()) / x.std()\n)\n</code></pre> <p>\u2705 Use <code>transform()</code> when:</p> <ul> <li>You\u2019re doing group-wise operations</li> <li>You want to broadcast result back to each row</li> <li>You must preserve original shape</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#comparison-table","title":"\u2696\ufe0f Comparison Table","text":"Feature <code>map()</code> <code>apply()</code> <code>transform()</code> Works on Series only Series / DataFrame Series / DataFrame Acts on Individual values Rows or columns Element-wise with shape Can access multiple columns? \u274c No \u2705 Yes \u274c No Keeps original shape \u2705 Yes \u274c Not always \u2705 Yes Great for Clean-up, mapping Custom row logic Group-based engineering"},{"location":"pandas_manipulation/map_apply_transform/#bonus-chain-all-together","title":"\ud83e\uddea Bonus: Chain all together","text":"<pre><code># Map loyalty to discount\ndf['DiscountRate'] = df['LoyaltyLevel'].map(discount_map)\n\n# Format customer ID\ndf['CustomerTag'] = df['CustomerID'].map(lambda x: f'CUST-{x}')\n\n# Final amount after discount\ndf['FinalAmount'] = df.apply(\n    lambda row: row['PurchaseAmount'] * (1 - row['DiscountRate']),\n    axis=1\n)\n\n# Region-wise normalization\ndf['RegionMean'] = df.groupby('Region')['PurchaseAmount'].transform('mean')\ndf['NormalizedPurchase'] = df['PurchaseAmount'] / df['RegionMean']\n\n# Z-score\ndf['Zscore'] = df.groupby('Region')['PurchaseAmount'].transform(\n    lambda x: (x - x.mean()) / x.std()\n)\n\n# High-value tag\ndf['HighValueTag'] = df.apply(\n    lambda row: 'VIP' if row['FinalAmount'] &gt; 300 else 'Regular',\n    axis=1\n)\n</code></pre>"},{"location":"pandas_manipulation/map_apply_transform/#add-calculated-columns-based-on-the-combined-logic-from-the-markdown-example","title":"Add calculated columns based on the combined logic from the markdown example","text":""},{"location":"pandas_manipulation/map_apply_transform/#discount-mapping","title":"Discount mapping","text":"<p>discount_map = {'Gold': 0.10, 'Silver': 0.05, 'Platinum': 0.15, 'Bronze': 0.02} df_extended['DiscountRate'] = df_extended['LoyaltyLevel'].map(discount_map)</p>"},{"location":"pandas_manipulation/map_apply_transform/#customer-tag","title":"Customer tag","text":"<p>df_extended['CustomerTag'] = df_extended['CustomerID'].map(lambda x: f'CUST-{x}')</p>"},{"location":"pandas_manipulation/map_apply_transform/#final-amount-after-discount","title":"Final amount after discount","text":"<p>df_extended['FinalAmount'] = df_extended.apply(     lambda row: row['PurchaseAmount'] * (1 - row['DiscountRate']),     axis=1 )</p>"},{"location":"pandas_manipulation/map_apply_transform/#region-wise-mean-and-normalization","title":"Region-wise mean and normalization","text":"<p>df_extended['RegionMean'] = df_extended.groupby('Region')['PurchaseAmount'].transform('mean') df_extended['NormalizedPurchase'] = df_extended['PurchaseAmount'] / df_extended['RegionMean']</p>"},{"location":"pandas_manipulation/map_apply_transform/#region-wise-z-score","title":"Region-wise Z-score","text":"<p>df_extended['Zscore'] = df_extended.groupby('Region')['PurchaseAmount'].transform(     lambda x: (x - x.mean()) / x.std() )</p>"},{"location":"pandas_manipulation/map_apply_transform/#high-value-customer-tagging","title":"High-value customer tagging","text":"<p>df_extended['HighValueTag'] = df_extended.apply(     lambda row: 'VIP' if row['FinalAmount'] &gt; 300 else 'Regular',     axis=1 )</p> CustomerID Region PurchaseAmount LoyaltyLevel DiscountRate CustomerTag FinalAmount RegionMean NormalizedPurchase Zscore HighValueTag 101 East 250 Gold 0.10 CUST-101 225.00 166.67 1.50 1.0911 Regular 102 West 300 Silver 0.05 CUST-102 285.00 416.67 0.72 -1.1209 Regular 103 East 150 Gold 0.10 CUST-103 135.00 166.67 0.90 -0.2182 Regular 104 South 400 Platinum 0.15 CUST-104 340.00 375.00 1.07 0.7071 VIP 105 West 500 Silver 0.05 CUST-105 475.00 416.67 1.20 0.8006 VIP 106 East 100 Bronze 0.02 CUST-106 98.00 166.67 0.60 -1.3093 Regular 107 South 350 Gold 0.10 CUST-107 315.00 375.00 0.93 -0.7071 VIP 108 West 450 Silver 0.05 CUST-108 427.50 416.67 1.02 0.3203 VIP <p>Absolutely \u2014 let\u2019s talk about why <code>NormalizedPurchase</code> is useful in real-world data work:</p>"},{"location":"pandas_manipulation/map_apply_transform/#what-is-normalizedpurchase","title":"\ud83c\udfaf What is <code>NormalizedPurchase</code>?","text":"<p>This column was created by:</p> <pre><code>df['NormalizedPurchase'] = df['PurchaseAmount'] / df['RegionMean']\n</code></pre> <p>It shows how a customer\u2019s purchase compares to the average purchase amount in their region.</p>"},{"location":"pandas_manipulation/map_apply_transform/#why-is-this-useful","title":"\ud83d\udca1 Why is this useful?","text":""},{"location":"pandas_manipulation/map_apply_transform/#1-contextual-comparison","title":"\u2705 1. Contextual Comparison","text":"<p>A customer who spent \\$400:</p> <ul> <li>May be above average in one region</li> <li>But below average in another</li> </ul> <p><code>NormalizedPurchase</code> allows you to compare spending behavior relative to regional norms, instead of using raw numbers.</p>"},{"location":"pandas_manipulation/map_apply_transform/#2-fair-scoring-in-regional-campaigns","title":"\u2705 2. Fair Scoring in Regional Campaigns","text":"<p>If you\u2019re running a reward campaign:</p> <ul> <li>Simply picking \"top spenders\" may favor high-spending regions (like West)</li> <li>Using <code>NormalizedPurchase</code> helps pick outstanding customers in every region fairly</li> </ul> <p>Example: A customer with <code>NormalizedPurchase = 1.5</code> is spending 50% more than the average customer in their region.</p>"},{"location":"pandas_manipulation/map_apply_transform/#3-outlier-detection","title":"\u2705 3. Outlier Detection","text":"<p>Customers with:</p> <ul> <li>Values \u226b 1 are potential high-value or flagship customers</li> <li>Values \u226a 1 might be inactive, or at-risk customers</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#4-feature-engineering-for-ml","title":"\u2705 4. Feature Engineering for ML","text":"<p>If you build models (e.g., churn prediction, segmentation), using <code>NormalizedPurchase</code>:</p> <ul> <li>Removes region bias</li> <li>Helps the model learn behavior patterns, not raw differences</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#tldr","title":"\ud83e\udde0 TL;DR","text":"Raw Purchase Normalized Purchase Meaning \\$400 1.5 50% above region average \\$300 0.75 25% below region average \\$500 1.2 20% above region average <p>\u2705 Use <code>NormalizedPurchase</code> to level the playing field, understand customer behavior, and make region-aware decisions.</p>"},{"location":"pandas_manipulation/notes_on_melt/","title":"\ud83d\udcc9 Pandas <code>melt()</code> Explained (Wide to Long Format)","text":"<p>The <code>pandas.melt()</code> function is used to reshape a dataframe from wide format to long format. This is useful when you want to gather multiple columns into key-value pairs for easier analysis or visualization.</p>"},{"location":"pandas_manipulation/notes_on_melt/#example-use-case","title":"\ud83e\uddea Example Use Case","text":"<p>Suppose you have a dataframe named <code>df_main</code> with the shape <code>(2, 4)</code> and columns like:</p> <ul> <li>Before melting:</li> </ul> SAM Year Species31 Species61 A 2021 10 5 B 2021 0 3 <pre><code>df_long = pd.melt(\n    df_wide,\n    id_vars=['SAM', 'Year'], # cols to keep fixed\n    value_vars=['Species31', 'Species61'], # cols names from old df_wide we like to melt\n    var_name='Species', # name of new colum \n    value_name='Catches' # name of new column that stores the value from those melted\n)\n</code></pre> <ul> <li>After melting:</li> </ul> SAM Year Species Catch A 2021 Species31 10 B 2021 Species31 0 A 2021 Species61 5 B 2021 Species61 3"},{"location":"pandas_manipulation/notes_on_melt/#parameter-breakdown-of-pdmeltdf_to_be_melted-id_vars-value_vars-var_name-value_name","title":"parameter breakdown of <code>pd.melt(df_to_be_melted, id_vars, value_vars, var_name, value_name)</code>","text":"Parameter Description <code>id_vars</code> Columns to keep fixed (e.g., metadata like <code>SAM</code>, <code>Year</code>, etc.) <code>value_vars</code> The column names you want to melt (e.g., <code>Species31</code>, <code>Species61</code>, ...) <code>var_name</code> Name for the new column that stores the melted column names (e.g., <code>'Species'</code>) <code>value_name</code> Name for the new column that stores the values from those melted columns (e.g., <code>'Catch'</code>)"},{"location":"pandas_manipulation/notes_on_melt/#summary","title":"Summary","text":"<p>Use <code>melt()</code> to convert wide data (many similar columns) into long format (fewer columns, more rows).</p> <ul> <li> <p>Great for tidy data, easier filtering, plotting, or exporting.</p> </li> <li> <p>Keep identifier columns with id_vars.</p> </li> <li> <p>Collapse similar columns with value_vars.</p> </li> <li> <p>Control output names with <code>var_name</code> and <code>value_name</code>.</p> </li> </ul>"},{"location":"pandas_manipulation/pandas_pivot_reshape/","title":"\ud83d\udc3c Pandas Data Transformation Cheat Sheet","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#example-dataset","title":"\ud83d\udcd8 Example Dataset","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = {\n    'Date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02'],\n    'City': ['Toronto', 'Vancouver', 'Toronto', 'Vancouver'],\n    'Temperature': [5, 7, 6, 8],\n    'Humidity': [65, 70, 66, 72]\n}\ndf = pd.DataFrame(data)\n</code></pre> <pre><code>         Date      City  Temperature  Humidity\n0  2024-01-01   Toronto            5        65\n1  2024-01-01  Vancouver           7        70\n2  2024-01-02   Toronto            6        66\n3  2024-01-02  Vancouver           8        72\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#1-pivot_table","title":"1. <code>pivot_table()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-aggregating-reshaping-data-by-indexcolumn","title":"\u2705 When to use: Aggregating + reshaping data by index/column","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-what-is-the-average-temperature-and-humidity-for-each-city","title":"\u2753 Question it solves: \"What is the average Temperature and Humidity for each City?\"","text":"<pre><code>pivot_df = df.pivot_table(index='City', values=['Temperature', 'Humidity'], aggfunc='mean')\n</code></pre> <pre><code>            Humidity  Temperature\nCity                            \nToronto          65.5         5.5\nVancouver        71.0         7.5\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#2-pivot","title":"2. <code>pivot()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-reshaping-without-aggregation-must-be-unique-combinations","title":"\u2705 When to use: Reshaping without aggregation (must be unique combinations)","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-how-can-i-see-temperature-values-by-date-and-city-in-a-matrix","title":"\u2753 Question it solves: \"How can I see Temperature values by Date and City in a matrix?\"","text":"<pre><code>pivot_df = df.pivot(index='Date', columns='City', values='Temperature')\n</code></pre> <pre><code>City         Toronto  Vancouver\nDate                          \n2024-01-01        5          7\n2024-01-02        6          8\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#3-groupby","title":"3. <code>groupby()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-grouping-aggregation-without-reshaping","title":"\u2705 When to use: Grouping + aggregation without reshaping","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-what-is-the-average-temperature-for-each-city","title":"\u2753 Question it solves: \"What is the average Temperature for each City?\"","text":"<pre><code>city_group = df.groupby('City')['Temperature'].mean()\n</code></pre> <pre><code>City\nToronto      5.5\nVancouver    7.5\nName: Temperature, dtype: float64\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#4-melt","title":"4. <code>melt()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-unpivot-a-wide-table-into-long-format","title":"\u2705 When to use: Unpivot a wide table into long format","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-how-can-i-convert-columns-like-temperature-humidity-into-rows-for-easier-plotting-or-analysis","title":"\u2753 Question it solves: \"How can I convert columns (like Temperature, Humidity) into rows for easier plotting or analysis?\"","text":"<pre><code>melted = pd.melt(df, id_vars=['Date', 'City'], value_vars=['Temperature', 'Humidity'], \n                 var_name='Measurement', value_name='Value')\n</code></pre> <pre><code>         Date      City Measurement  Value\n0  2024-01-01   Toronto  Temperature      5\n1  2024-01-01  Vancouver  Temperature      7\n2  2024-01-02   Toronto  Temperature      6\n3  2024-01-02  Vancouver  Temperature      8\n4  2024-01-01   Toronto     Humidity     65\n5  2024-01-01  Vancouver     Humidity     70\n6  2024-01-02   Toronto     Humidity     66\n7  2024-01-02  Vancouver     Humidity     72\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-melt-for-visualization","title":"\ud83d\udcca When to use <code>melt()</code> for visualization","text":"<p><code>melt()</code> is especially useful for Seaborn or Matplotlib when you want to: - Plot multiple measurements on the same axes - Use <code>hue</code> for different measurements</p> <pre><code>sns.lineplot(data=melted, x='Date', y='Value', hue='Measurement')\nplt.title('Line Plot of Temperature and Humidity')\nplt.show()\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#5-stack","title":"5. <code>stack()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-convert-columns-to-a-row-wise-multiindex-wide-long","title":"\u2705 When to use: Convert columns to a row-wise MultiIndex (wide \u2192 long)","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-how-can-i-convert-the-columns-of-a-pivoted-table-into-row-level-entries","title":"\u2753 Question it solves: \"How can I convert the columns of a pivoted table into row-level entries?\"","text":"<pre><code>pivot_df = df.pivot(index='Date', columns='City', values='Temperature')\nstacked = pivot_df.stack()  # MultiIndex: Date + City\n</code></pre> <pre><code>Date        City     \n2024-01-01  Toronto      5\n            Vancouver    7\n2024-01-02  Toronto      6\n            Vancouver    8\ndtype: int64\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#6-unstack","title":"6. <code>unstack()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-convert-inner-level-of-row-index-to-column-level-long-wide","title":"\u2705 When to use: Convert inner level of row index to column level (long \u2192 wide)","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-how-can-i-reverse-the-effect-of-stack-and-get-back-the-original-column-layout","title":"\u2753 Question it solves: \"How can I reverse the effect of <code>stack()</code> and get back the original column layout?\"","text":"<pre><code>unstacked = stacked.unstack()  # City becomes columns again\n</code></pre> <pre><code>City         Toronto  Vancouver\nDate                          \n2024-01-01        5          7\n2024-01-02        6          8\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#summary-table","title":"\ud83d\udd01 Summary Table","text":"Function Use Case Reshape Aggregation Solves What Question? Suitable for Plotting <code>pivot()</code> Reshape data (must be unique keys) \u2705 \u274c See data matrix across dimensions Wide format <code>pivot_table()</code> Reshape + aggregate \u2705 \u2705 Average/summarize across categories Wide format <code>groupby()</code> Group + aggregate \u274c \u2705 Summary stats per group Use with aggregation <code>melt()</code> Wide \u2192 long \u2705 \u274c Flatten data for analysis/plotting \u2705 (long format) <code>stack()</code> Columns \u2192 row MultiIndex \u2705 \u274c Convert wide to nested long format Not commonly used <code>unstack()</code> Row MultiIndex \u2192 columns \u2705 \u274c Reverse of stacking Not commonly used"},{"location":"pandas_manipulation/top_3genre_avg_rating/","title":"Top 3 Actors by Genre Average Rating","text":"<p>Identify the top 3 actors with the highest average movie ratings within their most frequent (i.e., top) genre. For each actor, return their name, top genre, and average rating in that genre.</p> <p>Conditions: * If an actor has multiple top genres (same number of appearances), choose the one with the highest average rating. * If multiple actors share the same average rating (i.e., a tie in rank), include all tied actors, even if this results in more than 3 total actors. Do not skip rank: if all tied actors share the same rank, the next rank follows sequentially.</p> <p>DataFrame: top_actors_rating actor_name: object genre: object movie_rating: float64 movie_title: object release_date: datetime64[ns] production_company: object</p> actor_name genre movie_rating movie_title release_date production_company Ryan Gosling drama 9 Urban Hunt 2017-07-03 Google Ryan Gosling sci-fi 8.9 Veil of Secrets 2015-12-23 Apple Chris Evans drama 6.1 Crimson Chase 2017-08-10 Apple"},{"location":"pandas_manipulation/top_3genre_avg_rating/#sample-data-top_actors_rating","title":"\ud83c\udfac Sample Data: <code>top_actors_rating</code>","text":"actor_name genre movie_rating movie_title release_date production_company Tom Hanks Drama 8.5 Movie A 2000-01-01 Studio 1 Tom Hanks Comedy 7.0 Movie B 2002-01-01 Studio 1 Emma Stone Comedy 9.0 Movie C 2010-01-01 Studio 2 Emma Stone Romance 8.8 Movie D 2011-01-01 Studio 2 Denzel Washington Thriller 8.8 Movie E 2005-01-01 Studio 3 <pre><code>import pandas as pd\n\n# group by actor and genre. because if an actor has multiple genre, we take one with highest average rating\n\ngroupby_actor_genre = top_actors_ratings.groupby(['actor_name', 'genre']).agg(count_genre=('genre':'count'), avg_rating=('movie_rating':'mean')).reset_index()\n</code></pre>"},{"location":"pandas_manipulation/types_of_aggregation/","title":"\ud83e\uddee pandas Aggregation &amp; Transformation Functions Cheat Sheet","text":"<p>This note covers different pandas aggregation functions (<code>agg()</code>, <code>apply()</code>, <code>transform()</code>) with examples, outputs, and explanations for your MkDocs.</p>"},{"location":"pandas_manipulation/types_of_aggregation/#sample-dataframe","title":"\ud83d\udce5 Sample DataFrame","text":"group value1 value2 value3 A 10 100 5 A 15 150 3 B 10 50 6 B 20 60 2 C 15 200 7 C 30 250 8"},{"location":"pandas_manipulation/types_of_aggregation/#1-agg-aggregate-with-built-in-and-custom-functions","title":"1\ufe0f\u20e3 <code>agg()</code> - Aggregate with built-in and custom functions","text":"<pre><code>df.agg({\n    'value1': ['min', 'max', 'mean'],\n    'value2': ['mean', 'std'],\n    'value3': lambda x: (x &gt; 4).sum()\n})\n</code></pre> value1 value2 value3 min 10 max 30 mean 16.67 mean 105.83 4 <p>Explanation:</p> <ul> <li>Multiple built-in functions per column.</li> <li>Custom lambda counts values &gt; 4 in <code>value3</code>.</li> </ul>"},{"location":"pandas_manipulation/types_of_aggregation/#2-groupbyagg-group-wise-aggregation","title":"2\ufe0f\u20e3 <code>groupby().agg()</code> - Group-wise aggregation","text":"<pre><code>df.groupby('group').agg({\n    'value1': ['mean', 'sum'],\n    'value3': lambda x: (x &gt; 5).sum()\n})\n</code></pre> group value1 mean value1 sum value3 (count &gt;5) A 12.5 25 0 B 15.0 30 1 C 22.5 45 2 <p>Explanation: Aggregation by group with mixed functions.</p>"},{"location":"pandas_manipulation/types_of_aggregation/#3-apply-apply-any-function-across-dataframe-or-group","title":"3\ufe0f\u20e3 <code>apply()</code> - Apply any function across DataFrame or group","text":"<pre><code>def range_func(x):\n    return x.max() - x.min()\n\n# Apply to entire DataFrame numeric columns\ndf[['value1', 'value2']].apply(range_func)\n</code></pre> value1 value2 20 200 <pre><code># Apply custom function on groupby object (sum of range per group)\ndf.groupby('group').apply(lambda g: g[['value1', 'value2']].apply(range_func))\n</code></pre> group value1 value2 A 5 50 B 10 10 C 15 50 <p>Explanation:</p> <ul> <li><code>apply()</code> can run any function, returning scalar or DataFrame.</li> <li>More flexible but sometimes slower than <code>agg()</code>.</li> </ul>"},{"location":"pandas_manipulation/types_of_aggregation/#4-transform-returns-same-shape-as-input-good-for-feature-engineering","title":"4\ufe0f\u20e3 <code>transform()</code> - Returns same shape as input, good for feature engineering","text":"<pre><code># Normalize value1 within each group (subtract mean)\ndf['value1_norm'] = df.groupby('group')['value1'].transform(lambda x: x - x.mean())\ndf\n</code></pre> group value1 value2 value3 value1_norm A 10 100 5 -2.5 A 15 150 3 2.5 B 10 50 6 -5.0 B 20 60 2 5.0 C 15 200 7 -7.5 C 30 250 8 7.5 <p>Explanation:</p> <ul> <li><code>transform()</code> outputs a result with the same index and shape as original.</li> <li>Useful for adding normalized or scaled features.</li> </ul>"},{"location":"pandas_manipulation/types_of_aggregation/#5-difference-between-agg-apply-and-transform","title":"5\ufe0f\u20e3 Difference between <code>agg()</code>, <code>apply()</code>, and <code>transform()</code>","text":"Function Input Type Output Shape Use Case <code>agg()</code> Series or DataFrame Aggregated scalar(s) or DataFrame Summary statistics or aggregation <code>apply()</code> Series or DataFrame Can be scalar, Series, or DataFrame Flexible function application, complex ops <code>transform()</code> Series or DataFrame Same shape as input Element-wise transformations preserving index"},{"location":"pandas_manipulation/types_of_aggregation/#summary","title":"Summary","text":"<ul> <li>Use <code>agg()</code> for aggregations that reduce data size (e.g., sum, mean).</li> <li>Use <code>apply()</code> for custom, flexible functions that may change shape.</li> <li>Use <code>transform()</code> when you want to return transformed data matching input shape (e.g., normalization).</li> </ul>"},{"location":"pandas_manipulation/types_of_aggregation/#renaming-columns-after-aggregation","title":"Renaming columns after aggregation","text":"<pre><code>result = df.agg({\n    'value1': 'mean',\n    'value2': 'sum'\n})\n\nresult.rename({\n    'value1': 'Avg Value1',\n    'value2': 'Total Value2'\n}, inplace=True)\n\nprint(result)\n</code></pre> Avg Value1 Total Value2 16.67 810 <p>Happy coding with pandas! \ud83d\ude80</p>"},{"location":"python/coin_combinations/","title":"Count the Number of Ways to Make a Target Amount Using Coins","text":"<p>Suppose you want to find how many ways you can make a target amount using a given list of coin denominations.</p>"},{"location":"python/coin_combinations/#example","title":"Example:","text":"<pre><code>target_amount = 4\ncoins = [2, 3]\n</code></pre>"},{"location":"python/coin_combinations/#initialization","title":"Initialization","text":"<p>We initialize a list called <code>ways</code> to store the number of ways to make each amount from <code>0</code> to <code>target_amount</code>.</p> <ul> <li><code>ways[0] = 1</code> \u2192 There's exactly one way to make amount <code>0</code>: using no coins.</li> <li>The rest are initialized to <code>0</code>.</li> </ul> <p>So initially: ways = [1, 0, 0, 0, 0]</p>"},{"location":"python/coin_combinations/#step-by-step-iteration-coin-2","title":"Step-by-Step Iteration (coin = 2)","text":"<p>Loop from <code>coin</code> to <code>target_amount</code>, updating <code>ways[amount]</code> using:</p> <pre><code>ways[amount] = ways[amount] + ways[amount - coin]\n</code></pre> Amount Calculation Updated <code>ways</code> 2 ways[2] = ways[2] + ways[0] = 0 + 1 = 1 [1, 0, 1, 0, 0] 3 ways[3] = ways[3] + ways[1] = 0 + 0 = 0 [1, 0, 1, 0, 0] 4 ways[4] = ways[4] + ways[2] = 0 + 1 = 1 [1, 0, 1, 0, 1]"},{"location":"python/coin_combinations/#python-code","title":"Python Code","text":"<pre><code>def coin_combinations(target_amount, coins):\n    # Initialize array to store number of ways for each amount\n    ways = [0] * (target_amount + 1)\n\n    # Base case: one way to make amount 0\n    ways[0] = 1\n\n    # Loop through each coin\n    for coin in coins:\n        for amount in range(coin, target_amount + 1):\n            ways[amount] += ways[amount - coin]\n\n    return ways[target_amount]\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(coin_combinations(4, [1, 2, 3]))  # Output: 4\n    # Ways: [1,1,1,1], [1,1,2], [2,2], [1,3]\n\n    print(coin_combinations(5, [1, 2, 5]))  # Output: 4\n    # Ways: [1,1,1,1,1], [1,1,1,2], [1,2,2], [5]\n</code></pre>"},{"location":"python/find_unique_element/","title":"Python: Two-Pointer Technique to Find Unique Elements","text":""},{"location":"python/find_unique_element/#objective","title":"Objective","text":"<p>Use two-pointer movement to identify unique elements from a sorted list of integers.</p>"},{"location":"python/find_unique_element/#problem-statement","title":"Problem Statement","text":"<p>Given a sorted list of integers, return a list of unique elements (i.e., remove duplicates) using the two-pointer technique.</p>"},{"location":"python/find_unique_element/#example","title":"Example","text":"<ul> <li> <p>Input nums = [1, 1, 2, 2, 3, 4, 4, 5]</p> </li> <li> <p>Output nums = [1, 2, 3, 4, 5]</p> </li> </ul>"},{"location":"python/find_unique_element/#code-snippet","title":"Code Snippet","text":"<pre><code>\ndef remove_duplicates(nums):\n    if not nums:\n        return []\n\n    i = 0  # slow pointer, keep track of the last unique number's index\n\n    for j in range(1, len(nums)):  # fast pointer, iterate through the list\n\n        if nums[j] != nums[i]:\n            i += 1\n            nums[i] = nums[j]\n\n    return nums[:i+1]\n</code></pre>"},{"location":"python/find_unique_element/#execution-steps","title":"Execution Steps","text":"Step i j num[i] num[j] num[j] != num[i] Action nums 0 0 1 1 1 False [1,1,2,2,3,4,4,5] 1 0 2 1 2 True i += 1 [1,2,2,2,3,4,4,5] 2 1 3 2 2 True i += 1 [1,2,3,2,3,4,4,5] 3 2 4 3 3 True i += 1 [1,2,3,4,3,4,4,5] 4 3 5 4 4 True i += 1 [1,2,3,4,5,4,4,5] 5 4 6 5 5 True i += 1 [1,2,3,4,5,5,4,5]"},{"location":"python/find_unique_element/#how-it-works","title":"How it works","text":"<ol> <li>Initialization: Start with two pointers, <code>i</code> (slow) and <code>j</code> (fast). <code>i</code> keeps track of the last unique element's index, while <code>j</code> iterates through the list.</li> <li>Comparison: For each element at index <code>j</code>, check if it is different from the element at index <code>i</code>. If it is, increment <code>i</code> and update <code>nums[i]</code> with <code>nums[j]</code>.  </li> <li>Result: The unique elements are now at the beginning of the list, and the length of the unique elements is <code>i + 1</code>.</li> <li>Return: Return the list of unique elements by slicing <code>nums</code> up to <code>i + 1</code>.</li> </ol>"},{"location":"python/square_root/","title":"Find the Integer Square Root Without Using <code>sqrt()</code>","text":""},{"location":"python/square_root/#the-problem","title":"\u2705 The Problem","text":"<p>Write a function in Python to find the square root of a non-negative integer, rounded down to the nearest whole number.</p> <ul> <li>\u274c You cannot use the built-in <code>math.sqrt()</code> function.</li> <li>\u2705 The result should be the largest whole number whose square is less than or equal to the input.</li> </ul>"},{"location":"python/square_root/#example","title":"Example","text":"Input Expected Output Why? 9 3 3 \u00d7 3 = 9 15 3 3 \u00d7 3 = 9 &lt; 15, 4\u00d74=16 is too big 16 4 4 \u00d7 4 = 16 24 4 4 \u00d7 4 = 16 &lt; 24, 5\u00d75=25 is too big"},{"location":"python/square_root/#the-idea-use-binary-search","title":"\ud83d\udee0\ufe0f The Idea: Use Binary Search","text":""},{"location":"python/square_root/#why-binary-search","title":"Why Binary Search?","text":"<p>We're looking for a number between 1 and <code>n</code> that, when squared, is less than or equal to <code>n</code>. This makes it a perfect job for binary search, which quickly narrows down a range.</p>"},{"location":"python/square_root/#step-by-step-logic","title":"\ud83d\udd04 Step-by-Step Logic","text":""},{"location":"python/square_root/#initialize","title":"Initialize","text":"<ul> <li><code>left = 1</code></li> <li><code>right = n // 2</code> (because no square root of <code>n</code> is larger than <code>n/2</code>, unless <code>n</code> is 0 or 1)</li> </ul>"},{"location":"python/square_root/#loop-while-left-right","title":"Loop While <code>left &lt;= right</code>","text":"<ol> <li><code>mid = (left + right) // 2</code></li> <li>Check <code>mid * mid</code>:</li> <li>\u2705 If it's equal to <code>n</code>, return <code>mid</code> (exact square root!)</li> <li>\u2795 If it's less than <code>n</code>, it might be the answer - try bigger numbers: <code>left = mid + 1</code></li> <li>\u2796 If it's more than <code>n</code>, it's too big - try smaller numbers: <code>right = mid - 1</code></li> </ol>"},{"location":"python/square_root/#when-the-loop-ends","title":"When the loop ends","text":"<ul> <li>Return <code>right</code>. This is the biggest number such that <code>right * right &lt;= n</code></li> </ul>"},{"location":"python/square_root/#python-code","title":"\ud83e\uddea Python Code","text":"<pre><code>def integer_sqrt(n):\n    if n &lt; 2:\n        return n  # Handles 0 and 1 directly\n\n    left, right = 1, n // 2\n\n    while left &lt;= right:\n        mid = (left + right) // 2\n        if mid * mid == n:\n            return mid\n        elif mid * mid &lt; n:\n            left = mid + 1\n        else:\n            right = mid - 1\n\n    return right  # right is the floor of the square root\n</code></pre> <pre><code>print(integer_sqrt(0))   # 0\nprint(integer_sqrt(1))   # 1\nprint(integer_sqrt(9))   # 3\nprint(integer_sqrt(15))  # 3\nprint(integer_sqrt(16))  # 4\nprint(integer_sqrt(24))  # 4\nprint(integer_sqrt(25))  # 5\nprint(integer_sqrt(100)) # 10\n</code></pre>"}]}