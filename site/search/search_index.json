{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Science Essential Notes","text":"<p>Driven by a passion for making complex ideas simple, I\u2019ve formatted useful data science notes for quick reference, revision, and real-world use.</p> <p>I hope you find them concise, practical, and helpful.</p> <p>Start exploring by clicking on the tabs above!</p>"},{"location":"#whats-inside","title":"What's Inside","text":"<ul> <li> <p>Python   Some useful algorithms.</p> </li> <li> <p>SQL   Real-world query examples covering data retrieval, join, groupby, aggregation, &amp; window functions.</p> </li> <li> <p>Applied Statistics   Key concepts like p-values, statistical power, A/B testing, and bootstrapping\u2014explained simply.</p> </li> <li> <p>Basic Business Metrics   Definitions and use cases of essential KPIs such as conversion rate, churn, and customer lifetime value (LTV).</p> </li> <li> <p>Git Commands   Frequently used Git commands for efficient version control.</p> </li> <li>Pandas dataframe wrangling   map(), apply(), transform(), pivot_table(), melt()</li> <li>MLOps Basics   Beginner-friendly notes on Docker, FastAPI, and Flask for building deployable ML solutions.</li> <li>ML Pipeline</li> </ul>"},{"location":"AB_testing/AB_testing/","title":"A/B Testing","text":"<p>A/B testing is a statistical method used to determine whether a new variant (e.g., a website design, pricing model, or treatment) results in a measurable improvement over an existing version (the control).</p> <p>It helps answer questions such as:</p> <ul> <li>Does the new website design increase conversion rates for an e-commerce store?</li> <li>Does a new drug improve recovery rates in a clinical trial?</li> <li>Does offering free shipping increase average order value in a retail business?</li> <li>Does a personalized email campaign increase click-through rates in a marketing platform?</li> <li>Does a redesigned onboarding flow reduce user churn for a SaaS product?</li> <li>Does a new pricing model improve subscription upgrades in a fintech app?</li> </ul>"},{"location":"AB_testing/AB_testing/#steps-to-conduct-an-ab-test","title":"Steps to Conduct an A/B Test","text":""},{"location":"AB_testing/AB_testing/#1-formulate-hypotheses-and-set-significance-level","title":"1. Formulate Hypotheses and Set Significance Level","text":"<ul> <li>Null Hypothesis (H\u2080): There is no difference between the control and variant.</li> <li>Alternative Hypothesis (H\u2081): There is a statistically significant difference.</li> <li>Set the significance level (\u03b1), typically 0.05.</li> </ul>"},{"location":"AB_testing/AB_testing/#2-random-assignment-and-isolation","title":"2. Random Assignment and Isolation","text":"<ul> <li>Randomly assign users into:</li> <li>Group A (Control): Existing version</li> <li>Group B (Treatment/Variant): New version</li> <li> <p>Ensure both groups are balanced in key features (e.g., device type, geography, traffic source).</p> </li> <li> <p>Isolation ensures that users in one group (A or B) do not influence the behavior or outcomes of users in the other group.</p> </li> <li>Why isolation matters: Without isolation, results may be biased due to spillover effects, where the treatment indirectly impacts the control group (or vice versa).</li> <li>Example: Suppose you're testing a new referral program (Treatment B). If a user in Group B refers a user in Group A, the latter might behave like a Treatment user, contaminating the control group and invalidating the results.</li> <li>Best practice: Assign at the user level (not session level), and avoid shared environments (e.g., shared devices or accounts) when possible.</li> <li>Ensure users in both groups are using the same operating system (e.g., only iOS or only Android) to eliminate platform-related variability.</li> </ul>"},{"location":"AB_testing/AB_testing/#3-ensure-adequate-sample-size-and-power","title":"3. Ensure Adequate Sample Size and Power","text":"<ul> <li>Before running an A/B test, make sure you have enough users (sample size) to detect a meaningful difference.</li> <li>Effect size (lift): This is the minimum improvement you want to be able to detect \u2014 for example, a 5% increase in conversion rate.</li> <li>Baseline conversion rate: The current conversion rate without any changes (Control group).</li> <li>Power: The chance of correctly detecting a real effect (usually set at 80% or higher). If power is too low, you might miss real improvements.</li> <li>Why it matters: If your sample size is too small, the test may not detect true differences \u2014 leading to false negatives.</li> </ul>"},{"location":"AB_testing/AB_testing/#how-to-calculate-sample-size-simply-in-python-with-statsmodels","title":"How to calculate sample size simply in Python with <code>statsmodels</code>:","text":"<pre><code>from statsmodels.stats.power import NormalIndPower\nfrom statsmodels.stats.proportion import proportion_effectsize\n\n# Baseline conversion rate (control)\np1 = 0.10  # e.g., 10%\n# Expected conversion rate after change (treatment)\np2 = 0.12  # e.g., 12%, which is a 2% absolute lift\n\n# Calculate effect size (Cohen's h)\neffect_size = proportion_effectsize(p2, p1)\n\n# Set power and significance level (alpha)\npower = 0.8        # 80% chance to detect effect\nalpha = 0.05       # 5% chance of false positive (Type I error)\n\n# Initialize power analysis object\nanalysis = NormalIndPower()\n\n# Calculate required sample size per group\nsample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=1)\n\nprint(f\"Sample size needed per group: {int(sample_size)}\")\n</code></pre>"},{"location":"AB_testing/AB_testing/#4-run-the-test-for-sufficient-duration","title":"4. Run the Test for Sufficient Duration","text":"<ul> <li>The test should run long enough to:</li> <li>Cover natural behavior cycles (weekends, holidays)</li> <li>Avoid early stopping or \"peeking\"</li> </ul>"},{"location":"AB_testing/AB_testing/#5-analyze-results-using-p-value","title":"5. Analyze Results Using p-value","text":"<ul> <li>Calculate the p-value.</li> <li>If p &lt; \u03b1, reject the null hypothesis and conclude the effect is statistically significant.</li> </ul>"},{"location":"AB_testing/AB_testing/#6-check-guardrail-metrics","title":"6. Check Guardrail Metrics","text":"<ul> <li>Monitor secondary metrics such as:</li> <li>Bounce rate</li> <li>Customer complaints</li> <li>Load times</li> <li>Ensure there are no unintended negative impacts.</li> </ul>"},{"location":"AB_testing/AB_testing/#7-decide-whether-to-roll-out","title":"7. Decide Whether to Roll Out","text":"<ul> <li>If the result is:</li> <li>Statistically significant</li> <li>Practically meaningful</li> <li>No harm to guardrails</li> </ul> <p>Then, proceed with deploying the new version.</p>"},{"location":"AB_testing/AB_testing/#additional-notes","title":"Additional Notes","text":"<ul> <li>Multiple Testing: Adjust for multiple comparisons (e.g., Bonferroni correction).</li> <li>Effect Size vs. Statistical Significance: Small p-values do not always mean large or meaningful effects.</li> <li>Practical Significance: Consider the business impact beyond statistical metrics.</li> </ul>"},{"location":"AB_testing/AB_testing/#summary-table","title":"Summary Table","text":"Concept Description Example Null Hypothesis (H\u2080) No difference between control and variant \"Conversion rate in group A = group B\" Alternative Hypothesis (H\u2081) Statistically significant difference exists \"Conversion rate in group B &gt; group A\" \u03b1 (Significance Level) Probability of false positive (Type I error), typically 0.05 If p-value &lt; 0.05, reject H\u2080 Power Probability of detecting a true effect, typically 80% 80% chance of detecting a 5% lift if it exists Effect Size (Lift) The measurable difference between variant and control Group A: 10% CR, Group B: 12% CR \u2192 Effect size = 2% Guardrail Metrics Secondary metrics to monitor for unintended consequences Bounce rate, time on site, refund requests, site load time"},{"location":"Applied_statistics/AB_testing/","title":"A/B Testing","text":"<p>A/B testing is a statistical method used to determine whether a new variant (e.g., a website design, pricing model, or treatment) results in a measurable improvement over an existing version (the control).</p> <p>It helps answer questions such as:</p> <ul> <li>Does the new website design increase conversion rates for an e-commerce store?</li> <li>Does a new drug improve recovery rates in a clinical trial?</li> <li>Does offering free shipping increase average order value in a retail business?</li> <li>Does a personalized email campaign increase click-through rates in a marketing platform?</li> <li>Does a redesigned onboarding flow reduce user churn for a SaaS product?</li> <li>Does a new pricing model improve subscription upgrades in a fintech app?</li> </ul>"},{"location":"Applied_statistics/AB_testing/#steps-to-conduct-an-ab-test","title":"Steps to Conduct an A/B Test","text":""},{"location":"Applied_statistics/AB_testing/#1-formulate-hypotheses-and-set-significance-level","title":"1. Formulate Hypotheses and Set Significance Level","text":"<ul> <li>Null Hypothesis (H\u2080): There is no difference between the control and variant.</li> <li>Alternative Hypothesis (H\u2081): There is a statistically significant difference.</li> <li>Set the significance level (\u03b1), typically 0.05.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#2-random-assignment-and-isolation","title":"2. Random Assignment and Isolation","text":"<ul> <li>Randomly assign users into:</li> <li>Group A (Control): Existing version</li> <li>Group B (Treatment/Variant): New version</li> <li> <p>Ensure both groups are balanced in key features (e.g., device type, geography, traffic source).</p> </li> <li> <p>Isolation ensures that users in one group (A or B) do not influence the behavior or outcomes of users in the other group.</p> </li> <li>Why isolation matters: Without isolation, results may be biased due to spillover effects, where the treatment indirectly impacts the control group (or vice versa).</li> <li>Example: Suppose you're testing a new referral program (Treatment B). If a user in Group B refers a user in Group A, the latter might behave like a Treatment user, contaminating the control group and invalidating the results.</li> <li>Best practice: Assign at the user level (not session level), and avoid shared environments (e.g., shared devices or accounts) when possible.</li> <li>Ensure users in both groups are using the same operating system (e.g., only iOS or only Android) to eliminate platform-related variability.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#3-ensure-adequate-sample-size-and-power","title":"3. Ensure Adequate Sample Size and Power","text":"<ul> <li>Before running an A/B test, make sure you have enough users (sample size) to detect a meaningful difference.</li> <li>Effect size (lift): This is the minimum improvement you want to be able to detect \u2014 for example, a 5% increase in conversion rate.</li> <li>Baseline conversion rate: The current conversion rate without any changes (Control group).</li> <li>Power: The chance of correctly detecting a real effect (usually set at 80% or higher). If power is too low, you might miss real improvements.</li> <li>Why it matters: If your sample size is too small, the test may not detect true differences \u2014 leading to false negatives.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#how-to-calculate-sample-size-simply-in-python-with-statsmodels","title":"How to calculate sample size simply in Python with <code>statsmodels</code>:","text":"<pre><code>from statsmodels.stats.power import NormalIndPower\nfrom statsmodels.stats.proportion import proportion_effectsize\n\n# Baseline conversion rate (control)\np1 = 0.10  # e.g., 10%\n# Expected conversion rate after change (treatment)\np2 = 0.12  # e.g., 12%, which is a 2% absolute lift\n\n# Calculate effect size (Cohen's h)\neffect_size = proportion_effectsize(p2, p1)\n\n# Set power and significance level (alpha)\npower = 0.8        # 80% chance to detect effect\nalpha = 0.05       # 5% chance of false positive (Type I error)\n\n# Initialize power analysis object\nanalysis = NormalIndPower()\n\n# Calculate required sample size per group\nsample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=1)\n\nprint(f\"Sample size needed per group: {int(sample_size)}\")\n</code></pre>"},{"location":"Applied_statistics/AB_testing/#4-run-the-test-for-sufficient-duration","title":"4. Run the Test for Sufficient Duration","text":"<ul> <li>The test should run long enough to:</li> <li>Cover natural behavior cycles (weekends, holidays)</li> <li>Avoid early stopping or \"peeking\"</li> </ul>"},{"location":"Applied_statistics/AB_testing/#5-analyze-results-using-p-value","title":"5. Analyze Results Using p-value","text":"<ul> <li>Calculate the p-value.</li> <li>If p &lt; \u03b1, reject the null hypothesis and conclude the effect is statistically significant.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#6-check-guardrail-metrics","title":"6. Check Guardrail Metrics","text":"<ul> <li>Monitor secondary metrics such as:</li> <li>Bounce rate</li> <li>Customer complaints</li> <li>Load times</li> <li>Ensure there are no unintended negative impacts.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#7-decide-whether-to-roll-out","title":"7. Decide Whether to Roll Out","text":"<ul> <li>If the result is:</li> <li>Statistically significant</li> <li>Practically meaningful</li> <li>No harm to guardrails</li> </ul> <p>Then, proceed with deploying the new version.</p>"},{"location":"Applied_statistics/AB_testing/#additional-notes","title":"Additional Notes","text":"<ul> <li>Multiple Testing: Adjust for multiple comparisons (e.g., Bonferroni correction).</li> <li>Effect Size vs. Statistical Significance: Small p-values do not always mean large or meaningful effects.</li> <li>Practical Significance: Consider the business impact beyond statistical metrics.</li> </ul>"},{"location":"Applied_statistics/AB_testing/#summary-table","title":"Summary Table","text":"Concept Description Example Null Hypothesis (H\u2080) No difference between control and variant \"Conversion rate in group A = group B\" Alternative Hypothesis (H\u2081) Statistically significant difference exists \"Conversion rate in group B &gt; group A\" \u03b1 (Significance Level) Probability of false positive (Type I error), typically 0.05 If p-value &lt; 0.05, reject H\u2080 Power Probability of detecting a true effect, typically 80% 80% chance of detecting a 5% lift if it exists Effect Size (Lift) The measurable difference between variant and control Group A: 10% CR, Group B: 12% CR \u2192 Effect size = 2% Guardrail Metrics Secondary metrics to monitor for unintended consequences Bounce rate, time on site, refund requests, site load time"},{"location":"Applied_statistics/Explanation_pvalue/","title":"Explanation of p-value in statistical inferences","text":""},{"location":"Applied_statistics/Explanation_pvalue/#what-does-a-p-value-mean","title":"What does a p-value mean?","text":"<ul> <li>A p-value is the probability of obtaining test results at least as extreme as the observed results, under the assumption that the null hypothesis (H\u2080) is true.</li> <li>A p-value helps us figure out how surprising our results are if we assume the null hypothesis (H\u2080) is true.</li> <li>When we say \"at least as extreme\" we're asking: if the null hypothesis were true, what's the probability of getting results that are this unusual or even more unusual than what we actually observed?</li> <li>Think of it this way: your observed data gives you a certain amount of \"evidence against\" the null hypothesis. The p-value asks, What's the chance of getting this much evidence against the null hypothesis, or even stronger evidence, if the null hypothesis is actually true?</li> <li>Let's say we think a coin is fair (H\u2080: The coin is fair - the chance of heads is 50%). We toss it 100 times and get only 5 heads. That seems strange, right?</li> <li>The p-value tells us: \"If the coin really is fair, what\u2019s the chance we\u2019d see something as extreme as 5 heads (or fewer)?\"</li> <li>That\u2019s a one-tailed test - looking just at one end (very few heads).</li> <li>A two-tailed test would ask: \"What\u2019s the chance of getting something really extreme on either end - like 5 or fewer heads or 95 or more heads?\"</li> <li>If the p-value is really small (say, less than 0.05), that means our result is pretty unusual under the assumption of fairness. So we might start to doubt the coin is actually fair.</li> </ul> <p>Note 1: A small p-value doesn\u2019t prove the coin is unfair - it just means what we saw would be unlikely if it were fair.</p> <p>Note 2: A small p-value does not mean H\u2080 is false. It means the observed data would be unlikely under H\u2080. It means we fail to reject H$_o$.</p> <pre><code>import scipy.stats as stats\n\n# Set the parameters\nn = 100          # Number of coin tosses\np = 0.5          # Probability of heads under the null hypothesis (fair coin)\nk = 5            # Observed number of heads (very low!)\n\n# ================================================\n# One-tailed p-value:\n# We want the probability of getting k or fewer heads if the coin is fair.\n# That's why we use CDF (Cumulative Distribution Function) \u2014 it adds up\n# the probability of 0, 1, 2, ..., up to k heads.\n# If we used PDF, it would only give the probability of *exactly* 5 heads.\n# ================================================\np_value_one_tail = stats.binom.cdf(k, n, p)\nprint(\"One-tailed p-value (5 or fewer heads):\", p_value_one_tail)\n\n# ================================================\n# Two-tailed p-value:\n# We want to find how extreme the result is on *both* sides of the expected value (which is 50 heads).\n# Getting only 5 heads is 45 less than expected \u2192 delta = 45\n# So, we also look at the *other* extreme: 50 + 45 = 95 heads or more\n# CDF gives cumulative up to a value, so for 95 or more, we subtract CDF(94) from 1.\n# ================================================\n\nexpected = n * p                   # Expected number of heads = 50\ndelta = abs(k - expected)         # How far away our result (5) is from expected (50)\n\n# Left tail: probability of getting &lt;= 5 heads (already calculated, but let's redo for clarity)\np_low = stats.binom.cdf(expected - delta, n, p)  # same as stats.binom.cdf(5, 100, 0.5)\n\n# Right tail: probability of getting &gt;= 95 heads\n# Since CDF gives probability of &lt;= x, we subtract from 1 to get &gt;= 95\np_high = 1 - stats.binom.cdf(expected + delta - 1, n, p)  # 1 - CDF(94)\n\n# Add both tails to get the total two-tailed p-value\np_value_two_tail = p_low + p_high\nprint(\"Two-tailed p-value (5 or fewer OR 95 or more heads):\", p_value_two_tail)\n</code></pre>"},{"location":"Applied_statistics/ci_median/","title":"Confidence Interval (CI) for the Median using Bootstrap","text":""},{"location":"Applied_statistics/ci_median/#why-we-need-it","title":"\ud83d\udccc Why We Need It","text":"<p>The median is a robust measure of central tendency, especially in skewed distributions. However, it does not have a straightforward formula for calculating its confidence interval (unlike the mean). So, we use bootstrap resampling to estimate the confidence interval for the median.</p>"},{"location":"Applied_statistics/ci_median/#step-by-step-process","title":"\ud83d\udd22 Step-by-Step Process","text":"<ol> <li>Start with your dataset.</li> <li> <p>Example: <code>[3, 5, 7, 2, 4, 20, 7]</code></p> </li> <li> <p>Use bootstrap resampling:</p> </li> <li>Randomly sample with replacement from the dataset.</li> <li>Each bootstrap sample should be the same size as the original dataset.</li> <li> <p>Repeat this process many times (e.g., 10,000 iterations).</p> </li> <li> <p>Calculate the median of each resampled dataset and store the values.</p> </li> <li> <p>Sort the list of all bootstrap medians (optional for plotting).</p> </li> <li> <p>Compute percentiles:</p> </li> <li>For a 95% confidence interval, take the 2.5th percentile and 97.5th percentile of the bootstrap medians.</li> </ol>"},{"location":"Applied_statistics/ci_median/#correct-python-code","title":"\u2705 Correct Python Code","text":"<pre><code>import numpy as np\n\n# Original dataset\noriginal_dataset = [3, 5, 7, 2, 4, 20, 7]\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Number of bootstrap samples\nN = 10000\n\n# Store medians\nmedians = []\n\nfor _ in range(N):\n    sample = np.random.choice(original_dataset, size=len(original_dataset), replace=True)\n    medians.append(np.median(sample))\n\n# Compute 95% confidence interval\nlower, upper = np.percentile(medians, [2.5, 97.5])\n\nprint(\"95% Confidence Interval for the median:\", lower, \"to\", upper)\n</code></pre>"},{"location":"Applied_statistics/ci_median/#notes","title":"\ud83d\udd0d Notes","text":"<ul> <li>The method used here is called the percentile bootstrap method.</li> <li>You can adjust the confidence level (e.g., use [5, 95] for a 90% CI).</li> <li>Bootstrap is useful when theoretical distributions are unknown or complicated.</li> </ul>"},{"location":"Applied_statistics/ci_median/#optional-plotting-the-bootstrap-distribution","title":"\ud83d\udcca Optional: Plotting the Bootstrap Distribution","text":"<pre><code>import matplotlib.pyplot as plt\n\nplt.hist(medians, bins=50, edgecolor='k')\nplt.axvline(lower, color='red', linestyle='--', label=f'2.5% = {lower:.2f}')\nplt.axvline(upper, color='red', linestyle='--', label=f'97.5% = {upper:.2f}')\nplt.title('Bootstrap Distribution of Median')\nplt.xlabel('Median')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Applied_statistics/ci_median/#summary","title":"\ud83e\udde0 Summary","text":"<p>Bootstrap is a powerful, non-parametric tool to compute confidence intervals for statistics that don't have easy closed-form solutions \u2014 like the median. It's simple to implement and widely used in real-world data science.</p>"},{"location":"Applied_statistics/ci_median/#optional-clarification","title":"Optional Clarification","text":"<ul> <li> <p>This is called the percentile bootstrap method for estimating confidence intervals.</p> </li> <li> <p>You can also use bias-corrected and accelerated (BCa) bootstrap for improved accuracy, especially when the sampling distribution is skewed.</p> </li> </ul>"},{"location":"Common_business_metrics/common_business_metrics/","title":"Common Business metrics","text":"Term Plain Definition (with Time-Frame) Example (with Time-Frame) Conversion Rate The % of users who take a desired action (like signing up or buying) within a specific period. If 100 people visit a site in one day and 5 make a purchase, conversion rate = 5%. Click-Through Rate (CTR) The % of people who clicked on a link out of those who saw it during a campaign or time window. If 1,000 people see an ad over a week and 50 click on it, CTR = 5%. Bounce Rate The % of visitors who leave a website after viewing only one page during a session or time frame. High bounce rate last week may mean users didn\u2019t find what they wanted. Churn Rate The % of users or customers who stop using your product or cancel a subscription within a given time frame (e.g., monthly). If you have 100 customers and 5 leave in January, churn rate = 5%. Retention Rate The % of users who keep using your product after a certain time period (e.g., after 1 month). If 100 people sign up and 60 are still active after 1 month, retention = 60%. Average Order Value (AOV) The average amount spent per purchase, usually calculated over a time frame like a day or month. If 10 orders total \\$500 in a month, then AOV = \\$50. Lifetime Value (LTV) The total money a customer is expected to spend over their relationship typically measured in months or years. If a typical customer pays \\$20/month for 2 years, LTV = \\$480. Impressions The number of times your ad or content is shown to users during a campaign or time window. 1,000 people saw your ad in March = 1,000 impressions. Engagement Rate A measure of how much users interact with your content (likes, comments, clicks) within a given period. 200 likes from 1,000 views last week = 20% post engagement rate. Upsell When a customer buys a more expensive version or adds extras usually within a specific time after initial purchase. Buying a laptop and adding an extended warranty within 30 days. A/B Test Lift The difference (increase or decrease) in a metric between control and variant measured during the test period. If control conversion = 10% and variant = 12% after 2 weeks test, lift = +2%."},{"location":"MLOps/devops_notes/","title":"\ud83d\udcc1 MLOps Basics: Docker, FastAPI, and Flask","text":"<p>Welcome! This guide explains essential backend tools:</p> <ul> <li>\ud83d\udea2 Docker: Package and run your apps anywhere</li> <li>\u26a1 FastAPI: Fast, async-ready web APIs</li> <li>\ud83d\udd25 Flask: Lightweight web apps and APIs</li> </ul>"},{"location":"MLOps/devops_notes/#docker","title":"\ud83d\udea2 Docker","text":""},{"location":"MLOps/devops_notes/#what-is-docker","title":"What is Docker?","text":"<p>Docker is like a magic container box that carries your app and everything it needs (code, Python, libraries) so it runs anywhere - your laptop, server, or the cloud.</p>"},{"location":"MLOps/devops_notes/#when-to-use-docker","title":"When to Use Docker","text":"<ul> <li>You want to share your project easily with someone else.</li> <li>Your app works on your machine, but not on someone else's.</li> <li>You're deploying your app to a cloud service.</li> </ul>"},{"location":"MLOps/devops_notes/#key-concepts","title":"\ud83d\udd0d Key Concepts","text":"Term What It Means Image A snapshot/blueprint of your app and everything it needs Container A running instance of an image Dockerfile A script with steps to create an image"},{"location":"MLOps/devops_notes/#sample-dockerfile","title":"\ud83d\udcc2 Sample Dockerfile","text":"<pre><code># Use the official Python image from Docker Hub\nFROM python:3.10\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the requirements file into the container\nCOPY requirements.txt .\n\n# Install the required Python packages\nRUN pip install -r requirements.txt\n\n# Copy the rest of the application code\nCOPY . .\n\n# Set the default command to run the app\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"MLOps/devops_notes/#docker-commands","title":"\u2696\ufe0f Docker Commands","text":""},{"location":"MLOps/devops_notes/#build-docker-image","title":"Build Docker Image:","text":"<pre><code>docker build -t myapp .\n</code></pre> <ul> <li><code>-t myapp</code>: names the image <code>myapp</code></li> <li><code>.</code>: current directory contains the Dockerfile</li> </ul>"},{"location":"MLOps/devops_notes/#run-docker-container","title":"Run Docker Container:","text":"<pre><code>docker run -p 8000:8000 myapp\n</code></pre> <ul> <li><code>-p 8000:8000</code>: maps your computer's port 8000 to Docker's port 8000</li> </ul>"},{"location":"MLOps/devops_notes/#run-python-shell-inside-docker","title":"Run Python Shell Inside Docker","text":"<pre><code>docker run -it python:3.10\n</code></pre> <ul> <li><code>-it</code>: interactive terminal</li> <li>Opens Python shell inside a container</li> </ul>"},{"location":"MLOps/devops_notes/#mount-local-folder-into-container","title":"Mount Local Folder into Container","text":"<pre><code>docker run -v $(pwd):/app -w /app python:3.10 python app.py\n</code></pre> <ul> <li><code>-v $(pwd):/app</code>: mounts your current folder to <code>/app</code> inside container</li> <li><code>-w /app</code>: sets the working directory</li> </ul>"},{"location":"MLOps/devops_notes/#fastapi","title":"\u26a1 FastAPI","text":""},{"location":"MLOps/devops_notes/#what-is-fastapi","title":"\ud83e\udd14 What is FastAPI?","text":"<p>FastAPI is a Python web framework used to build APIs that are fast, async, and come with automatic documentation.</p> <p>FastAPI is a fast, modern way to build APIs with Python. It's like building an express highway \ud83d\ude97 for your data.</p>"},{"location":"MLOps/devops_notes/#when-to-use-fastapi","title":"When to Use FastAPI","text":"<ul> <li>You need a REST API for your app or machine learning model.</li> <li>You want built-in docs (OpenAPI, Swagger).</li> <li>You care about speed and async support.</li> </ul>"},{"location":"MLOps/devops_notes/#how-to-use","title":"How to Use","text":""},{"location":"MLOps/devops_notes/#step-1-install-fastapi-and-uvicorn","title":"Step 1: Install FastAPI and Uvicorn","text":"<pre><code>pip install fastapi uvicorn\n</code></pre>"},{"location":"MLOps/devops_notes/#step-2-create-mainpy","title":"Step 2: Create main.py","text":"<pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Hello FastAPI\"}\n</code></pre>"},{"location":"MLOps/devops_notes/#step-3-run-the-app","title":"Step 3: Run the App","text":"<pre><code>uvicorn main:app --reload\n</code></pre>"},{"location":"MLOps/devops_notes/#example-use-case","title":"Example Use Case","text":"<ul> <li>You built a machine learning model and want to expose a <code>/predict</code> endpoint for it. FastAPI makes this super easy.</li> </ul> Term What It Means API A way to connect programs (like a menu at a restaurant) Endpoint A specific URL that your app listens to (e.g. <code>/predict</code>) GET/POST Types of HTTP requests (read vs. send data) Pydantic Helps validate data like forms with rules Uvicorn The web server that runs FastAPI"},{"location":"MLOps/devops_notes/#a-sample-mainpy-with-comments","title":"\ud83d\udd39 A Sample <code>main.py</code> with Comments","text":"<pre><code># Import FastAPI library\nfrom fastapi import FastAPI\n\n# Create the FastAPI app instance\napp = FastAPI()\n\n# Define a GET endpoint for the homepage\n@app.get(\"/\")\ndef home():\n    # Return a JSON response\n    return {\"message\": \"Hello FastAPI\"}\n\n# Define a GET endpoint with a path parameter\n@app.get(\"/greet/{name}\")\ndef greet(name: str):\n    return {\"message\": f\"Hello, {name}!\"}\n\n# Use a query parameter with a default value\n@app.get(\"/search\")\ndef search(q: str = \"default\"):\n    return {\"result\": q}\n\n# Use a POST endpoint with data validation\nfrom pydantic import BaseModel\n\nclass Item(BaseModel):\n    name: str\n    price: float\n\n@app.post(\"/items\")\ndef create_item(item: Item):\n    # Pydantic auto-validates the item structure\n    return {\"item\": item}\n</code></pre>"},{"location":"MLOps/devops_notes/#run-fastapi-app","title":"\ud83d\udd27 Run FastAPI App","text":"<pre><code>uvicorn main:app --reload\n</code></pre> <ul> <li><code>main:app</code>: file name = <code>main.py</code>, object = <code>app</code></li> <li><code>--reload</code>: restarts server on code changes</li> </ul>"},{"location":"MLOps/devops_notes/#dockerfile-for-fastapi","title":"\ud83d\udcc2 Dockerfile for FastAPI","text":"<pre><code>FROM python:3.10\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"MLOps/devops_notes/#docker-fastapi-example","title":"\ud83e\uddea Docker + FastAPI Example","text":""},{"location":"MLOps/devops_notes/#requirementstxt","title":"requirements.txt","text":"<pre><code>fastapi\nuvicorn\n</code></pre>"},{"location":"MLOps/devops_notes/#build-and-run","title":"Build and Run","text":"<pre><code>docker build -t fastapi-app .        # Swagger UI\ndocker run -p 8000:8000 fastapi-app  # FastAPI homepage\n</code></pre>"},{"location":"MLOps/devops_notes/#access-fastapi","title":"Access (FastAPI)","text":"<ul> <li> <p>http://localhost:8000/docs \u2192 Swagger UI</p> </li> <li> <p>http://localhost:8000 \u2192 FastAPI homepage</p> </li> </ul>"},{"location":"MLOps/devops_notes/#flask","title":"\ud83d\udd25 Flask","text":""},{"location":"MLOps/devops_notes/#what-is-flask","title":"\ud83e\udd14 What is Flask?","text":"<p>Flask is a minimal Python web framework. You can build anything: a website, API, or dashboard.</p> <p>It\u2019s like a barebones food truck \ud83c\udf54 where you add features as you need.</p> Term Meaning Route A URL that triggers a function Template HTML file rendered dynamically Flask App Main object that connects routes to logic Flask CLI Command line tool to run apps (<code>flask run</code>)"},{"location":"MLOps/devops_notes/#sample-apppy-with-comments","title":"\ud83d\udd39 Sample <code>app.py</code> with Comments","text":"<pre><code># Import Flask class\nfrom flask import Flask, jsonify, render_template\n\n# Create the Flask app instance\napp = Flask(__name__)\n\n# Define a basic homepage route\n@app.route(\"/\")\ndef hello():\n    return \"Hello from Flask!\"\n\n# Define a route with a variable path\n@app.route(\"/hello/&lt;name&gt;\")\ndef greet(name):\n    return f\"Hello, {name}!\"\n\n# Define a JSON API endpoint\n@app.route(\"/api\")\ndef api_data():\n    return jsonify({\"data\": 123})\n\n# Render HTML from a template\n@app.route(\"/home\")\ndef home():\n    return render_template(\"home.html\")\n</code></pre>"},{"location":"MLOps/devops_notes/#run-flask-app","title":"\ud83d\udd27 Run Flask App","text":"<pre><code>export FLASK_APP=app.py\nflask run\n</code></pre>"},{"location":"MLOps/devops_notes/#dockerfile-for-flask","title":"\ud83d\udcc2 Dockerfile for Flask","text":"<pre><code>FROM python:3.10\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nENV FLASK_APP=app.py\nCMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n</code></pre>"},{"location":"MLOps/devops_notes/#docker-flask-example","title":"\ud83e\uddea Docker + Flask Example","text":""},{"location":"MLOps/devops_notes/#requirementstx","title":"requirements.tx","text":"<pre><code>flask\n</code></pre>"},{"location":"MLOps/devops_notes/#build-and-run-flask","title":"Build and Run Flask","text":"<pre><code>docker build -t flask-app .\ndocker run -p 5000:5000 flask-app\n</code></pre> <p>```</p>"},{"location":"MLOps/devops_notes/#access-flask","title":"Access (Flask)","text":""},{"location":"MLOps/devops_notes/#httplocalhost5000-flask-homepage","title":"* http://localhost:5000 \u2192 Flask homepage","text":""},{"location":"MLOps/devops_notes/#summary-table","title":"\ud83d\udcda Summary Table","text":"Tool Purpose Common Use Docker Package apps + environments Make sure apps run the same everywhere FastAPI High-speed web APIs ML models, web APIs with docs Flask Lightweight web framework Quick websites, APIs, dashboards"},{"location":"ML_Pipeline/ml_pipeline/","title":"Production-ready end-to-end layout for a tabular classification ML pipeline","text":"<p>\u2705 Feature Engineering \u2705 Preprocessing (Imputation + Scaling + Encoding) \u2705 Handling Imbalanced Data \u2705 Pipeline \u2705 Cross-Validation \u2705 Evaluation Metrics</p>"},{"location":"ML_Pipeline/ml_pipeline/#1-import-libraries","title":"1. Import libraries","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom imblearn.pipeline import Pipeline as ImbPipeline  # For SMOTE\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#2-load-data","title":"2. Load data","text":"<pre><code>df = pd.read_csv(\"your_dataset.csv\")  # Replace with your real dataset path\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#3-exploratory-data-aanlysis-eda","title":"3. Exploratory Data Aanlysis (EDA)","text":"<pre><code>print(df.shape)\nprint(df.dtypes)\nprint(df.isnull().sum())\nprint(df['target'].value_counts())  # replace column name 'target' as needed\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#3-feature-engineering-custom-transformation","title":"3. Feature Engineering (custom transformation)","text":"<pre><code>def add_custom_features(X):\n    X = X.copy()\n    if \"applicant_income\" in X and \"loan_amount\" in X:\n        X[\"income_to_loan_ratio\"] = X[\"applicant_income\"] / (X[\"loan_amount\"] + 1)\n    return X\n\nfeature_engineering = FunctionTransformer(add_custom_features, validate=False)\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#4-define-features-and-target-x-y","title":"4. Define Features and target: X, y","text":"<pre><code>X = df.drop(\"target\", axis=1)\ny = df[\"target\"]\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#5-train-test-split-with-stratify","title":"5. Train-Test Split With Stratify","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#6-separate-numeric-and-categorical-features","title":"6. Separate Numeric and Categorical Features","text":"<pre><code>num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\n# Add engineered column name to num_cols if added\n\nif \"income_to_loan_ratio\" not in num_cols and \"income_to_loan_ratio\" in X_train.columns:\n    num_cols.append(\"income_to_loan_ratio\")\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#7-define-transformer","title":"7. Define Transformer","text":""},{"location":"ML_Pipeline/ml_pipeline/#numerical-transformer","title":"Numerical Transformer","text":"<pre><code>num_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", StandardScaler())\n])\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#categorical-transformer","title":"Categorical Transformer","text":"<pre><code>cat_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#8-combine-with-columntransformer","title":"8. Combine with ColumnTransformer","text":"<pre><code>preprocessor = ColumnTransformer(transformers=[\n    (\"num\", num_pipeline, num_cols),\n    (\"cat\", cat_pipeline, cat_cols)\n])\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#9-model-hyperparameter-setup","title":"9. Model &amp; Hyperparameter Setup","text":"<pre><code>param_grids = {\n    \"LogisticRegression\": {\n        \"classifier\": [LogisticRegression(max_iter=1000, class_weight=\"balanced\")],\n        \"classifier__C\": [0.1, 1, 10]\n    },\n\n    \"RandomForest\": {\n        \"classifier\": [RandomForestClassifier(class_weight=\"balanced\")],\n        \"classifier__n_estimators\": [100, 200],\n        \"classifier__max_depth\": [None, 10, 20]\n    },\n\n    \"XGBoost\": {\n        \"classifier\": [XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")],\n        \"classifier__n_estimators\": [100, 200],\n        \"classifier__max_depth\": [3, 5],\n        \"classifier__learning_rate\": [0.01, 0.1]\n    }\n}\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#9-make-a-full-pipeline","title":"9. Make a full Pipeline","text":"<ul> <li>Define pipeline with a generic \"classifier\" step (can be any model)</li> </ul> <pre><code>pipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"classifier\", LogisticRegression()) # a generic placeholder\n])\n</code></pre> <ul> <li>Note Pipeline with SMOTE for imbalance handling</li> </ul> <pre><code># pipeline = ImbPipeline(steps=[\n#     (\"feature_engineering\", feature_engineering),\n#     (\"preprocessor\", preprocessor),\n#     (\"sampler\", SMOTE(random_state=42)),\n#     (\"classifier\", RandomForestClassifier(class_weight='balanced', random_state=42))\n# ])\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#10-train-with-gridsearchcv-and-select-best-model","title":"10. Train with GridSearchCV and Select Best Model","text":"<pre><code>results = {}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor name, param_grid in param_grids.items():\n\n    grid = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, scoring=\"f1_macro\", n_jobs=-1)\n\n    # fit each model\n    grid.fit(X_train, y_train)\n\n    results[name] = {\n        \"best_score\": grid.best_score_,\n        \"best_estimator\": grid.best_estimator_, # Mean cross-validated score\n        \"best_params\": grid.best_params_ # Best hyperparameters\n    }\n\n# Print comparison\nfor model_name, res in results.items():\n    print(f\"{model_name}: F1 Score = {res['best_score']:.4f}\")\n\n# Select best model\nbest_model_name = max(results, key=lambda k: results[k]['best_score'])\nbest_model = results[best_model_name][\"best_estimator_\"]\nprint(f\"Best model: {best_model_name}\")\n\n</code></pre>"},{"location":"ML_Pipeline/ml_pipeline/#11-evaluate-on-test-data-predict-and-evaluate-on-test-data","title":"11. Evaluate on test data - predict and evaluate on test data","text":"<pre><code>y_pred = best_model.predict(X_test)\ny_prob = best_model.predict_proba(X_test)[:, 1]\n\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"ROC AUC Score:\", roc_auc_score(y_test, y_prob))\n</code></pre>"},{"location":"SQL/best_paid_title/","title":"Best Paid Job Titles","text":"<p>SELECT the job titles of the highest-paid employees.Your output should list the job title or titles with the highest salary, considering the possibility of multiple titles sharing the same salary.</p> <p>worker table:</p> Field Description worker_id Identifier of worker first_name First name of worker last_name Last name of worker salary Salary of worker joining_date Date of joining department Department name <p>title table:</p> Field Description title_id ID of the title worker_ref_id Identifier of worker worker_title Name of the position affected_from Date of modification <pre><code>with tbl_join_worker_title AS\n(SELECT W.WORKER_ID, W.SALARY, T.WORKER_TITLE,\nDENSE_RANK() OVER(ORDER BY W.SALARY desc) AS RNK\nFROM WORKER AS W \nINNER JOIN TITLE T ON W.WORKER_ID = T.WORKER_REF_ID\n)\nSELECT WORKER_TITLE AS BEST_PAID_TITLE\nFROM tbl_join_worker_title \nWHERE RNK=1 \n</code></pre>"},{"location":"SQL/consecutive_login_days/","title":"Consecutive log-in streak","text":"<p>Absolutely! Let\u2019s expand the markdown with a detailed example involving 3 users: <code>1032</code>, <code>1050</code>, and <code>1002</code>. This will help you clearly see how the <code>streak_marker</code>, <code>streak_id</code>, and final output work.</p> <p>Here\u2019s the updated and elaborated markdown example:</p> <pre><code># SQL Walkthrough: Finding Users with 10+ Consecutive Login Days\n\nThis walkthrough explains how to identify users who have logged in for **at least 10 consecutive days**, using SQL Common Table Expressions (CTEs). We show the process **step-by-step**, using a sample dataset for 3 users: `1032`, `1050`, and `1002`.\n\n---\n\n## \ud83e\uddfe Table Structure\n\nWe start with the following table:\n\n```sql\nuser_login(\n  user_id INT,\n  login_date DATE\n)\n</code></pre>"},{"location":"SQL/consecutive_login_days/#sample-data-for-3-users","title":"\ud83d\udc65 Sample Data for 3 Users","text":"user_id login_date 1032 2019-01-01 1032 2019-01-02 1032 2019-01-03 1032 2019-01-04 1032 2019-01-05 1032 2019-01-06 1032 2019-01-07 1032 2019-01-08 1032 2019-01-09 1032 2019-01-10 1032 2019-01-11 1032 2019-01-13 1050 2020-02-10 1050 2020-02-11 1050 2020-02-12 1050 2020-02-13 1050 2020-02-14 1050 2020-02-15 1050 2020-02-16 1050 2020-02-17 1050 2020-02-18 1050 2020-02-19 1002 2021-05-01 1002 2021-05-03 1002 2021-05-04 1002 2021-05-06"},{"location":"SQL/consecutive_login_days/#step-1-add-previous-login-date","title":"Step 1\ufe0f\u20e3: Add Previous Login Date","text":"<pre><code>WITH my_prior_login_dates AS (\n  SELECT\n    user_id,\n    login_date,\n    LAG(login_date) OVER(PARTITION BY user_id ORDER BY login_date) AS prior_login_date\n  FROM user_login\n)\n</code></pre>"},{"location":"SQL/consecutive_login_days/#output-simplified","title":"\ud83d\udd0d Output (simplified)","text":"user_id login_date prior_login_date 1032 2019-01-01 (null) 1032 2019-01-02 2019-01-01 ... ... ... 1050 2020-02-10 (null) 1050 2020-02-11 2020-02-10 ... ... ... 1002 2021-05-01 (null) 1002 2021-05-03 2021-05-01 ... ... ..."},{"location":"SQL/consecutive_login_days/#step-2-mark-streak-breaks","title":"Step 2\ufe0f\u20e3: Mark Streak Breaks","text":"<pre><code>, my_marker AS (\n  SELECT\n    *,\n    CASE \n      WHEN DATE_PART('day', login_date - prior_login_date) = 1 \n           OR prior_login_date IS NULL THEN 0\n      ELSE 1\n    END AS streak_marker\n  FROM my_prior_login_dates\n)\n</code></pre>"},{"location":"SQL/consecutive_login_days/#output","title":"\ud83d\udd0d Output","text":"user_id login_date prior_login_date streak_marker 1032 2019-01-01 (null) 0 1032 2019-01-02 2019-01-01 0 1032 2019-01-03 2019-01-02 0 ... ... ... ... 1032 2019-01-13 2019-01-11 1 1050 2020-02-10 (null) 0 1050 2020-02-11 2020-02-10 0 ... ... ... ... 1002 2021-05-01 (null) 0 1002 2021-05-03 2021-05-01 1 1002 2021-05-04 2021-05-03 0 1002 2021-05-06 2021-05-04 1"},{"location":"SQL/consecutive_login_days/#step-3-assign-streak-id","title":"Step 3\ufe0f\u20e3: Assign Streak ID","text":"<pre><code>, my_streaks AS (\n  SELECT\n    user_id,\n    login_date,\n    SUM(streak_marker) OVER (PARTITION BY user_id ORDER BY login_date) AS streak_id\n  FROM my_marker\n)\n</code></pre>"},{"location":"SQL/consecutive_login_days/#output_1","title":"\ud83d\udd0d Output","text":"user_id login_date streak_id 1032 2019-01-01 0 1032 2019-01-02 0 1032 2019-01-03 0 ... ... ... 1032 2019-01-13 1 1050 2020-02-10 0 1050 2020-02-11 0 ... ... ... 1002 2021-05-01 0 1002 2021-05-03 1 1002 2021-05-04 1 1002 2021-05-06 2 <ul> <li>Each <code>streak_id</code> represents a separate run of consecutive login days.</li> </ul>"},{"location":"SQL/consecutive_login_days/#step-4-filter-10-day-streaks","title":"Step 4\ufe0f\u20e3: Filter 10+ Day Streaks","text":"<pre><code>SELECT \n  user_id, \n  MIN(login_date) AS first_login_date, \n  COUNT(*) AS consecutive_login_days\nFROM my_streaks\nGROUP BY user_id, streak_id\nHAVING COUNT(*) &gt;= 10;\n</code></pre>"},{"location":"SQL/consecutive_login_days/#final-output","title":"\u2705 Final Output","text":"user_id first_login_date consecutive_login_days 1032 2019-01-01 11 1050 2020-02-10 10"},{"location":"SQL/consecutive_login_days/#why-1002-is-not-in-result","title":"\u26a0\ufe0f Why 1002 is not in result:","text":"<p>User <code>1002</code> never had 10 consecutive login days. Their logins were non-consecutive or had short streaks:</p> <ul> <li>2021-05-01 \u2192 gap \u2192 2021-05-03 \u2192 2021-05-04 \u2192 gap \u2192 2021-05-06</li> </ul>"},{"location":"SQL/consecutive_login_days/#summary","title":"\u2705 Summary","text":"Concept Description <code>LAG()</code> Finds prior login for each user <code>streak_marker</code> 1 if gap occurred, 0 if login is consecutive <code>streak_id</code> Increases when a new streak starts <code>GROUP BY</code> Each <code>(user_id, streak_id)</code> group is one streak <code>HAVING COUNT(*) &gt;= 10</code> Filters only those streaks with 10 or more days <p>This method allows you to track multiple streaks per user, and analyze login behavior in a clean, scalable way.</p>"},{"location":"SQL/first_name/","title":"Concat name counts and names","text":""},{"location":"SQL/first_name/#sql-task","title":"SQL Task","text":"<p>Goal: Return a single string like <code>2, Alan, Adam</code> for all student names that start with 'A' or 'a', using the <code>students</code> table.</p> <p>Table: <code>students</code></p> Column Name Description first_name The first name of student"},{"location":"SQL/first_name/#postgresql-query","title":"\u2705 PostgreSQL Query","text":"<pre><code>SELECT \n    COUNT(*) || ', ' || STRING_AGG(first_name, ', ') AS result\nFROM students\nWHERE first_name ILIKE 'a%';\n</code></pre>"},{"location":"SQL/first_name/#explanation-for-mysql","title":"Explanation for MySQL","text":"<ul> <li><code>ILIKE 'a%'</code>: case-insensitive match for names starting with 'a' or 'A'</li> <li><code>STRING_AGG(...)</code>: concatenates the names with a comma and a space</li> <li><code>COUNT(*) || ', ' || ...</code>: formats the output as a single string</li> </ul>"},{"location":"SQL/first_name/#mysql-query-80","title":"\u2705 MySQL Query (8.0+)","text":"<pre><code>SELECT \n    CONCAT(COUNT(*), ', ', GROUP_CONCAT(first_name SEPARATOR ', ')) AS result\nFROM students\nWHERE LOWER(first_name) LIKE 'a%';\n</code></pre>"},{"location":"SQL/first_name/#explanation","title":"Explanation:","text":"<ul> <li><code>LOWER(first_name) LIKE 'a%'</code>: ensures case-insensitive matching</li> <li><code>GROUP_CONCAT(...)</code>: joins names with a comma and a space</li> <li><code>CONCAT(...)</code>: formats the final output</li> </ul>"},{"location":"SQL/first_name/#output-in-a-tablular-form","title":"Output in a tablular form","text":"<pre><code>SELECT \n    COUNT(*) AS name_count,\n    STRING_AGG(first_name, ', ') AS names\nFROM students\nWHERE first_name ILIKE 'a%';\n</code></pre> <p>If the table has different first name like:</p> first_name Alan adam Bob Annie <p>The output will be</p> name_count names 3 Alan, adam, Annie"},{"location":"SQL/groupby_monthly_customer/","title":"\ud83d\udcca SQL: GroupBy Monthly Customer Stats","text":""},{"location":"SQL/groupby_monthly_customer/#objective","title":"\ud83d\udd0d Objective","text":"<p>Get the total number of customers per month and average sales per customer per month.</p>"},{"location":"SQL/groupby_monthly_customer/#sample-table-structure","title":"\ud83e\uddf1 Sample Table Structure","text":"<p>Assume we have a table called <code>sales_data</code>:</p> Column Data Type Description sale_id INT Unique identifier for the sale customer_id INT Customer making the purchase sale_amount DECIMAL Total amount of the sale sale_date DATE Date when the sale occurred"},{"location":"SQL/groupby_monthly_customer/#sql-query","title":"\ud83d\udccc SQL Query","text":"<pre><code>SELECT\n  DATE_FORMAT(sale_date, '%Y-%m') AS sale_month,\n  COUNT(DISTINCT customer_id) AS total_customers,\n  SUM(sale_amount) / COUNT(DISTINCT customer_id) AS avg_sales_per_customer\nFROM\n  sales_data\nGROUP BY\n  DATE_FORMAT(sale_date, '%Y-%m')\nORDER BY\n  sale_month;\n</code></pre>"},{"location":"SQL/listening_habit/","title":"\ud83c\udfa7 SQL Challenge: Analyze Spotify-like Listening Habits","text":""},{"location":"SQL/listening_habit/#question","title":"\ud83d\udcd8 Question","text":"<p>You're assigned to analyze a Spotify-like dataset that records user listening habits.</p>"},{"location":"SQL/listening_habit/#task","title":"\ud83d\udcdd Task","text":"<p>For each user, compute:</p> <ul> <li>\u2705 The total listening time (in minutes), rounded to the nearest whole number</li> <li>\u2705 The number of unique songs they've listened to</li> </ul>"},{"location":"SQL/listening_habit/#table-listening_habits","title":"\ud83d\uddc3\ufe0f Table: <code>listening_habits</code>","text":"Column Name Description <code>user_id</code> Identifier of user <code>song_id</code> Identifier of song <code>listen_duration</code> Listening time (in seconds)"},{"location":"SQL/listening_habit/#expected-output","title":"\ud83d\udccc Expected Output","text":"user_id total_listen_duration unique_song_count 101 8 2 102 5 2"},{"location":"SQL/listening_habit/#sql-answer","title":"\u2705 SQL Answer","text":"<pre><code>SELECT \n  user_id, \n  ROUND(SUM(listen_duration) / 60.0) AS total_listen_duration, \n  COUNT(DISTINCT song_id) AS unique_song_count\nFROM listening_habits\nGROUP BY user_id;\n</code></pre>"},{"location":"SQL/most_popular_product/","title":"\ud83d\udce6 SQL Question: Most Popular Product on Instagram Shop","text":""},{"location":"SQL/most_popular_product/#requirements","title":"\ud83d\udcdd Requirements:","text":"<p>You are working for Instagram Shop, and your team wants to know what is the most popular product. Write a SQL query to find 1 product with the highest number of orders. In case of a tie in order counts, select the product that comes first in alphabetical order.</p>"},{"location":"SQL/most_popular_product/#tables","title":"\ud83d\udcca Tables","text":""},{"location":"SQL/most_popular_product/#orders-table","title":"<code>orders</code> table","text":"Column Name Description id Unique identifier for each order product_id Identifier for the product customer_id Identifier for the customer order_date Date the order was placed"},{"location":"SQL/most_popular_product/#products-table","title":"<code>products</code> table:","text":"Column Name Description id Unique identifier for each product name Name of the product price Price of the product category_id Identifier for the category"},{"location":"SQL/most_popular_product/#sql-answer","title":"\u2705 SQL Answer:","text":"<pre><code>WITH tbl_PRODUCT_COUNT AS (\n  SELECT \n    P.name AS product_name, \n    COUNT(O.product_id) AS order_counts\n  FROM orders O \n  JOIN products P ON O.product_id = P.id\n  GROUP BY P.name\n)\nSELECT product_name\nFROM tbl_PRODUCT_COUNT\nORDER BY order_counts DESC, product_name ASC\nLIMIT 1;\n</code></pre>"},{"location":"SQL/most_popular_product/#explanation","title":"\ud83d\udca1 Explanation:","text":"<ul> <li><code>JOIN</code> combines order and product info.</li> <li><code>COUNT(O.product_id)</code> counts how many times each product was ordered.</li> <li><code>GROUP BY P.name</code> groups orders by product.</li> <li> <p><code>ORDER BY order_counts DESC, product_name ASC</code> ensures:</p> </li> <li> <p>Most popular product comes first.</p> </li> <li>Alphabetical order is used to break ties.</li> <li><code>LIMIT 1</code> returns just the top product.</li> </ul>"},{"location":"SQL/total_duration/","title":"Calculate Total Active Hours for Each User","text":"<p>Calculate the total active hours for each user. You should use the start and end times of user sessions, defined by the session state: '1' for session start and '0' for session end.</p> <p>user_sessions table:</p> Column Name Description customer_id Customer's unique identifier state Session's state (1 for start, 0 for end) timestamp Timestamp of the session state customer_id state timestamp c001 1 07:00:00 c001 0 09:30:00 c001 1 12:00:00 <p>Example Output:</p> customer_id total_hours c005 19 <pre><code>WITH tbl_start_state AS \n(SELECT customer_id, state AS start_state, timestamp AS start_time,\nROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY timestamp) AS rn\nfrom user_sessions\nwhere state = 1\n),\n\ntbl_end_state AS (Select customer_id, state AS end_state, \ntimestamp AS end_time,\nROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY timestamp) AS rn\nfrom user_sessions\nwhere state = 0\n),\n\ntbl_combine AS (SELECT s.customer_id, \ns.start_state, e.end_state, s.start_time, e.end_time\nFROM tbl_start_state s JOIN tbl_end_state e \nON s.customer_id = e.customer_id AND s.rn=e.rn), \n\ntbl_duration AS(\nselect customer_id, start_state, end_state, \nstart_time, end_time, \nEXTRACT(EPOCH FROM (end_time-start_time))/3600.0 AS hr_duration from tbl_combine)\n\nSELECT customer_id, SUM(hr_duration) AS total_hours\nFROM tbl_duration\nGROUP BY customer_id\n</code></pre>"},{"location":"git_commands/handy_git_commands/","title":"\ud83e\udde0 Handy Git Command Reference (with Examples)","text":"<p>A simple guide to common Git commands with clear explanations.</p>"},{"location":"git_commands/handy_git_commands/#1-setup","title":"\ud83d\udd27 1. Setup","text":"<p>Before using Git, set your name and email (only once per machine):</p> <pre><code>git config --global user.name \"John Doe\"\ngit config --global user.email \"john@example.com\"\n</code></pre>"},{"location":"git_commands/handy_git_commands/#2-start-a-repository","title":"\ud83d\udcc1 2. Start a Repository","text":""},{"location":"git_commands/handy_git_commands/#create-a-new-git-repo-in-your-project-folder","title":"\u27a4 Create a new Git repo in your project folder","text":"<pre><code>git init\n</code></pre> <p>Think of this as telling Git: \u201cHey, I want to start tracking changes here!\u201d</p>"},{"location":"git_commands/handy_git_commands/#clone-an-existing-repo-download-code-from-github","title":"\u27a4 Clone an existing repo (download code from GitHub)","text":"<pre><code>git clone https://github.com/user/project.git\n</code></pre>"},{"location":"git_commands/handy_git_commands/#3-save-your-work-commit-flow","title":"\ud83d\udcbe 3. Save Your Work (Commit Flow)","text":""},{"location":"git_commands/handy_git_commands/#check-whats-changed","title":"\u27a4 Check what\u2019s changed","text":"<pre><code>git status\n</code></pre>"},{"location":"git_commands/handy_git_commands/#stage-a-file-for-saving","title":"\u27a4 Stage a file for saving","text":"<pre><code>git add index.html\n</code></pre> <p>\u201cI want to include this file in the next save.\u201d</p>"},{"location":"git_commands/handy_git_commands/#stage-everything","title":"\u27a4 Stage everything","text":"<pre><code>git add .\n</code></pre>"},{"location":"git_commands/handy_git_commands/#save-your-changes-with-a-message","title":"\u27a4 Save your changes with a message:","text":"<pre><code>git commit -m \"Add homepage layout\"\n</code></pre> <p>Like hitting \u201cSave\u201d in your editor, but with a label explaining what you changed.</p>"},{"location":"git_commands/handy_git_commands/#4-work-on-a-new-feature-branching","title":"\ud83c\udf3f 4. Work on a New Feature (Branching)","text":""},{"location":"git_commands/handy_git_commands/#see-all-branches","title":"\u27a4 See all branches:","text":"<pre><code>git branch\n</code></pre>"},{"location":"git_commands/handy_git_commands/#create-a-new-branch","title":"\u27a4 Create a new branch:","text":"<pre><code>git branch new-feature\n</code></pre>"},{"location":"git_commands/handy_git_commands/#switch-to-the-new-branch","title":"\u27a4 Switch to the new branch","text":"<pre><code>git checkout new-feature\n</code></pre> <p>Or both in one step:</p> <pre><code>git checkout -b new-feature\n</code></pre> <p>\"Let me work on something new without touching the main version.\"</p>"},{"location":"git_commands/handy_git_commands/#5-combine-changes-merge","title":"\ud83d\udd01 5. Combine Changes (Merge)","text":""},{"location":"git_commands/handy_git_commands/#go-back-to-main-branch","title":"\u27a4 Go back to main branch:","text":"<pre><code>git checkout main\n</code></pre>"},{"location":"git_commands/handy_git_commands/#merge-changes-from-your-feature-branch","title":"\u27a4 Merge changes from your feature branch","text":"<pre><code>git merge new-feature\n</code></pre>"},{"location":"git_commands/handy_git_commands/#6-connect-to-remote-eg-github","title":"\ud83c\udf0d 6. Connect to Remote (e.g., GitHub)","text":""},{"location":"git_commands/handy_git_commands/#add-github-as-remote","title":"\u27a4 Add GitHub as remote:","text":"<pre><code>git remote add origin https://github.com/user/project.git\n</code></pre>"},{"location":"git_commands/handy_git_commands/#push-your-code-online","title":"\u27a4 Push your code online:","text":"<pre><code>git push -u origin main\n</code></pre>"},{"location":"git_commands/handy_git_commands/#git-remote-add-vs-git-push","title":"git remote add vs. git push","text":"Command What it does When you use it <code>git remote add</code> Sets up the connection to a remote (like GitHub) One time per project <code>git push</code> Uploads your code to GitHub Every time you want to update the repo ---- <p>Upload your local work to GitHub.</p>"},{"location":"git_commands/handy_git_commands/#get-latest-changes-from-remote","title":"\u27a4 Get latest changes from remote","text":"<pre><code>git pull\n</code></pre>"},{"location":"git_commands/handy_git_commands/#7-view-history","title":"\ud83d\udcdc 7. View History","text":""},{"location":"git_commands/handy_git_commands/#see-list-of-commits","title":"\u27a4 See list of commits","text":"<pre><code>git log\n</code></pre>"},{"location":"git_commands/handy_git_commands/#simple-view","title":"\u27a4 Simple view","text":"<pre><code>git log --oneline\n</code></pre>"},{"location":"git_commands/handy_git_commands/#compare-changes-in-files","title":"\u27a4 Compare changes in files","text":"<pre><code>git diff\n</code></pre>"},{"location":"git_commands/handy_git_commands/#8-undo-mistakes","title":"\ud83e\uddfd 8. Undo Mistakes","text":""},{"location":"git_commands/handy_git_commands/#undo-changes-in-a-file","title":"\u27a4 Undo changes in a file","text":"<pre><code>git restore index.html\n</code></pre>"},{"location":"git_commands/handy_git_commands/#unstage-a-file","title":"\u27a4 Unstage a file","text":"<pre><code>git reset HEAD index.html\n</code></pre>"},{"location":"git_commands/handy_git_commands/#revert-a-commit-safe-undo","title":"\u27a4 Revert a commit (safe undo)","text":"<pre><code>git revert &lt;commit-id&gt;\n</code></pre>"},{"location":"git_commands/handy_git_commands/#9-git-stash-temporarily-save-your-change","title":"\ud83d\udcbc 9. <code>git stash</code>: Temporarily Save Your Change","text":"<p><code>git stash</code> is used to temporarily save your uncommitted changes (both staged and unstaged) without committing them. It clears your working directory so you can safely switch branches or perform other tasks, and later bring back your changes.</p>"},{"location":"git_commands/handy_git_commands/#why-use-git-stash","title":"Why Use <code>git stash</code>?","text":"<p>Imagine you're in the middle of editing files, and suddenly need to:</p> <ul> <li>Switch branches</li> <li>Pull the latest code</li> <li>Fix a critical bug</li> </ul> <p>But Git won't let you proceed because of your uncommitted changes. Use <code>git stash</code> to set aside your work, do the other task, and come back later.</p>"},{"location":"git_commands/handy_git_commands/#example-stash-workflow","title":"\ud83e\uddea Example <code>stash</code> Workflow","text":"<pre><code># You're working on changes\ngit stash           # Save and clear working directory\n\ngit switch other-branch   # Now Git allows switching\n\n# Do some work in another branch...\n\ngit switch original-branch\ngit stash pop       # Bring back your saved changes\n\n</code></pre>"},{"location":"git_commands/handy_git_commands/#see-saved-stashes","title":"\u27a4 See saved stashes","text":"<pre><code>git stash list\n</code></pre>"},{"location":"git_commands/handy_git_commands/#reapply-stash","title":"\u27a4 Reapply stash","text":"<pre><code>git stash pop\n</code></pre>"},{"location":"git_commands/handy_git_commands/#10-tag-a-version-release","title":"\ud83d\udd16 10. Tag a Version (Release)","text":""},{"location":"git_commands/handy_git_commands/#add-a-tag","title":"\u27a4 Add a tag","text":"<pre><code>git tag v1.0\n</code></pre>"},{"location":"git_commands/handy_git_commands/#push-tag-to-github","title":"\u27a4 Push tag to GitHub","text":"<pre><code>git push origin v1.0\n</code></pre>"},{"location":"git_commands/handy_git_commands/#bonus-tips","title":"\u2728 Bonus Tips","text":"<pre><code>git shortlog -sn          # List contributors\ngit clean -fd             # Delete untracked files\ngit show &lt;commit&gt;         # Details about a commit\n</code></pre> <p>\u2705 Remember: Git is like a time machine for your code. Use it often, commit small changes, and write clear messages!</p>"},{"location":"pandas_manipulation/map_apply_transform/","title":"Map apply transform","text":""},{"location":"pandas_manipulation/map_apply_transform/#pandas-reference-map-apply-transform-with-business-use-cases","title":"Pandas Reference: <code>map()</code>, <code>apply()</code>, <code>transform()</code> \u2014 with Business Use Cases","text":""},{"location":"pandas_manipulation/map_apply_transform/#sample-business-dataset","title":"\ud83d\uddc3 Sample Business Dataset","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'CustomerID': [101, 102, 103, 104, 105, 106, 107, 108],\n    'Region': ['East', 'West', 'East', 'South', 'West', 'East', 'South', 'West'],\n    'PurchaseAmount': [250, 300, 150, 400, 500, 100, 350, 450],\n    'LoyaltyLevel': ['Gold', 'Silver', 'Gold', 'Platinum', 'Silver', 'Bronze', 'Gold', 'Silver']\n})\n</code></pre> CustomerID Region PurchaseAmount LoyaltyLevel 101 East 250 Gold 102 West 300 Silver 103 East 150 Gold 104 South 400 Platinum 105 West 500 Silver 106 East 100 Bronze 107 South 350 Gold 108 West 450 Silver"},{"location":"pandas_manipulation/map_apply_transform/#1-map-use-for-element-wise-transformation-on-a-series","title":"1\ufe0f\u20e3 <code>map()</code> - Use for Element-wise Transformation on a Series","text":""},{"location":"pandas_manipulation/map_apply_transform/#goal-map-loyalty-level-to-discount-rate","title":"\ud83d\udccc Goal: Map loyalty level to discount rate","text":"<pre><code>discount_map = {'Gold': 0.10, 'Silver': 0.05, 'Platinum': 0.15, 'Bronze': 0.02}\ndf['DiscountRate'] = df['LoyaltyLevel'].map(discount_map)\n</code></pre>"},{"location":"pandas_manipulation/map_apply_transform/#another-example-format-customerid-with-prefix","title":"\ud83d\udccc Another Example: Format CustomerID with prefix","text":"<pre><code>df['CustomerTag'] = df['CustomerID'].map(lambda x: f'CUST-{x}')\n</code></pre> <p>\u2705 Use <code>map()</code> when:</p> <ul> <li>We are transforming values element-by-element</li> <li>We are working with a single column (Series)</li> <li>We can use a function or dictionary</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#2-apply-use-for-rowcolumn-wise-custom-logic","title":"2\ufe0f\u20e3 <code>apply()</code> - Use for Row/Column-wise Custom Logic","text":""},{"location":"pandas_manipulation/map_apply_transform/#goal-compute-final-price-after-discount-row-wise","title":"\ud83d\udccc Goal: Compute final price after discount (row-wise)","text":"<pre><code>df['FinalAmount'] = df.apply(\n    lambda row: row['PurchaseAmount'] * (1 - row['DiscountRate']),\n    axis=1\n)\n</code></pre>"},{"location":"pandas_manipulation/map_apply_transform/#another-example-tag-high-value-customers","title":"\ud83d\udccc Another Example: Tag high-value customers","text":"<pre><code>df['HighValueTag'] = df.apply(\n    lambda row: 'VIP' if row['FinalAmount'] &gt; 300 else 'Regular',\n    axis=1\n)\n</code></pre> <p>\u2705 Use <code>apply()</code> when:</p> <ul> <li>We want access to multiple columns at once (row-wise logic)</li> <li>We need to return more complex values</li> <li>Be aware: <code>apply()</code> can change the shape</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#3-transform-use-for-group-wise-computation-shape-preservation","title":"3\ufe0f\u20e3 <code>transform()</code> \u2014 Use for Group-wise Computation &amp; Shape Preservation","text":""},{"location":"pandas_manipulation/map_apply_transform/#goal-normalize-purchase-amount-within-each-region","title":"\ud83d\udccc Goal: Normalize purchase amount within each region","text":"<pre><code>df['RegionMean'] = df.groupby('Region')['PurchaseAmount'].transform('mean')\ndf['NormalizedPurchase'] = df['PurchaseAmount'] / df['RegionMean']\n</code></pre>"},{"location":"pandas_manipulation/map_apply_transform/#another-example-z-score-of-purchases-within-each-region","title":"\ud83d\udccc Another Example: Z-score of purchases within each region","text":"<pre><code>df['Zscore'] = df.groupby('Region')['PurchaseAmount'].transform(\n    lambda x: (x - x.mean()) / x.std()\n)\n</code></pre> <p>\u2705 Use <code>transform()</code> when:</p> <ul> <li>You\u2019re doing group-wise operations</li> <li>You want to broadcast result back to each row</li> <li>You must preserve original shape</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#comparison-table","title":"\u2696\ufe0f Comparison Table","text":"Feature <code>map()</code> <code>apply()</code> <code>transform()</code> Works on Series only Series / DataFrame Series / DataFrame Acts on Individual values Rows or columns Element-wise with shape Can access multiple columns? \u274c No \u2705 Yes \u274c No Keeps original shape \u2705 Yes \u274c Not always \u2705 Yes Great for Clean-up, mapping Custom row logic Group-based engineering"},{"location":"pandas_manipulation/map_apply_transform/#bonus-chain-all-together","title":"\ud83e\uddea Bonus: Chain all together","text":"<pre><code># Map loyalty to discount\ndf['DiscountRate'] = df['LoyaltyLevel'].map(discount_map)\n\n# Format customer ID\ndf['CustomerTag'] = df['CustomerID'].map(lambda x: f'CUST-{x}')\n\n# Final amount after discount\ndf['FinalAmount'] = df.apply(\n    lambda row: row['PurchaseAmount'] * (1 - row['DiscountRate']),\n    axis=1\n)\n\n# Region-wise normalization\ndf['RegionMean'] = df.groupby('Region')['PurchaseAmount'].transform('mean')\ndf['NormalizedPurchase'] = df['PurchaseAmount'] / df['RegionMean']\n\n# Z-score\ndf['Zscore'] = df.groupby('Region')['PurchaseAmount'].transform(\n    lambda x: (x - x.mean()) / x.std()\n)\n\n# High-value tag\ndf['HighValueTag'] = df.apply(\n    lambda row: 'VIP' if row['FinalAmount'] &gt; 300 else 'Regular',\n    axis=1\n)\n</code></pre>"},{"location":"pandas_manipulation/map_apply_transform/#add-calculated-columns-based-on-the-combined-logic-from-the-markdown-example","title":"Add calculated columns based on the combined logic from the markdown example","text":""},{"location":"pandas_manipulation/map_apply_transform/#discount-mapping","title":"Discount mapping","text":"<p>discount_map = {'Gold': 0.10, 'Silver': 0.05, 'Platinum': 0.15, 'Bronze': 0.02} df_extended['DiscountRate'] = df_extended['LoyaltyLevel'].map(discount_map)</p>"},{"location":"pandas_manipulation/map_apply_transform/#customer-tag","title":"Customer tag","text":"<p>df_extended['CustomerTag'] = df_extended['CustomerID'].map(lambda x: f'CUST-{x}')</p>"},{"location":"pandas_manipulation/map_apply_transform/#final-amount-after-discount","title":"Final amount after discount","text":"<p>df_extended['FinalAmount'] = df_extended.apply(     lambda row: row['PurchaseAmount'] * (1 - row['DiscountRate']),     axis=1 )</p>"},{"location":"pandas_manipulation/map_apply_transform/#region-wise-mean-and-normalization","title":"Region-wise mean and normalization","text":"<p>df_extended['RegionMean'] = df_extended.groupby('Region')['PurchaseAmount'].transform('mean') df_extended['NormalizedPurchase'] = df_extended['PurchaseAmount'] / df_extended['RegionMean']</p>"},{"location":"pandas_manipulation/map_apply_transform/#region-wise-z-score","title":"Region-wise Z-score","text":"<p>df_extended['Zscore'] = df_extended.groupby('Region')['PurchaseAmount'].transform(     lambda x: (x - x.mean()) / x.std() )</p>"},{"location":"pandas_manipulation/map_apply_transform/#high-value-customer-tagging","title":"High-value customer tagging","text":"<p>df_extended['HighValueTag'] = df_extended.apply(     lambda row: 'VIP' if row['FinalAmount'] &gt; 300 else 'Regular',     axis=1 )</p> CustomerID Region PurchaseAmount LoyaltyLevel DiscountRate CustomerTag FinalAmount RegionMean NormalizedPurchase Zscore HighValueTag 101 East 250 Gold 0.10 CUST-101 225.00 166.67 1.50 1.0911 Regular 102 West 300 Silver 0.05 CUST-102 285.00 416.67 0.72 -1.1209 Regular 103 East 150 Gold 0.10 CUST-103 135.00 166.67 0.90 -0.2182 Regular 104 South 400 Platinum 0.15 CUST-104 340.00 375.00 1.07 0.7071 VIP 105 West 500 Silver 0.05 CUST-105 475.00 416.67 1.20 0.8006 VIP 106 East 100 Bronze 0.02 CUST-106 98.00 166.67 0.60 -1.3093 Regular 107 South 350 Gold 0.10 CUST-107 315.00 375.00 0.93 -0.7071 VIP 108 West 450 Silver 0.05 CUST-108 427.50 416.67 1.02 0.3203 VIP <p>Absolutely \u2014 let\u2019s talk about why <code>NormalizedPurchase</code> is useful in real-world data work:</p>"},{"location":"pandas_manipulation/map_apply_transform/#what-is-normalizedpurchase","title":"\ud83c\udfaf What is <code>NormalizedPurchase</code>?","text":"<p>This column was created by:</p> <pre><code>df['NormalizedPurchase'] = df['PurchaseAmount'] / df['RegionMean']\n</code></pre> <p>It shows how a customer\u2019s purchase compares to the average purchase amount in their region.</p>"},{"location":"pandas_manipulation/map_apply_transform/#why-is-this-useful","title":"\ud83d\udca1 Why is this useful?","text":""},{"location":"pandas_manipulation/map_apply_transform/#1-contextual-comparison","title":"\u2705 1. Contextual Comparison","text":"<p>A customer who spent \\$400:</p> <ul> <li>May be above average in one region</li> <li>But below average in another</li> </ul> <p><code>NormalizedPurchase</code> allows you to compare spending behavior relative to regional norms, instead of using raw numbers.</p>"},{"location":"pandas_manipulation/map_apply_transform/#2-fair-scoring-in-regional-campaigns","title":"\u2705 2. Fair Scoring in Regional Campaigns","text":"<p>If you\u2019re running a reward campaign:</p> <ul> <li>Simply picking \"top spenders\" may favor high-spending regions (like West)</li> <li>Using <code>NormalizedPurchase</code> helps pick outstanding customers in every region fairly</li> </ul> <p>Example: A customer with <code>NormalizedPurchase = 1.5</code> is spending 50% more than the average customer in their region.</p>"},{"location":"pandas_manipulation/map_apply_transform/#3-outlier-detection","title":"\u2705 3. Outlier Detection","text":"<p>Customers with:</p> <ul> <li>Values \u226b 1 are potential high-value or flagship customers</li> <li>Values \u226a 1 might be inactive, or at-risk customers</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#4-feature-engineering-for-ml","title":"\u2705 4. Feature Engineering for ML","text":"<p>If you build models (e.g., churn prediction, segmentation), using <code>NormalizedPurchase</code>:</p> <ul> <li>Removes region bias</li> <li>Helps the model learn behavior patterns, not raw differences</li> </ul>"},{"location":"pandas_manipulation/map_apply_transform/#tldr","title":"\ud83e\udde0 TL;DR","text":"Raw Purchase Normalized Purchase Meaning \\$400 1.5 50% above region average \\$300 0.75 25% below region average \\$500 1.2 20% above region average <p>\u2705 Use <code>NormalizedPurchase</code> to level the playing field, understand customer behavior, and make region-aware decisions.</p>"},{"location":"pandas_manipulation/notes_on_melt/","title":"\ud83d\udcc9 Pandas <code>melt()</code> Explained (Wide to Long Format)","text":"<p>The <code>pandas.melt()</code> function is used to reshape a dataframe from wide format to long format. This is useful when you want to gather multiple columns into key-value pairs for easier analysis or visualization.</p>"},{"location":"pandas_manipulation/notes_on_melt/#example-use-case","title":"\ud83e\uddea Example Use Case","text":"<p>Suppose you have a dataframe named <code>df_main</code> with the shape <code>(2, 4)</code> and columns like:</p> <ul> <li>Before melting:</li> </ul> SAM Year Species31 Species61 A 2021 10 5 B 2021 0 3 <pre><code>df_long = pd.melt(\n    df_wide,\n    id_vars=['SAM', 'Year'], # cols to keep fixed\n    value_vars=['Species31', 'Species61'], # cols names from old df_wide we like to melt\n    var_name='Species', # name of new colum \n    value_name='Catches' # name of new column that stores the value from those melted\n)\n</code></pre> <ul> <li>After melting:</li> </ul> SAM Year Species Catch A 2021 Species31 10 B 2021 Species31 0 A 2021 Species61 5 B 2021 Species61 3"},{"location":"pandas_manipulation/notes_on_melt/#parameter-breakdown-of-pdmeltdf_to_be_melted-id_vars-value_vars-var_name-value_name","title":"parameter breakdown of <code>pd.melt(df_to_be_melted, id_vars, value_vars, var_name, value_name)</code>","text":"Parameter Description <code>id_vars</code> Columns to keep fixed (e.g., metadata like <code>SAM</code>, <code>Year</code>, etc.) <code>value_vars</code> The column names you want to melt (e.g., <code>Species31</code>, <code>Species61</code>, ...) <code>var_name</code> Name for the new column that stores the melted column names (e.g., <code>'Species'</code>) <code>value_name</code> Name for the new column that stores the values from those melted columns (e.g., <code>'Catch'</code>)"},{"location":"pandas_manipulation/notes_on_melt/#summary","title":"Summary","text":"<p>Use <code>melt()</code> to convert wide data (many similar columns) into long format (fewer columns, more rows).</p> <ul> <li> <p>Great for tidy data, easier filtering, plotting, or exporting.</p> </li> <li> <p>Keep identifier columns with id_vars.</p> </li> <li> <p>Collapse similar columns with value_vars.</p> </li> <li> <p>Control output names with <code>var_name</code> and <code>value_name</code>.</p> </li> </ul>"},{"location":"pandas_manipulation/pandas_pivot_reshape/","title":"\ud83d\udc3c Pandas Data Transformation Cheat Sheet","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#example-dataset","title":"\ud83d\udcd8 Example Dataset","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = {\n    'Date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02'],\n    'City': ['Toronto', 'Vancouver', 'Toronto', 'Vancouver'],\n    'Temperature': [5, 7, 6, 8],\n    'Humidity': [65, 70, 66, 72]\n}\ndf = pd.DataFrame(data)\n</code></pre> <pre><code>         Date      City  Temperature  Humidity\n0  2024-01-01   Toronto            5        65\n1  2024-01-01  Vancouver           7        70\n2  2024-01-02   Toronto            6        66\n3  2024-01-02  Vancouver           8        72\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#1-pivot_table","title":"1. <code>pivot_table()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-aggregating-reshaping-data-by-indexcolumn","title":"\u2705 When to use: Aggregating + reshaping data by index/column","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-what-is-the-average-temperature-and-humidity-for-each-city","title":"\u2753 Question it solves: \"What is the average Temperature and Humidity for each City?\"","text":"<pre><code>pivot_df = df.pivot_table(index='City', values=['Temperature', 'Humidity'], aggfunc='mean')\n</code></pre> <pre><code>            Humidity  Temperature\nCity                            \nToronto          65.5         5.5\nVancouver        71.0         7.5\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#2-pivot","title":"2. <code>pivot()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-reshaping-without-aggregation-must-be-unique-combinations","title":"\u2705 When to use: Reshaping without aggregation (must be unique combinations)","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-how-can-i-see-temperature-values-by-date-and-city-in-a-matrix","title":"\u2753 Question it solves: \"How can I see Temperature values by Date and City in a matrix?\"","text":"<pre><code>pivot_df = df.pivot(index='Date', columns='City', values='Temperature')\n</code></pre> <pre><code>City         Toronto  Vancouver\nDate                          \n2024-01-01        5          7\n2024-01-02        6          8\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#3-groupby","title":"3. <code>groupby()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-grouping-aggregation-without-reshaping","title":"\u2705 When to use: Grouping + aggregation without reshaping","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-what-is-the-average-temperature-for-each-city","title":"\u2753 Question it solves: \"What is the average Temperature for each City?\"","text":"<pre><code>city_group = df.groupby('City')['Temperature'].mean()\n</code></pre> <pre><code>City\nToronto      5.5\nVancouver    7.5\nName: Temperature, dtype: float64\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#4-melt","title":"4. <code>melt()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-unpivot-a-wide-table-into-long-format","title":"\u2705 When to use: Unpivot a wide table into long format","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-how-can-i-convert-columns-like-temperature-humidity-into-rows-for-easier-plotting-or-analysis","title":"\u2753 Question it solves: \"How can I convert columns (like Temperature, Humidity) into rows for easier plotting or analysis?\"","text":"<pre><code>melted = pd.melt(df, id_vars=['Date', 'City'], value_vars=['Temperature', 'Humidity'], \n                 var_name='Measurement', value_name='Value')\n</code></pre> <pre><code>         Date      City Measurement  Value\n0  2024-01-01   Toronto  Temperature      5\n1  2024-01-01  Vancouver  Temperature      7\n2  2024-01-02   Toronto  Temperature      6\n3  2024-01-02  Vancouver  Temperature      8\n4  2024-01-01   Toronto     Humidity     65\n5  2024-01-01  Vancouver     Humidity     70\n6  2024-01-02   Toronto     Humidity     66\n7  2024-01-02  Vancouver     Humidity     72\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-melt-for-visualization","title":"\ud83d\udcca When to use <code>melt()</code> for visualization","text":"<p><code>melt()</code> is especially useful for Seaborn or Matplotlib when you want to: - Plot multiple measurements on the same axes - Use <code>hue</code> for different measurements</p> <pre><code>sns.lineplot(data=melted, x='Date', y='Value', hue='Measurement')\nplt.title('Line Plot of Temperature and Humidity')\nplt.show()\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#5-stack","title":"5. <code>stack()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-convert-columns-to-a-row-wise-multiindex-wide-long","title":"\u2705 When to use: Convert columns to a row-wise MultiIndex (wide \u2192 long)","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-how-can-i-convert-the-columns-of-a-pivoted-table-into-row-level-entries","title":"\u2753 Question it solves: \"How can I convert the columns of a pivoted table into row-level entries?\"","text":"<pre><code>pivot_df = df.pivot(index='Date', columns='City', values='Temperature')\nstacked = pivot_df.stack()  # MultiIndex: Date + City\n</code></pre> <pre><code>Date        City     \n2024-01-01  Toronto      5\n            Vancouver    7\n2024-01-02  Toronto      6\n            Vancouver    8\ndtype: int64\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#6-unstack","title":"6. <code>unstack()</code>","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#when-to-use-convert-inner-level-of-row-index-to-column-level-long-wide","title":"\u2705 When to use: Convert inner level of row index to column level (long \u2192 wide)","text":""},{"location":"pandas_manipulation/pandas_pivot_reshape/#question-it-solves-how-can-i-reverse-the-effect-of-stack-and-get-back-the-original-column-layout","title":"\u2753 Question it solves: \"How can I reverse the effect of <code>stack()</code> and get back the original column layout?\"","text":"<pre><code>unstacked = stacked.unstack()  # City becomes columns again\n</code></pre> <pre><code>City         Toronto  Vancouver\nDate                          \n2024-01-01        5          7\n2024-01-02        6          8\n</code></pre>"},{"location":"pandas_manipulation/pandas_pivot_reshape/#summary-table","title":"\ud83d\udd01 Summary Table","text":"Function Use Case Reshape Aggregation Solves What Question? Suitable for Plotting <code>pivot()</code> Reshape data (must be unique keys) \u2705 \u274c See data matrix across dimensions Wide format <code>pivot_table()</code> Reshape + aggregate \u2705 \u2705 Average/summarize across categories Wide format <code>groupby()</code> Group + aggregate \u274c \u2705 Summary stats per group Use with aggregation <code>melt()</code> Wide \u2192 long \u2705 \u274c Flatten data for analysis/plotting \u2705 (long format) <code>stack()</code> Columns \u2192 row MultiIndex \u2705 \u274c Convert wide to nested long format Not commonly used <code>unstack()</code> Row MultiIndex \u2192 columns \u2705 \u274c Reverse of stacking Not commonly used"},{"location":"pandas_manipulation/top_3genre_avg_rating/","title":"Top 3 Actors by Genre Average Rating","text":"<p>Identify the top 3 actors with the highest average movie ratings within their most frequent (i.e., top) genre. For each actor, return their name, top genre, and average rating in that genre.</p> <p>Conditions: * If an actor has multiple top genres (same number of appearances), choose the one with the highest average rating. * If multiple actors share the same average rating (i.e., a tie in rank), include all tied actors, even if this results in more than 3 total actors. Do not skip rank: if all tied actors share the same rank, the next rank follows sequentially.</p> <p>DataFrame: top_actors_rating actor_name: object genre: object movie_rating: float64 movie_title: object release_date: datetime64[ns] production_company: object</p> actor_name genre movie_rating movie_title release_date production_company Ryan Gosling drama 9 Urban Hunt 2017-07-03 Google Ryan Gosling sci-fi 8.9 Veil of Secrets 2015-12-23 Apple Chris Evans drama 6.1 Crimson Chase 2017-08-10 Apple"},{"location":"pandas_manipulation/top_3genre_avg_rating/#sample-data-top_actors_rating","title":"\ud83c\udfac Sample Data: <code>top_actors_rating</code>","text":"actor_name genre movie_rating movie_title release_date production_company Tom Hanks Drama 8.5 Movie A 2000-01-01 Studio 1 Tom Hanks Comedy 7.0 Movie B 2002-01-01 Studio 1 Emma Stone Comedy 9.0 Movie C 2010-01-01 Studio 2 Emma Stone Romance 8.8 Movie D 2011-01-01 Studio 2 Denzel Washington Thriller 8.8 Movie E 2005-01-01 Studio 3 <pre><code>import pandas as pd\n\n# group by actor and genre. because if an actor has multiple genre, we take one with highest average rating\n\ngroupby_actor_genre = top_actors_ratings.groupby(['actor_name', 'genre']).agg(count_genre=('genre':'count'), avg_rating=('movie_rating':'mean')).reset_index()\n</code></pre>"},{"location":"pandas_manipulation/types_of_aggregation/","title":"\ud83e\uddee pandas Aggregation &amp; Transformation Functions Cheat Sheet","text":"<p>This note covers different pandas aggregation functions (<code>agg()</code>, <code>apply()</code>, <code>transform()</code>) with examples, outputs, and explanations for your MkDocs.</p>"},{"location":"pandas_manipulation/types_of_aggregation/#sample-dataframe","title":"\ud83d\udce5 Sample DataFrame","text":"group value1 value2 value3 A 10 100 5 A 15 150 3 B 10 50 6 B 20 60 2 C 15 200 7 C 30 250 8"},{"location":"pandas_manipulation/types_of_aggregation/#1-agg-aggregate-with-built-in-and-custom-functions","title":"1\ufe0f\u20e3 <code>agg()</code> - Aggregate with built-in and custom functions","text":"<pre><code>df.agg({\n    'value1': ['min', 'max', 'mean'],\n    'value2': ['mean', 'std'],\n    'value3': lambda x: (x &gt; 4).sum()\n})\n</code></pre> value1 value2 value3 min 10 max 30 mean 16.67 mean 105.83 4 <p>Explanation:</p> <ul> <li>Multiple built-in functions per column.</li> <li>Custom lambda counts values &gt; 4 in <code>value3</code>.</li> </ul>"},{"location":"pandas_manipulation/types_of_aggregation/#2-groupbyagg-group-wise-aggregation","title":"2\ufe0f\u20e3 <code>groupby().agg()</code> - Group-wise aggregation","text":"<pre><code>df.groupby('group').agg({\n    'value1': ['mean', 'sum'],\n    'value3': lambda x: (x &gt; 5).sum()\n})\n</code></pre> group value1 mean value1 sum value3 (count &gt;5) A 12.5 25 0 B 15.0 30 1 C 22.5 45 2 <p>Explanation: Aggregation by group with mixed functions.</p>"},{"location":"pandas_manipulation/types_of_aggregation/#3-apply-apply-any-function-across-dataframe-or-group","title":"3\ufe0f\u20e3 <code>apply()</code> - Apply any function across DataFrame or group","text":"<pre><code>def range_func(x):\n    return x.max() - x.min()\n\n# Apply to entire DataFrame numeric columns\ndf[['value1', 'value2']].apply(range_func)\n</code></pre> value1 value2 20 200 <pre><code># Apply custom function on groupby object (sum of range per group)\ndf.groupby('group').apply(lambda g: g[['value1', 'value2']].apply(range_func))\n</code></pre> group value1 value2 A 5 50 B 10 10 C 15 50 <p>Explanation:</p> <ul> <li><code>apply()</code> can run any function, returning scalar or DataFrame.</li> <li>More flexible but sometimes slower than <code>agg()</code>.</li> </ul>"},{"location":"pandas_manipulation/types_of_aggregation/#4-transform-returns-same-shape-as-input-good-for-feature-engineering","title":"4\ufe0f\u20e3 <code>transform()</code> - Returns same shape as input, good for feature engineering","text":"<pre><code># Normalize value1 within each group (subtract mean)\ndf['value1_norm'] = df.groupby('group')['value1'].transform(lambda x: x - x.mean())\ndf\n</code></pre> group value1 value2 value3 value1_norm A 10 100 5 -2.5 A 15 150 3 2.5 B 10 50 6 -5.0 B 20 60 2 5.0 C 15 200 7 -7.5 C 30 250 8 7.5 <p>Explanation:</p> <ul> <li><code>transform()</code> outputs a result with the same index and shape as original.</li> <li>Useful for adding normalized or scaled features.</li> </ul>"},{"location":"pandas_manipulation/types_of_aggregation/#5-difference-between-agg-apply-and-transform","title":"5\ufe0f\u20e3 Difference between <code>agg()</code>, <code>apply()</code>, and <code>transform()</code>","text":"Function Input Type Output Shape Use Case <code>agg()</code> Series or DataFrame Aggregated scalar(s) or DataFrame Summary statistics or aggregation <code>apply()</code> Series or DataFrame Can be scalar, Series, or DataFrame Flexible function application, complex ops <code>transform()</code> Series or DataFrame Same shape as input Element-wise transformations preserving index"},{"location":"pandas_manipulation/types_of_aggregation/#summary","title":"Summary","text":"<ul> <li>Use <code>agg()</code> for aggregations that reduce data size (e.g., sum, mean).</li> <li>Use <code>apply()</code> for custom, flexible functions that may change shape.</li> <li>Use <code>transform()</code> when you want to return transformed data matching input shape (e.g., normalization).</li> </ul>"},{"location":"pandas_manipulation/types_of_aggregation/#renaming-columns-after-aggregation","title":"Renaming columns after aggregation","text":"<pre><code>result = df.agg({\n    'value1': 'mean',\n    'value2': 'sum'\n})\n\nresult.rename({\n    'value1': 'Avg Value1',\n    'value2': 'Total Value2'\n}, inplace=True)\n\nprint(result)\n</code></pre> Avg Value1 Total Value2 16.67 810 <p>Happy coding with pandas! \ud83d\ude80</p>"},{"location":"python/coin_combinations/","title":"Count the Number of Ways to Make a Target Amount Using Coins","text":"<p>Suppose you want to find how many ways you can make a target amount using a given list of coin denominations.</p>"},{"location":"python/coin_combinations/#example","title":"Example:","text":"<pre><code>target_amount = 4\ncoins = [2, 3]\n</code></pre>"},{"location":"python/coin_combinations/#initialization","title":"Initialization","text":"<p>We initialize a list called <code>ways</code> to store the number of ways to make each amount from <code>0</code> to <code>target_amount</code>.</p> <ul> <li><code>ways[0] = 1</code> \u2192 There's exactly one way to make amount <code>0</code>: using no coins.</li> <li>The rest are initialized to <code>0</code>.</li> </ul> <p>So initially: ways = [1, 0, 0, 0, 0]</p>"},{"location":"python/coin_combinations/#step-by-step-iteration-coin-2","title":"Step-by-Step Iteration (coin = 2)","text":"<p>Loop from <code>coin</code> to <code>target_amount</code>, updating <code>ways[amount]</code> using:</p> <pre><code>ways[amount] = ways[amount] + ways[amount - coin]\n</code></pre> Amount Calculation Updated <code>ways</code> 2 ways[2] = ways[2] + ways[0] = 0 + 1 = 1 [1, 0, 1, 0, 0] 3 ways[3] = ways[3] + ways[1] = 0 + 0 = 0 [1, 0, 1, 0, 0] 4 ways[4] = ways[4] + ways[2] = 0 + 1 = 1 [1, 0, 1, 0, 1]"},{"location":"python/coin_combinations/#python-code","title":"Python Code","text":"<pre><code>def coin_combinations(target_amount, coins):\n    # Initialize array to store number of ways for each amount\n    ways = [0] * (target_amount + 1)\n\n    # Base case: one way to make amount 0\n    ways[0] = 1\n\n    # Loop through each coin\n    for coin in coins:\n        for amount in range(coin, target_amount + 1):\n            ways[amount] += ways[amount - coin]\n\n    return ways[target_amount]\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(coin_combinations(4, [1, 2, 3]))  # Output: 4\n    # Ways: [1,1,1,1], [1,1,2], [2,2], [1,3]\n\n    print(coin_combinations(5, [1, 2, 5]))  # Output: 4\n    # Ways: [1,1,1,1,1], [1,1,1,2], [1,2,2], [5]\n</code></pre>"},{"location":"python/find_unique_element/","title":"Python: Two-Pointer Technique to Find Unique Elements","text":""},{"location":"python/find_unique_element/#objective","title":"Objective","text":"<p>Use two-pointer movement to identify unique elements from a sorted list of integers.</p>"},{"location":"python/find_unique_element/#problem-statement","title":"Problem Statement","text":"<p>Given a sorted list of integers, return a list of unique elements (i.e., remove duplicates) using the two-pointer technique.</p>"},{"location":"python/find_unique_element/#example","title":"Example","text":"<ul> <li> <p>Input nums = [1, 1, 2, 2, 3, 4, 4, 5]</p> </li> <li> <p>Output nums = [1, 2, 3, 4, 5]</p> </li> </ul>"},{"location":"python/find_unique_element/#code-snippet","title":"Code Snippet","text":"<pre><code>\ndef remove_duplicates(nums):\n    if not nums:\n        return []\n\n    i = 0  # slow pointer, keep track of the last unique number's index\n\n    for j in range(1, len(nums)):  # fast pointer, iterate through the list\n\n        if nums[j] != nums[i]:\n            i += 1\n            nums[i] = nums[j]\n\n    return nums[:i+1]\n</code></pre>"},{"location":"python/find_unique_element/#execution-steps","title":"Execution Steps","text":"Step i j num[i] num[j] num[j] != num[i] Action nums 0 0 1 1 1 False [1,1,2,2,3,4,4,5] 1 0 2 1 2 True i += 1 [1,2,2,2,3,4,4,5] 2 1 3 2 2 True i += 1 [1,2,3,2,3,4,4,5] 3 2 4 3 3 True i += 1 [1,2,3,4,3,4,4,5] 4 3 5 4 4 True i += 1 [1,2,3,4,5,4,4,5] 5 4 6 5 5 True i += 1 [1,2,3,4,5,5,4,5]"},{"location":"python/find_unique_element/#how-it-works","title":"How it works","text":"<ol> <li>Initialization: Start with two pointers, <code>i</code> (slow) and <code>j</code> (fast). <code>i</code> keeps track of the last unique element's index, while <code>j</code> iterates through the list.</li> <li>Comparison: For each element at index <code>j</code>, check if it is different from the element at index <code>i</code>. If it is, increment <code>i</code> and update <code>nums[i]</code> with <code>nums[j]</code>.  </li> <li>Result: The unique elements are now at the beginning of the list, and the length of the unique elements is <code>i + 1</code>.</li> <li>Return: Return the list of unique elements by slicing <code>nums</code> up to <code>i + 1</code>.</li> </ol>"},{"location":"python/square_root/","title":"Find the Integer Square Root Without Using <code>sqrt()</code>","text":""},{"location":"python/square_root/#the-problem","title":"\u2705 The Problem","text":"<p>Write a function in Python to find the square root of a non-negative integer, rounded down to the nearest whole number.</p> <ul> <li>\u274c You cannot use the built-in <code>math.sqrt()</code> function.</li> <li>\u2705 The result should be the largest whole number whose square is less than or equal to the input.</li> </ul>"},{"location":"python/square_root/#example","title":"Example","text":"Input Expected Output Why? 9 3 3 \u00d7 3 = 9 15 3 3 \u00d7 3 = 9 &lt; 15, 4\u00d74=16 is too big 16 4 4 \u00d7 4 = 16 24 4 4 \u00d7 4 = 16 &lt; 24, 5\u00d75=25 is too big"},{"location":"python/square_root/#the-idea-use-binary-search","title":"\ud83d\udee0\ufe0f The Idea: Use Binary Search","text":""},{"location":"python/square_root/#why-binary-search","title":"Why Binary Search?","text":"<p>We're looking for a number between 1 and <code>n</code> that, when squared, is less than or equal to <code>n</code>. This makes it a perfect job for binary search, which quickly narrows down a range.</p>"},{"location":"python/square_root/#step-by-step-logic","title":"\ud83d\udd04 Step-by-Step Logic","text":""},{"location":"python/square_root/#initialize","title":"Initialize","text":"<ul> <li><code>left = 1</code></li> <li><code>right = n // 2</code> (because no square root of <code>n</code> is larger than <code>n/2</code>, unless <code>n</code> is 0 or 1)</li> </ul>"},{"location":"python/square_root/#loop-while-left-right","title":"Loop While <code>left &lt;= right</code>","text":"<ol> <li><code>mid = (left + right) // 2</code></li> <li>Check <code>mid * mid</code>:</li> <li>\u2705 If it's equal to <code>n</code>, return <code>mid</code> (exact square root!)</li> <li>\u2795 If it's less than <code>n</code>, it might be the answer - try bigger numbers: <code>left = mid + 1</code></li> <li>\u2796 If it's more than <code>n</code>, it's too big - try smaller numbers: <code>right = mid - 1</code></li> </ol>"},{"location":"python/square_root/#when-the-loop-ends","title":"When the loop ends","text":"<ul> <li>Return <code>right</code>. This is the biggest number such that <code>right * right &lt;= n</code></li> </ul>"},{"location":"python/square_root/#python-code","title":"\ud83e\uddea Python Code","text":"<pre><code>def integer_sqrt(n):\n    if n &lt; 2:\n        return n  # Handles 0 and 1 directly\n\n    left, right = 1, n // 2\n\n    while left &lt;= right:\n        mid = (left + right) // 2\n        if mid * mid == n:\n            return mid\n        elif mid * mid &lt; n:\n            left = mid + 1\n        else:\n            right = mid - 1\n\n    return right  # right is the floor of the square root\n</code></pre> <pre><code>print(integer_sqrt(0))   # 0\nprint(integer_sqrt(1))   # 1\nprint(integer_sqrt(9))   # 3\nprint(integer_sqrt(15))  # 3\nprint(integer_sqrt(16))  # 4\nprint(integer_sqrt(24))  # 4\nprint(integer_sqrt(25))  # 5\nprint(integer_sqrt(100)) # 10\n</code></pre>"}]}